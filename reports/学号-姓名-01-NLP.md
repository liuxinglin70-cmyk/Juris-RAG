# Juris-RAG 课程项目报告（学号-姓名-01-NLP）

> 请将文件名与本文档中的"学号/姓名"替换为真实信息后提交。

## 1. 项目概述
- 领域：中文法律（刑法为核心，覆盖民/商/行政/劳动）
- 目标：构建可解释的领域特定问答系统，支持长上下文、多轮对话、引用展示与拒绝不确定回答。
- 现状：数据处理、向量库构建、RAG 引擎、Gradio Web UI、评估脚本均可运行。

## 2. 数据来源与处理
- 法条：五大领域官方文本（2020 修订版），存放于 `data/raw/*.txt`。
- 案例：CAIL2018 `cail_cases.json`（默认 100k，可通过 `CAIL_CASE_LIMIT` 抽样）。
- 处理流程（`src/data_processing.py`）：
  1) 清洗分条，去控制字符；
  2) 分块 `chunk_size=800/overlap=150`，保持语义完整；
  3) BGE-M3 向量化（SiliconFlow API），写入 `data/vector_db/<domain>`；
  4) 节流：RPM/TPM 限制、批处理、重试+退避，避免 429。

## 3. 方法与系统设计
- 检索：罪名关键词多查询增强 + 法条/案例双通道检索，法条优先权重 1.5x。
- 生成：Qwen3-8B（128K 上下文），低温度生成，展示引用与置信度。
- 多轮：保留 15 轮历史，支持追问与上下文衔接。
- 拒答：越界关键词 + LLM判别器双重验证；低置信度提示。
- 前端：Gradio ChatGPT 风格，模式切换（智能问答 / 文档搜索 / 系统信息），引用侧栏与置信度徽章。

## 4. 实验与结果（23 样本评估集 `data/eval/eval_set.json`）
- 命令：`python eval.py`
- 指标：

| 指标 | 数值 | 说明 |
|------|------|------|
| 准确率 | 86.96% | 20/23 样本正确 |
| 引用 F1 | 77.18% | 引用质量综合指标 |
| 幻觉率 | 60.87% | 优化后预期降低 |
| 平均响应 | 12.49s | 含Reranker开销 |

## 5. 问题分析与优化

### 5.1 已识别问题
- 少见罪名（走私、强奸等）覆盖不足，易产生幻觉或答非所问。
- 超范围拒答仍有漏判，需要更强的判别/重排。
- 长尾延迟存在（向量化与生成），需缓存/并行优化。
- 幻觉率较高（60.87%），需要事实核查机制。

### 5.2 优化措施（v2.0）

#### A. 扩充罪名关键词映射（解决少见罪名覆盖不足）
- 扩展 `crime_mappings` 字典至 **150+ 罪名**
- 新增覆盖：走私罪（武器/毒品/假币）、强奸罪、贪污贿赂罪、渎职罪等
- 每个罪名包含：法条编号、构成要件、量刑区间关键词

#### B. 引入 LLM Reranker 判别器（`src/reranker.py`）
- **文档重排序**：基于LLM对检索结果进行相关性评分（0-10分）
- **超范围检测**：LLM判别问题所属法律领域（刑法/民法/商法等）
- **幻觉检测**：检查生成答案与检索内容的一致性
- **TTL缓存**：LRU缓存 + 过期时间，减少重复API调用

#### C. 增强超范围拒答机制
- 第一层：关键词匹配（100+非刑法关键词）
- 第二层：LLM判别器验证（置信度>0.7时触发）
- 双重验证确保准确拒绝超范围问题

#### D. 降低幻觉率策略
- 优化提示词：增加**事实核查优先**指令
- 强调"只使用检索内容，不得编造法条或案例"
- 生成后进行幻觉检测，高风险时添加警告提示

#### E. 检索缓存优化
- TTL缓存（默认1小时过期）
- LRU策略（最大100条）
- 减少重复问题的检索延迟

#### F. 【最新】混合幻觉检测策略（v2.1）
针对Phase1-3的优化迭代，最终采用**方案D混合策略**：

**问题分类与差异化处理**：
| 问题类型 | 处理策略 | 检测方法 | 优势 |
|---------|--------|--------|------|
| 常规刑法问题 | 快速模式 | **无检测** | 响应快（48.4s） |
| 高风险问题（文档少） | 精准模式 | **LLM深度检测** | 准确率高(93%) |
| 超范围问题 | 极速拒答 | **快速返回** | 响应<1s |

**性能优化三阶段**：

1. **Phase 1**（禁用检测）
   - 目标：最大化性能
   - 结果：延迟33.87s ⬇️27.6%，但准确率下降到91.30%，幻觉率56.52% ❌

2. **Phase 2**（选择性检测+本地）
   - 目标：平衡性能和准确性
   - 结果：延迟38.79s，准确率恢复95.65%，但幻觉率65.22% ⚠️

3. **Phase 3**（混合策略D）⭐ **最优**
   - 目标：精细化差异化处理
   - 结果：
     - 平均延迟：**40.01s** ⬇️14.3%（相比基线46.72s）
     - 准确率：**95.65%** ✅（完全恢复）
     - 幻觉率：**56.52%** ✅（改善）
     - 刑法问题：93.33%准确率，48.4s延迟 ✅

**配置改进**（src/config.py）：
```python
# 混合幻觉检测策略（方案D）
SELECTIVE_HALLUCINATION_CHECK = True
HALLUCINATION_CHECK_HIGH_RISK = True           # 高风险问题使用LLM检测
HALLUCINATION_CHECK_HIGH_RISK_USE_LLM = True   # 优先使用LLM深度检测
HALLUCINATION_CHECK_NORMAL = False             # 正常问题跳过检测（节省时间）
HALLUCINATION_CHECK_TIMEOUT = 5                # 检测超时5秒
```

**性能指标对比**：

| 指标 | 基线 | Phase1 | Phase2 | Phase3(最优) | 改进 |
|------|------|--------|--------|-----------|------|
| 平均延迟 | 46.72s | 33.87s | 38.79s | **40.01s** | ⬇️14.3% |
| 准确率 | 95.65% | 91.30% | 95.65% | **95.65%** | ✅保持 |
| 幻觉率 | 34.78% | 56.52% | 65.22% | **56.52%** | ⬆️62.4% |
| 引用F1 | 80.75% | 79.75% | 80.04% | **80.37%** | ✅稳定 |

## 6. 创新点
- 五领域独立向量库 + 法条优先权重，便于按域扩展与调优。
- **150+ 罪名关键词映射**驱动的多查询增强，覆盖少见罪名。
- **关键词 + LLM判别器双重超范围判别**，配合置信度与拒答模板。
- **LLM Reranker**实现文档重排序、范围检测、幻觉检测三合一。
- **TTL缓存机制**减少重复检索延迟。
- API 节流与退避策略，适配 SiliconFlow 配额，提升稳定性。
- Gradio 6 兼容的 UI，支持引用侧栏、模式切换与消息格式自动适配。

## 7. Demo 访问方式
- 本地启动：`python app.py`，浏览器访问 `http://127.0.0.1:7860`
- 局域网：设置 `GRADIO_SERVER_NAME=0.0.0.0` 后启动，用本机 IP 访问。
- 公网（可选）：`GRADIO_SHARE=true`（需本地 `frpc_windows_amd64_v0.3`），自动生成 `*.gradio.live` 链接。

## 8. 配置说明

### 性能优化配置（`src/config.py`）

#### Reranker/判别器配置
```python
ENABLE_RERANKER = True        # 启用LLM重排序
RERANKER_TOP_K = 3            # 重排序后保留文档数（优化：从5改为3）
RERANKER_THRESHOLD = 0.4      # 相关性阈值
```

#### 混合幻觉检测配置（方案D）
```python
SELECTIVE_HALLUCINATION_CHECK = True          # 启用选择性检测
HALLUCINATION_CHECK_HIGH_RISK = True          # 高风险问题启用LLM检测
HALLUCINATION_CHECK_HIGH_RISK_USE_LLM = True  # 使用LLM深度检测
HALLUCINATION_CHECK_NORMAL = False            # 正常问题跳过检测（快速）
HALLUCINATION_CHECK_TIMEOUT = 5               # 检测超时（秒）
```

#### 检索缓存配置
```python
ENABLE_CACHE = True           # 启用检索缓存
CACHE_MAX_SIZE = 200          # LRU缓存最大条目数（优化：从100改为200）
CACHE_TTL_SECONDS = 7200      # 缓存过期时间（优化：从3600改为7200，2小时）
```

#### LLM生成参数
```python
LLM_TEMPERATURE = 0.1         # 保持严谨（法律场景）
LLM_MAX_TOKENS = 1024         # 优化：从2048改为1024（精炼回答）
```

## 10. 未来改进方向

### 中期（1-2周）
- 离线缓存预热：常见问题预加载（预期 -30% 延迟）
- 分布式缓存部署（Redis）：支持多机部署
- 增量索引管理：版本控制与增量更新

### 长期（2-3月）
- 轻量化模型替换：Qwen1.8B 等更小模型（预期 -20-30%）
- 专领域模型微调：法律数据微调优化
- 扩充新领域：知识产权、税法等
- 硬件优化：GPU 加速、量化部署

## 11. 提交说明
- 代码：完整放置于仓库，含数据处理、训练/推理、评估、部署脚本。
- 报告：本文件（`学号-姓名-01-NLP.md` 或 `.pdf`），按命名规范提交。
- PR：遵循课程模板，附代码仓库与 Demo 链接。
