"""
Juris-RAG è¯„ä¼°æ¨¡å—
åŒ…å«å‡†ç¡®ç‡ã€å¼•ç”¨F1ã€å¹»è§‰ç‡ç­‰è¯„ä¼°æŒ‡æ ‡
"""
import os
import json
import time
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict

# å¯¼å…¥é…ç½®å’ŒRAGå¼•æ“
try:
    from src.config import (
        EVAL_DATA_PATH, REPORTS_PATH, EVAL_BATCH_SIZE,
        SILICONFLOW_API_KEY, SILICONFLOW_BASE_URL, LLM_MODEL
    )
    from src.rag_engine import JurisRAGEngine, RAGResponse
except ImportError:
    EVAL_DATA_PATH = "./data/eval"
    REPORTS_PATH = "./reports"
    EVAL_BATCH_SIZE = 10
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    LLM_MODEL = "Qwen/Qwen2.5-7B-Instruct"
    from rag_engine import JurisRAGEngine, RAGResponse


@dataclass
class EvalSample:
    """è¯„ä¼°æ ·æœ¬"""
    question: str
    ground_truth: str
    expected_sources: List[str] = None  # æœŸæœ›å¼•ç”¨çš„æ¥æº
    category: str = "general"  # é—®é¢˜ç±»åˆ«


@dataclass
class EvalResult:
    """å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ"""
    question: str
    ground_truth: str
    predicted_answer: str
    citations: List[str]
    confidence: float
    is_correct: bool
    citation_precision: float
    citation_recall: float
    citation_f1: float
    has_hallucination: bool
    relevance_score: float
    latency: float  # å“åº”æ—¶é—´ï¼ˆç§’ï¼‰


@dataclass
class EvalReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    timestamp: str
    total_samples: int
    metrics: Dict
    category_metrics: Dict
    samples: List[EvalResult]


class JurisEvaluator:
    """æ³•å¾‹RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.engine = None
        self.eval_samples: List[EvalSample] = []
        self.results: List[EvalResult] = []
        
    def initialize_engine(self):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        if self.engine is None:
            self.engine = JurisRAGEngine(streaming=False)
        return self.engine
    
    def load_eval_data(self, file_path: str = None) -> List[EvalSample]:
        """
        åŠ è½½è¯„ä¼°æ•°æ®é›†
        
        Args:
            file_path: è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSON Linesæ ¼å¼ï¼‰
            
        Returns:
            List[EvalSample]: è¯„ä¼°æ ·æœ¬åˆ—è¡¨
        """
        if file_path and os.path.exists(file_path):
            samples = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        sample = EvalSample(
                            question=data.get('question', ''),
                            ground_truth=data.get('answer', ''),
                            expected_sources=data.get('sources', []),
                            category=data.get('category', 'general')
                        )
                        samples.append(sample)
                    except:
                        continue
            self.eval_samples = samples
            print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªè¯„ä¼°æ ·æœ¬")
            return samples
        else:
            # ä½¿ç”¨å†…ç½®æµ‹è¯•é›†
            print("ğŸ“ ä½¿ç”¨å†…ç½®æµ‹è¯•é›†...")
            self.eval_samples = self._get_builtin_eval_set()
            return self.eval_samples
    
    def _get_builtin_eval_set(self) -> List[EvalSample]:
        """è·å–å†…ç½®çš„è¯„ä¼°æµ‹è¯•é›†"""
        return [
            # åˆ‘æ³•åŸºç¡€çŸ¥è¯†
            EvalSample(
                question="æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
                ground_truth="æ•…æ„æ€äººçš„ï¼Œå¤„æ­»åˆ‘ã€æ— æœŸå¾’åˆ‘æˆ–è€…åå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ï¼›æƒ…èŠ‚è¾ƒè½»çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="ç›—çªƒå…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œæˆ–è€…å¤šæ¬¡ç›—çªƒã€å…¥æˆ·ç›—çªƒã€æºå¸¦å‡¶å™¨ç›—çªƒã€æ‰’çªƒçš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ï¼Œå¹¶å¤„æˆ–è€…å•å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ä»€ä¹ˆæ˜¯æ­£å½“é˜²å«ï¼Ÿ",
                ground_truth="ä¸ºäº†ä½¿å›½å®¶ã€å…¬å…±åˆ©ç›Šã€æœ¬äººæˆ–è€…ä»–äººçš„äººèº«ã€è´¢äº§å’Œå…¶ä»–æƒåˆ©å…å—æ­£åœ¨è¿›è¡Œçš„ä¸æ³•ä¾µå®³ï¼Œè€Œé‡‡å–çš„åˆ¶æ­¢ä¸æ³•ä¾µå®³çš„è¡Œä¸ºï¼Œå¯¹ä¸æ³•ä¾µå®³äººé€ æˆæŸå®³çš„ï¼Œå±äºæ­£å½“é˜²å«ï¼Œä¸è´Ÿåˆ‘äº‹è´£ä»»ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="æŠ¢åŠ«ç½ªæ€ä¹ˆå¤„ç½šï¼Ÿ",
                ground_truth="ä»¥æš´åŠ›ã€èƒè¿«æˆ–è€…å…¶ä»–æ–¹æ³•æŠ¢åŠ«å…¬ç§è´¢ç‰©çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="æ•…æ„ä¼¤å®³ç½ªçš„åˆ‘æœŸæ˜¯å¤šå°‘ï¼Ÿ",
                ground_truth="æ•…æ„ä¼¤å®³ä»–äººèº«ä½“çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚è‡´äººé‡ä¼¤çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            # ç‰¹æ®Šæƒ…å½¢
            EvalSample(
                question="æœªæˆå¹´äººçŠ¯ç½ªæ€ä¹ˆå¤„ç†ï¼Ÿ",
                ground_truth="å·²æ»¡åå››å‘¨å²ä¸æ»¡åå…«å‘¨å²çš„äººçŠ¯ç½ªï¼Œåº”å½“ä»è½»æˆ–è€…å‡è½»å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            EvalSample(
                question="è‡ªé¦–å¯ä»¥å‡åˆ‘å—ï¼Ÿ",
                ground_truth="çŠ¯ç½ªä»¥åè‡ªåŠ¨æŠ•æ¡ˆï¼Œå¦‚å®ä¾›è¿°è‡ªå·±çš„ç½ªè¡Œçš„ï¼Œæ˜¯è‡ªé¦–ã€‚å¯¹äºè‡ªé¦–çš„çŠ¯ç½ªåˆ†å­ï¼Œå¯ä»¥ä»è½»æˆ–è€…å‡è½»å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            # æ¡ˆä¾‹ç›¸å…³
            EvalSample(
                question="è¯ˆéª—æ¡ˆä¸€èˆ¬æ€ä¹ˆåˆ¤ï¼Ÿ",
                ground_truth="è¯ˆéª—å…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ï¼Œå¹¶å¤„æˆ–è€…å•å¤„ç½šé‡‘ï¼›æ•°é¢å·¨å¤§æˆ–è€…æœ‰å…¶ä»–ä¸¥é‡æƒ…èŠ‚çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "case"],
                category="case_related"
            ),
            EvalSample(
                question="äº¤é€šè‚‡äº‹ç½ªæ€ä¹ˆåˆ¤ï¼Ÿ",
                ground_truth="è¿åäº¤é€šè¿è¾“ç®¡ç†æ³•è§„ï¼Œå› è€Œå‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œè‡´äººé‡ä¼¤ã€æ­»äº¡æˆ–è€…ä½¿å…¬ç§è´¢äº§é­å—é‡å¤§æŸå¤±çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            # è¾¹ç•Œæµ‹è¯•
            EvalSample(
                question="æ°‘æ³•å…¸å…³äºåˆåŒçš„è§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="",  # è¶…å‡ºèŒƒå›´ï¼Œåº”è¯¥æ‹’ç»å›ç­”
                expected_sources=[],
                category="out_of_scope"
            ),
        ]
    
    def evaluate_single(self, sample: EvalSample) -> EvalResult:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬
        
        Args:
            sample: è¯„ä¼°æ ·æœ¬
            
        Returns:
            EvalResult: è¯„ä¼°ç»“æœ
        """
        start_time = time.time()
        
        # è·å–æ¨¡å‹å›ç­”
        response = self.engine.query(sample.question)
        
        latency = time.time() - start_time
        
        # æå–å¼•ç”¨æ¥æº
        citations = [c.source for c in response.citations]
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        is_correct = self._check_correctness(
            response.answer, 
            sample.ground_truth, 
            sample.category
        )
        
        citation_precision, citation_recall, citation_f1 = self._calculate_citation_f1(
            citations,
            sample.expected_sources or []
        )
        
        has_hallucination = self._detect_hallucination(
            response.answer,
            response.retrieved_docs,
            sample.ground_truth
        )
        
        relevance_score = self._calculate_relevance(
            response.answer,
            sample.question,
            sample.ground_truth
        )
        
        # æ¸…ç©ºå¯¹è¯å†å²ï¼Œé¿å…å½±å“ä¸‹ä¸€ä¸ªæ ·æœ¬
        self.engine.clear_history()
        
        return EvalResult(
            question=sample.question,
            ground_truth=sample.ground_truth,
            predicted_answer=response.answer,
            citations=citations,
            confidence=response.confidence,
            is_correct=is_correct,
            citation_precision=citation_precision,
            citation_recall=citation_recall,
            citation_f1=citation_f1,
            has_hallucination=has_hallucination,
            relevance_score=relevance_score,
            latency=latency
        )
    
    def _check_correctness(
        self, 
        predicted: str, 
        ground_truth: str,
        category: str
    ) -> bool:
        """
        æ£€æŸ¥å›ç­”æ˜¯å¦æ­£ç¡®
        ä½¿ç”¨å…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
        """
        if category == "out_of_scope":
            # å¯¹äºè¶…å‡ºèŒƒå›´çš„é—®é¢˜ï¼Œæ‹’ç»å›ç­”è§†ä¸ºæ­£ç¡®
            refuse_keywords = ["æ— æ³•å›ç­”", "ä¸åœ¨", "è¶…å‡º", "æ— æ³•å‡†ç¡®", "æ²¡æœ‰ç›¸å…³", "æ— æ³•æ‰¾åˆ°"]
            return any(kw in predicted for kw in refuse_keywords)
        
        if not ground_truth:
            return False
        
        # æå–å…³é”®è¯
        # ç®€å•å®ç°ï¼šæ£€æŸ¥ground_truthä¸­çš„å…³é”®æ•°å­—å’Œå…³é”®è¯æ˜¯å¦å‡ºç°åœ¨predictedä¸­
        
        # æå–æ•°å­—ï¼ˆå¦‚åˆ‘æœŸï¼‰
        gt_numbers = re.findall(r'(\d+)å¹´', ground_truth)
        pred_numbers = re.findall(r'(\d+)å¹´', predicted)
        
        # æ£€æŸ¥å…³é”®åˆ‘ç½šè¯
        penalty_keywords = ['æ­»åˆ‘', 'æ— æœŸå¾’åˆ‘', 'æœ‰æœŸå¾’åˆ‘', 'æ‹˜å½¹', 'ç®¡åˆ¶', 'ç½šé‡‘']
        gt_penalties = [kw for kw in penalty_keywords if kw in ground_truth]
        pred_penalties = [kw for kw in penalty_keywords if kw in predicted]
        
        # è®¡ç®—åŒ¹é…åº¦
        number_match = len(set(gt_numbers) & set(pred_numbers)) / max(len(gt_numbers), 1)
        penalty_match = len(set(gt_penalties) & set(pred_penalties)) / max(len(gt_penalties), 1)
        
        # ç»¼åˆåˆ¤æ–­
        return (number_match >= 0.5 and penalty_match >= 0.5) or \
               (penalty_match >= 0.8 and len(gt_penalties) > 0)
    
    def _calculate_citation_f1(
        self, 
        predicted_sources: List[str],
        expected_sources: List[str]
    ) -> Tuple[float, float, float]:
        """
        è®¡ç®—å¼•ç”¨F1å€¼
        
        Returns:
            Tuple[precision, recall, f1]
        """
        if not expected_sources:
            # æ²¡æœ‰æœŸæœ›æ¥æºæ—¶ï¼Œæœ‰å¼•ç”¨å°±ç®—æ­£ç¡®
            return (1.0, 1.0, 1.0) if predicted_sources else (0.0, 0.0, 0.0)
        
        if not predicted_sources:
            return (0.0, 0.0, 0.0)
        
        # å°†æ¥æºæ ‡å‡†åŒ–ï¼ˆå¿½ç•¥å¤§å°å†™ï¼Œéƒ¨åˆ†åŒ¹é…ï¼‰
        def normalize_source(s):
            return s.lower().strip()
        
        pred_set = set(normalize_source(s) for s in predicted_sources)
        exp_set = set(normalize_source(s) for s in expected_sources)
        
        # è®¡ç®—äº¤é›†ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        matches = 0
        for pred in pred_set:
            for exp in exp_set:
                if exp in pred or pred in exp:
                    matches += 1
                    break
        
        precision = matches / len(pred_set) if pred_set else 0
        recall = matches / len(exp_set) if exp_set else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return (precision, recall, f1)
    
    def _detect_hallucination(
        self,
        answer: str,
        retrieved_docs: List,
        ground_truth: str
    ) -> bool:
        """
        æ£€æµ‹å¹»è§‰
        ç®€å•å®ç°ï¼šæ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«æ£€ç´¢æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„æ³•æ¡ç¼–å·
        """
        # æå–å›ç­”ä¸­çš„æ³•æ¡ç¼–å·
        answer_articles = set(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', answer))
        
        if not answer_articles:
            return False
        
        # æå–æ£€ç´¢æ–‡æ¡£ä¸­çš„æ³•æ¡ç¼–å·
        doc_articles = set()
        for doc in retrieved_docs:
            doc_articles.update(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', doc.page_content))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›ç­”ä¸­æåˆ°ä½†æ–‡æ¡£ä¸­æ²¡æœ‰çš„æ³•æ¡
        hallucinated = answer_articles - doc_articles
        
        # å¦‚æœground_truthä¸­æœ‰è¿™äº›æ³•æ¡ï¼Œä¸ç®—å¹»è§‰
        if ground_truth:
            gt_articles = set(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', ground_truth))
            hallucinated = hallucinated - gt_articles
        
        return len(hallucinated) > 0
    
    def _calculate_relevance(
        self,
        answer: str,
        question: str,
        ground_truth: str
    ) -> float:
        """
        è®¡ç®—å›ç­”ç›¸å…³æ€§åˆ†æ•°
        ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯é‡å åº¦
        """
        import jieba
        
        # åˆ†è¯
        q_words = set(jieba.cut(question))
        a_words = set(jieba.cut(answer))
        gt_words = set(jieba.cut(ground_truth)) if ground_truth else set()
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'æœ‰', 'å¯¹', 'è¢«', 'ä¸º', 'ä»¥', 'åŠ'}
        q_words = q_words - stopwords
        a_words = a_words - stopwords
        gt_words = gt_words - stopwords
        
        # è®¡ç®—ä¸é—®é¢˜çš„ç›¸å…³æ€§
        q_overlap = len(q_words & a_words) / max(len(q_words), 1)
        
        # è®¡ç®—ä¸æ ‡å‡†ç­”æ¡ˆçš„ç›¸å…³æ€§
        if gt_words:
            gt_overlap = len(gt_words & a_words) / max(len(gt_words), 1)
        else:
            gt_overlap = 0.5  # æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ—¶ç»™ä¸­ç­‰åˆ†
        
        # ç»¼åˆå¾—åˆ†
        relevance = 0.4 * q_overlap + 0.6 * gt_overlap
        return min(relevance, 1.0)
    
    def run_evaluation(self, samples: List[EvalSample] = None) -> EvalReport:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            samples: è¯„ä¼°æ ·æœ¬åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨å·²åŠ è½½çš„æ ·æœ¬
            
        Returns:
            EvalReport: è¯„ä¼°æŠ¥å‘Š
        """
        if samples is None:
            samples = self.eval_samples
        
        if not samples:
            print("âŒ æ²¡æœ‰è¯„ä¼°æ ·æœ¬ï¼è¯·å…ˆåŠ è½½æ•°æ®ã€‚")
            return None
        
        print(f"\nğŸ§ª å¼€å§‹è¯„ä¼°ï¼Œå…± {len(samples)} ä¸ªæ ·æœ¬...")
        print("=" * 60)
        
        # åˆå§‹åŒ–å¼•æ“
        self.initialize_engine()
        
        self.results = []
        category_results = defaultdict(list)
        
        for i, sample in enumerate(samples, 1):
            print(f"\n[{i}/{len(samples)}] è¯„ä¼°: {sample.question[:30]}...")
            
            try:
                result = self.evaluate_single(sample)
                self.results.append(result)
                category_results[sample.category].append(result)
                
                # æ‰“å°ç®€è¦ç»“æœ
                status = "âœ…" if result.is_correct else "âŒ"
                print(f"   {status} æ­£ç¡®æ€§: {result.is_correct}, ç½®ä¿¡åº¦: {result.confidence:.2f}, è€—æ—¶: {result.latency:.2f}s")
                
            except Exception as e:
                print(f"   âš ï¸ è¯„ä¼°å¤±è´¥: {e}")
                continue
            
            # æ‰¹æ¬¡é—´éš”
            if i % EVAL_BATCH_SIZE == 0:
                print(f"\n   å·²å®Œæˆ {i}/{len(samples)} ({100*i/len(samples):.1f}%)")
                time.sleep(1)  # é¿å…APIé™é€Ÿ
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        metrics = self._calculate_metrics(self.results)
        
        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        category_metrics = {}
        for category, results in category_results.items():
            category_metrics[category] = self._calculate_metrics(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = EvalReport(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            total_samples=len(samples),
            metrics=metrics,
            category_metrics=category_metrics,
            samples=self.results
        )
        
        return report
    
    def _calculate_metrics(self, results: List[EvalResult]) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not results:
            return {}
        
        n = len(results)
        
        return {
            "accuracy": sum(1 for r in results if r.is_correct) / n,
            "avg_confidence": sum(r.confidence for r in results) / n,
            "citation_precision": sum(r.citation_precision for r in results) / n,
            "citation_recall": sum(r.citation_recall for r in results) / n,
            "citation_f1": sum(r.citation_f1 for r in results) / n,
            "hallucination_rate": sum(1 for r in results if r.has_hallucination) / n,
            "avg_relevance": sum(r.relevance_score for r in results) / n,
            "avg_latency": sum(r.latency for r in results) / n,
            "total_samples": n
        }
    
    def print_report(self, report: EvalReport):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        print(f"â° è¯„ä¼°æ—¶é—´: {report.timestamp}")
        print(f"ğŸ“ æ ·æœ¬æ•°é‡: {report.total_samples}")
        
        print("\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print("-" * 40)
        m = report.metrics
        print(f"   å‡†ç¡®ç‡ (Accuracy):     {m['accuracy']:.2%}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦:            {m['avg_confidence']:.2%}")
        print(f"   å¼•ç”¨ç²¾ç¡®ç‡:            {m['citation_precision']:.2%}")
        print(f"   å¼•ç”¨å¬å›ç‡:            {m['citation_recall']:.2%}")
        print(f"   å¼•ç”¨F1:               {m['citation_f1']:.2%}")
        print(f"   å¹»è§‰ç‡:               {m['hallucination_rate']:.2%}")
        print(f"   å¹³å‡ç›¸å…³æ€§:            {m['avg_relevance']:.2%}")
        print(f"   å¹³å‡å“åº”æ—¶é—´:          {m['avg_latency']:.2f}s")
        
        if report.category_metrics:
            print("\nğŸ“‚ åˆ†ç±»æŒ‡æ ‡:")
            print("-" * 40)
            for category, metrics in report.category_metrics.items():
                print(f"\n   ã€{category}ã€‘(n={metrics['total_samples']})")
                print(f"      å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
                print(f"      å¼•ç”¨F1: {metrics['citation_f1']:.2%}")
                print(f"      å¹»è§‰ç‡: {metrics['hallucination_rate']:.2%}")
        
        print("\n" + "=" * 60)
    
    def save_report(self, report: EvalReport, output_path: str = None):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        if output_path is None:
            os.makedirs(REPORTS_PATH, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(REPORTS_PATH, f"eval_report_{timestamp}.json")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        report_dict = {
            "timestamp": report.timestamp,
            "total_samples": report.total_samples,
            "metrics": report.metrics,
            "category_metrics": report.category_metrics,
            "samples": [asdict(s) for s in report.samples]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
        return output_path


def run_baseline_comparison():
    """
    è¿è¡ŒåŸºçº¿å¯¹æ¯”å®éªŒ
    æ¯”è¾ƒä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½
    """
    print("ğŸ”¬ åŸºçº¿å¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    evaluator = JurisEvaluator()
    evaluator.load_eval_data()
    
    # è¿è¡Œè¯„ä¼°
    report = evaluator.run_evaluation()
    
    if report:
        evaluator.print_report(report)
        evaluator.save_report(report)
    
    return report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›ï¸ Juris-RAG è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not SILICONFLOW_API_KEY:
        print("âŒ è¯·å…ˆè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = JurisEvaluator()
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    eval_file = os.path.join(EVAL_DATA_PATH, "eval_set.json")
    evaluator.load_eval_data(eval_file)
    
    # è¿è¡Œè¯„ä¼°
    report = evaluator.run_evaluation()
    
    if report:
        # æ‰“å°æŠ¥å‘Š
        evaluator.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Š
        evaluator.save_report(report)
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
