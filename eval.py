"""
Juris-RAG è¯„ä¼°æ¨¡å—
åŒ…å«å‡†ç¡®ç‡ã€å¼•ç”¨F1ã€å¹»è§‰ç‡ç­‰è¯„ä¼°æŒ‡æ ‡
"""
import os
import json
import time
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict

# å¯¼å…¥é…ç½®å’ŒRAGå¼•æ“
try:
    from src.config import (
        EVAL_DATA_PATH, REPORTS_PATH, EVAL_BATCH_SIZE,
        SILICONFLOW_API_KEY, SILICONFLOW_BASE_URL, LLM_MODEL,
        LLM_RPM_LIMIT, LLM_TPM_LIMIT, LLM_MIN_INTERVAL
    )
    from src.rag_engine import JurisRAGEngine, RAGResponse
except ImportError:
    EVAL_DATA_PATH = "./data/eval"
    REPORTS_PATH = "./reports"
    EVAL_BATCH_SIZE = 1
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    LLM_MODEL = "Qwen/Qwen3-8B"
    LLM_RPM_LIMIT = int(os.getenv("LLM_RPM_LIMIT", "1000"))
    LLM_TPM_LIMIT = int(os.getenv("LLM_TPM_LIMIT", "50000"))
    LLM_MIN_INTERVAL = 60.0 / LLM_RPM_LIMIT if LLM_RPM_LIMIT > 0 else 0.0
    from rag_engine import JurisRAGEngine, RAGResponse


@dataclass
class EvalSample:
    """è¯„ä¼°æ ·æœ¬"""
    question: str
    ground_truth: str
    expected_sources: List[str] = None  # æœŸæœ›å¼•ç”¨çš„æ¥æº
    category: str = "general"  # é—®é¢˜ç±»åˆ«


@dataclass
class EvalResult:
    """å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ"""
    question: str
    ground_truth: str
    predicted_answer: str
    citations: List[str]
    confidence: float
    is_correct: bool
    citation_precision: float
    citation_recall: float
    citation_f1: float
    has_hallucination: bool
    relevance_score: float
    latency: float  # å“åº”æ—¶é—´ï¼ˆç§’ï¼‰


@dataclass
class EvalReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    timestamp: str
    total_samples: int
    metrics: Dict
    category_metrics: Dict
    samples: List[EvalResult]


class JurisEvaluator:
    """æ³•å¾‹RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.engine = None
        self.eval_samples: List[EvalSample] = []
        self.results: List[EvalResult] = []
        self._last_call_ts: float = 0.0
        
    def initialize_engine(self):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        if self.engine is None:
            self.engine = JurisRAGEngine(streaming=False)
        return self.engine
    
    def load_eval_data(self, file_path: str = None) -> List[EvalSample]:
        """
        åŠ è½½è¯„ä¼°æ•°æ®é›†
        
        Args:
            file_path: è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSON Linesæ ¼å¼ï¼‰
            
        Returns:
            List[EvalSample]: è¯„ä¼°æ ·æœ¬åˆ—è¡¨
        """
        if file_path and os.path.exists(file_path):
            samples = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        sample = EvalSample(
                            question=data.get('question', ''),
                            ground_truth=data.get('answer', ''),
                            expected_sources=data.get('sources', []),
                            category=data.get('category', 'general')
                        )
                        samples.append(sample)
                    except:
                        continue
            self.eval_samples = samples
            print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªè¯„ä¼°æ ·æœ¬")
            return samples
        else:
            # ä½¿ç”¨å†…ç½®æµ‹è¯•é›†
            print("ğŸ“ ä½¿ç”¨å†…ç½®æµ‹è¯•é›†...")
            self.eval_samples = self._get_builtin_eval_set()
            return self.eval_samples
    
    def _get_builtin_eval_set(self) -> List[EvalSample]:
        """è·å–å†…ç½®çš„è¯„ä¼°æµ‹è¯•é›†"""
        return [
            # åˆ‘æ³•åŸºç¡€çŸ¥è¯†
            EvalSample(
                question="æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
                ground_truth="æ•…æ„æ€äººçš„ï¼Œå¤„æ­»åˆ‘ã€æ— æœŸå¾’åˆ‘æˆ–è€…åå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ï¼›æƒ…èŠ‚è¾ƒè½»çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="ç›—çªƒå…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œæˆ–è€…å¤šæ¬¡ç›—çªƒã€å…¥æˆ·ç›—çªƒã€æºå¸¦å‡¶å™¨ç›—çªƒã€æ‰’çªƒçš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ï¼Œå¹¶å¤„æˆ–è€…å•å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ä»€ä¹ˆæ˜¯æ­£å½“é˜²å«ï¼Ÿ",
                ground_truth="ä¸ºäº†ä½¿å›½å®¶ã€å…¬å…±åˆ©ç›Šã€æœ¬äººæˆ–è€…ä»–äººçš„äººèº«ã€è´¢äº§å’Œå…¶ä»–æƒåˆ©å…å—æ­£åœ¨è¿›è¡Œçš„ä¸æ³•ä¾µå®³ï¼Œè€Œé‡‡å–çš„åˆ¶æ­¢ä¸æ³•ä¾µå®³çš„è¡Œä¸ºï¼Œå¯¹ä¸æ³•ä¾µå®³äººé€ æˆæŸå®³çš„ï¼Œå±äºæ­£å½“é˜²å«ï¼Œä¸è´Ÿåˆ‘äº‹è´£ä»»ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="æŠ¢åŠ«ç½ªæ€ä¹ˆå¤„ç½šï¼Ÿ",
                ground_truth="ä»¥æš´åŠ›ã€èƒè¿«æˆ–è€…å…¶ä»–æ–¹æ³•æŠ¢åŠ«å…¬ç§è´¢ç‰©çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="æ•…æ„ä¼¤å®³ç½ªçš„åˆ‘æœŸæ˜¯å¤šå°‘ï¼Ÿ",
                ground_truth="æ•…æ„ä¼¤å®³ä»–äººèº«ä½“çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚è‡´äººé‡ä¼¤çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            # ç‰¹æ®Šæƒ…å½¢
            EvalSample(
                question="æœªæˆå¹´äººçŠ¯ç½ªæ€ä¹ˆå¤„ç†ï¼Ÿ",
                ground_truth="å·²æ»¡åå››å‘¨å²ä¸æ»¡åå…«å‘¨å²çš„äººçŠ¯ç½ªï¼Œåº”å½“ä»è½»æˆ–è€…å‡è½»å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            EvalSample(
                question="è‡ªé¦–å¯ä»¥å‡åˆ‘å—ï¼Ÿ",
                ground_truth="çŠ¯ç½ªä»¥åè‡ªåŠ¨æŠ•æ¡ˆï¼Œå¦‚å®ä¾›è¿°è‡ªå·±çš„ç½ªè¡Œçš„ï¼Œæ˜¯è‡ªé¦–ã€‚å¯¹äºè‡ªé¦–çš„çŠ¯ç½ªåˆ†å­ï¼Œå¯ä»¥ä»è½»æˆ–è€…å‡è½»å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            # æ¡ˆä¾‹ç›¸å…³
            EvalSample(
                question="è¯ˆéª—æ¡ˆä¸€èˆ¬æ€ä¹ˆåˆ¤ï¼Ÿ",
                ground_truth="è¯ˆéª—å…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ï¼Œå¹¶å¤„æˆ–è€…å•å¤„ç½šé‡‘ï¼›æ•°é¢å·¨å¤§æˆ–è€…æœ‰å…¶ä»–ä¸¥é‡æƒ…èŠ‚çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "case"],
                category="case_related"
            ),
            EvalSample(
                question="äº¤é€šè‚‡äº‹ç½ªæ€ä¹ˆåˆ¤ï¼Ÿ",
                ground_truth="è¿åäº¤é€šè¿è¾“ç®¡ç†æ³•è§„ï¼Œå› è€Œå‘ç”Ÿé‡å¤§äº‹æ•…ï¼Œè‡´äººé‡ä¼¤ã€æ­»äº¡æˆ–è€…ä½¿å…¬ç§è´¢äº§é­å—é‡å¤§æŸå¤±çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            # æ‰©å±•æµ‹è¯•æ ·æœ¬
            EvalSample(
                question="å¼ºå¥¸ç½ªå¦‚ä½•åˆ¤åˆ‘ï¼Ÿ",
                ground_truth="ä»¥æš´åŠ›ã€èƒè¿«æˆ–è€…å…¶ä»–æ‰‹æ®µå¼ºå¥¸å¦‡å¥³çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="èµ°ç§ç½ªä¼šé¢ä¸´ä»€ä¹ˆå¤„ç½šï¼Ÿ",
                ground_truth="èµ°ç§æ­¦å™¨ã€å¼¹è¯ã€æ ¸ææ–™æˆ–è€…ä¼ªé€ çš„è´§å¸çš„ï¼Œå¤„ä¸ƒå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ï¼Œå¯ä»¥å¹¶å¤„ç½šé‡‘æˆ–è€…æ²¡æ”¶è´¢äº§ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="è´©æ¯’ç½ªçš„é‡åˆ‘æ ‡å‡†ï¼Ÿ",
                ground_truth="èµ°ç§ã€è´©è¿ã€åˆ¶é€ ã€æŒæœ‰ã€ä½¿ç”¨æ¯’å“çš„ï¼Œä¾ç…§æœ¬ç« è§„å®šå¤„ç½šã€‚é‡åˆ‘è€ƒè™‘æ¯’å“æ•°é‡ã€æ€§è´¨ç­‰å› ç´ ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ä»€ä¹ˆæ˜¯å…±åŒçŠ¯ç½ªï¼Ÿ",
                ground_truth="äºŒäººä»¥ä¸Šå…±åŒæ•…æ„çŠ¯ç½ªçš„ï¼Œæ˜¯å…±åŒçŠ¯ç½ªã€‚å…±åŒçŠ¯ç½ªäººåº”å½“æ‰¿æ‹…è´£ä»»ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ç¼“åˆ‘çš„æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="å¯¹è¢«åˆ¤å¤„æ‹˜å½¹ã€ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘çš„çŠ¯ç½ªåˆ†å­ï¼Œæ ¹æ®çŠ¯ç½ªäººçš„çŠ¯ç½ªæƒ…èŠ‚å’Œæ‚”ç½ªè¡¨ç°ï¼Œå¦‚æœè®¤ä¸ºæš‚ç¼“æ‰§è¡ŒåŸåˆ¤åˆ‘ç½šï¼Œç¡®å®ä¸è‡´å†å±å®³ç¤¾ä¼šçš„ï¼Œå¯ä»¥å®£å‘Šç¼“åˆ‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            EvalSample(
                question="ç´¯çŠ¯åº”è¯¥å¦‚ä½•å¤„ç½šï¼Ÿ",
                ground_truth="å‰ç½ªå’Œåç½ªéƒ½æ˜¯æ•…æ„çŠ¯ç½ªæˆ–è€…éƒ½æ˜¯è¿‡å¤±çŠ¯ç½ªçš„ï¼Œä»¥åŠå…ˆåçŠ¯ç½ªçš„é—´éš”è·ç¦»å’ŒçŠ¯ç½ªäººæ”¹é€ è¡¨ç°ç­‰æƒ…å†µï¼Œåº”å½“ä»é‡å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            EvalSample(
                question="è´ªæ±¡è´¿èµ‚ç½ªæ€ä¹ˆåˆ¤ï¼Ÿ",
                ground_truth="å›½å®¶å·¥ä½œäººå‘˜åˆ©ç”¨èŒåŠ¡ä¸Šçš„ä¾¿åˆ©ï¼Œç´¢å–ä»–äººè´¢ç‰©æˆ–è€…éæ³•æ”¶å—ä»–äººè´¢ç‰©ï¼Œä¸ºä»–äººè°‹å–åˆ©ç›Šçš„ï¼Œæ˜¯å—è´¿ç½ªã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ä¼ªè¯ç½ªçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="åœ¨åˆ‘äº‹è¯‰è®¼ä¸­ï¼Œè¯äººã€é‰´å®šäººã€è®°å½•äººã€ç¿»è¯‘äººæ•…æ„ä½œè™šå‡è¯æ˜ã€é‰´å®šã€è®°å½•ã€ç¿»è¯‘ï¼Œæ„å›¾é™·å®³ä»–äººæˆ–è€…éšåŒ¿ç½ªè¯çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="ä»€ä¹ˆæ—¶å€™åº”è¯¥å‡è½»å¤„ç½šï¼Ÿ",
                ground_truth="çŠ¯ç½ªæƒ…èŠ‚è½»å¾®ã€å±å®³ä¸å¤§çš„ï¼Œä¸è®¤ä¸ºæ˜¯çŠ¯ç½ªï¼Œæˆ–è€…å¯ä»¥å‡è½»å¤„ç½šã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="special_case"
            ),
            EvalSample(
                question="æŠ¢åŠ«ç½ªå’Œç›—çªƒç½ªçš„åŒºåˆ«ï¼Ÿ",
                ground_truth="æŠ¢åŠ«ç½ªä»¥æš´åŠ›ã€èƒè¿«æˆ–è€…å…¶ä»–æ–¹æ³•å¼ºè¡Œå¤ºå–è´¢ç‰©ï¼›ç›—çªƒç½ªæ˜¯ç§˜å¯†ç›—å–ã€‚äºŒè€…æ‰‹æ®µå®Œå…¨ä¸åŒã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="èšä¼—æ–—æ®´ç½ªå¦‚ä½•å¤„ç½šï¼Ÿ",
                ground_truth="èšä¼—æ–—æ®´çš„ï¼Œå¯¹é¦–è¦åˆ†å­å’Œå…¶ä»–ç§¯æå‚åŠ çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="åŒ…åº‡ç½ªçš„æ„æˆæ¡ä»¶ï¼Ÿ",
                ground_truth="æ˜çŸ¥æ˜¯çŠ¯ç½ªçš„äººè€Œä¸ºå…¶éšç’ã€åŒ…åº‡ï¼Œæˆ–è€…ä¸ºå…¶æä¾›é€ƒåŒ¿çš„ä¾¿åˆ©ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            EvalSample(
                question="å¦¨å®³å…¬åŠ¡ç½ªæ€ä¹ˆå¤„ç½šï¼Ÿ",
                ground_truth="ä»¥æš´åŠ›ã€å¨èƒæ–¹æ³•é˜»ç¢å›½å®¶æœºå…³å·¥ä½œäººå‘˜ä¾æ³•æ‰§è¡ŒèŒåŠ¡çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹ã€ç®¡åˆ¶æˆ–ç½šé‡‘ã€‚",
                expected_sources=["åˆ‘æ³•", "statute"],
                category="criminal_law"
            ),
            # è¾¹ç•Œæµ‹è¯•
            EvalSample(
                question="æ°‘æ³•å…¸å…³äºåˆåŒçš„è§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="",  # è¶…å‡ºèŒƒå›´ï¼Œåº”è¯¥æ‹’ç»å›ç­”
                expected_sources=[],
                category="out_of_scope"
            ),
            EvalSample(
                question="è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆæ³•å¾‹é£é™©ï¼Ÿ",
                ground_truth="",  # è¶…å‡ºèŒƒå›´ï¼Œåº”è¯¥æ‹’ç»å›ç­”
                expected_sources=[],
                category="out_of_scope"
            ),
            EvalSample(
                question="å…¬å¸æ³•ä¸­å…³äºè‘£äº‹è´£ä»»çš„è§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ",
                ground_truth="",  # è¶…å‡ºèŒƒå›´ï¼Œåº”è¯¥æ‹’ç»å›ç­”
                expected_sources=[],
                category="out_of_scope"
            ),
        ]
    
    def evaluate_single(self, sample: EvalSample) -> EvalResult:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬
        
        Args:
            sample: è¯„ä¼°æ ·æœ¬
            
        Returns:
            EvalResult: è¯„ä¼°ç»“æœ
        """
        start_time = time.time()

        # ç®€å•é€Ÿç‡é™åˆ¶ï¼ˆæŒ‰ RPM èŠ‚æµï¼‰
        self._respect_rate_limit()
        
        # è·å–æ¨¡å‹å›ç­”
        response = self.engine.query(sample.question)
        
        latency = time.time() - start_time
        
        # æå–å¼•ç”¨æ¥æº
        citations = [c.source for c in response.citations]
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        is_correct = self._check_correctness(
            response.answer, 
            sample.ground_truth, 
            sample.category
        )
        
        citation_precision, citation_recall, citation_f1 = self._calculate_citation_f1(
            citations,
            sample.expected_sources or []
        )
        
        has_hallucination = self._detect_hallucination(
            response.answer,
            response.retrieved_docs,
            sample.ground_truth
        )
        
        relevance_score = self._calculate_relevance(
            response.answer,
            sample.question,
            sample.ground_truth
        )
        
        # æ¸…ç©ºå¯¹è¯å†å²ï¼Œé¿å…å½±å“ä¸‹ä¸€ä¸ªæ ·æœ¬
        self.engine.clear_history()
        
        return EvalResult(
            question=sample.question,
            ground_truth=sample.ground_truth,
            predicted_answer=response.answer,
            citations=citations,
            confidence=response.confidence,
            is_correct=is_correct,
            citation_precision=citation_precision,
            citation_recall=citation_recall,
            citation_f1=citation_f1,
            has_hallucination=has_hallucination,
            relevance_score=relevance_score,
            latency=latency
        )

    def _respect_rate_limit(self):
        """åŸºäº RPM åšæœ€å°é—´éš”èŠ‚æµï¼Œé€‚é… L0 é…é¢ã€‚"""
        if LLM_MIN_INTERVAL <= 0:
            return
        now = time.time()
        wait = LLM_MIN_INTERVAL - (now - self._last_call_ts)
        if wait > 0:
            time.sleep(wait)
        self._last_call_ts = time.time()
    
    def _check_correctness(
        self, 
        predicted: str, 
        ground_truth: str,
        category: str
    ) -> bool:
        """
        æ£€æŸ¥å›ç­”æ˜¯å¦æ­£ç¡®
        æ›´ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†ï¼šå†…å®¹å¿…é¡»åŸºæœ¬ä¸€è‡´
        """
        if category == "out_of_scope":
            # å¯¹äºè¶…å‡ºèŒƒå›´çš„é—®é¢˜ï¼Œæ‹’ç»å›ç­”è§†ä¸ºæ­£ç¡®
            refuse_keywords = ["æ— æ³•å›ç­”", "ä¸åœ¨", "è¶…å‡º", "æ— æ³•å‡†ç¡®", "æ²¡æœ‰ç›¸å…³", "æ— æ³•æ‰¾åˆ°", "è¶…å‡º"]
            return any(kw in predicted for kw in refuse_keywords)
        
        if not ground_truth:
            return False
        
        # æå–å…³é”®è¦ç´ è¿›è¡ŒåŒ¹é…
        
        # 1. æå–åˆ‘æœŸæ•°å­—ï¼ˆæœ€é‡è¦ï¼‰
        gt_numbers = re.findall(r'(\d+)å¹´', ground_truth)
        pred_numbers = re.findall(r'(\d+)å¹´', predicted)
        
        # 2. æ£€æŸ¥å…³é”®åˆ‘ç½šè¯
        penalty_keywords = ['æ­»åˆ‘', 'æ— æœŸå¾’åˆ‘', 'æœ‰æœŸå¾’åˆ‘', 'æ‹˜å½¹', 'ç®¡åˆ¶', 'ç½šé‡‘', 'æ²¡æ”¶']
        gt_penalties = set(kw for kw in penalty_keywords if kw in ground_truth)
        pred_penalties = set(kw for kw in penalty_keywords if kw in predicted)
        
        # 3. æ£€æŸ¥æ˜¯å¦æ‹’ç»å›ç­”ï¼ˆå¦‚æœåº”è¯¥å›ç­”å´æ‹’ç»äº†ï¼‰
        refused = "æ— æ³•å›ç­”" in predicted or "æ— æ³•å‡†ç¡®" in predicted or "æ— æ³•æ‰¾åˆ°" in predicted
        
        if refused and ground_truth:
            # åº”è¯¥èƒ½å›ç­”ä½†æ‹’ç»äº† -> é”™è¯¯
            return False
        
        # 4. ç»¼åˆåˆ¤æ–­æ­£ç¡®æ€§
        # ä¸¥æ ¼æ ‡å‡†ï¼šåˆ‘æœŸå’Œåˆ‘ç½šè¯éƒ½è¦åŒ¹é…
        
        if gt_penalties:
            # æœ‰åˆ‘ç½šè¯çš„ï¼Œåˆ‘ç½šè¯åŒ¹é…åº¦è¦é«˜
            penalty_match_ratio = len(gt_penalties & pred_penalties) / len(gt_penalties)
            if penalty_match_ratio < 0.5:
                return False  # ä¸»è¦åˆ‘ç½šè¯éƒ½æ²¡å¯¹ä¸Š
        
        if gt_numbers:
            # æœ‰æ•°å­—çš„ï¼ˆåˆ‘æœŸï¼‰ï¼Œæ•°å­—åŒ¹é…åº¦ä¹Ÿè¦é«˜
            number_match_ratio = len(set(gt_numbers) & set(pred_numbers)) / len(gt_numbers)
            if number_match_ratio < 0.5:
                # ä¸»è¦çš„åˆ‘æœŸæ•°å­—éƒ½æ²¡å¯¹ä¸Š
                return False
        
        # æ£€æŸ¥å…³é”®æ³•å¾‹æ¦‚å¿µæ˜¯å¦åŒ…å«
        if "æ¡" in ground_truth:
            # å¦‚æœæ˜¯æ³•æ¡å®šä¹‰ï¼Œè‡³å°‘è¦æœ‰æ³•å¾‹æ¦‚å¿µçš„åŒ¹é…
            concept_words = ['å®šä¹‰', 'è§„å®š', 'æ˜¯æŒ‡', 'å¤„', 'å¤„ç½š', 'åˆ‘äº‹è´£ä»»']
            gt_has_concept = any(w in ground_truth for w in concept_words)
            pred_has_concept = any(w in predicted for w in concept_words)
            
            if gt_has_concept and not pred_has_concept:
                return False
        
        # å¦‚æœä¸»è¦è¦ç´ éƒ½åŒ¹é…äº†ï¼Œè§†ä¸ºæ­£ç¡®
        return True
    
    def _calculate_citation_f1(
        self, 
        predicted_sources: List[str],
        expected_sources: List[str]
    ) -> Tuple[float, float, float]:
        """
        è®¡ç®—å¼•ç”¨F1å€¼ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        
        Returns:
            Tuple[precision, recall, f1]
        """
        if not expected_sources:
            # æ²¡æœ‰æœŸæœ›æ¥æºæ—¶ï¼Œæœ‰å¼•ç”¨å°±ç®—æ­£ç¡®
            return (1.0, 1.0, 1.0) if predicted_sources else (0.0, 0.0, 0.0)
        
        if not predicted_sources:
            return (0.0, 0.0, 0.0)
        
        # å°†æ¥æºæ ‡å‡†åŒ–å¹¶è¿›è¡Œéƒ¨åˆ†åŒ¹é…
        def normalize_source(s):
            s = s.lower().strip()
            # ç§»é™¤å¸¸è§å™ªéŸ³
            for noise in ['ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', '(', ')']:
                s = s.replace(noise, ' ')
            return s
        
        # å®šä¹‰åŒ¹é…å…³é”®è¯
        source_keywords = {
            'statute': ['åˆ‘æ³•', 'statute', 'æ³•æ¡', 'æ¡'],
            'case': ['cail', 'æ¡ˆä¾‹', 'case', 'å¸æ³•']
        }
        
        # æ£€æŸ¥é¢„æµ‹æ¥æºæ˜¯å¦åŒ¹é…æœŸæœ›æ¥æº
        matches = 0
        for exp in expected_sources:
            exp_norm = normalize_source(exp)
            for pred in predicted_sources:
                pred_norm = normalize_source(pred)
                # ç›´æ¥åŒ¹é…
                if exp_norm in pred_norm or pred_norm in exp_norm:
                    matches += 1
                    break
                # å…³é”®è¯åŒ¹é…
                for category, keywords in source_keywords.items():
                    if any(kw in exp_norm for kw in keywords) and any(kw in pred_norm for kw in keywords):
                        matches += 1
                        break
        
        precision = min(matches / len(predicted_sources), 1.0) if predicted_sources else 0
        recall = min(matches / len(expected_sources), 1.0) if expected_sources else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return (precision, recall, f1)
    
    def _detect_hallucination(
        self,
        answer: str,
        retrieved_docs: List,
        ground_truth: str
    ) -> bool:
        """
        æ£€æµ‹å¹»è§‰ - å¹³è¡¡æ£€æµ‹
        åªæ£€æµ‹æ˜ç¡®çš„ç¼–é€ è¡Œä¸ºï¼Œé¿å…è¿‡äºä¸¥æ ¼å¯¼è‡´è¯¯åˆ¤
        """
        # å¦‚æœå›ç­”æ˜ç¡®è¡¨ç¤ºæ— æ³•å›ç­”ï¼Œä¸ç®—å¹»è§‰
        refusal_keywords = ["æ— æ³•å›ç­”", "æ— æ³•å‡†ç¡®", "æœªæ‰¾åˆ°ç›¸å…³", "æ£€ç´¢å†…å®¹ä¸­æœªæ‰¾åˆ°", "æ²¡æœ‰ç›¸å…³"]
        if any(kw in answer for kw in refusal_keywords):
            return False
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ä½†æœ‰å®è´¨æ€§ç­”æ¡ˆï¼Œå¯èƒ½æ˜¯å¹»è§‰
        if not retrieved_docs and answer and len(answer) > 100:
            return True
        
        if not retrieved_docs:
            return False
        
        # æå–å›ç­”ä¸­çš„æ³•æ¡ç¼–å·
        answer_articles = set(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', answer))
        
        # å¦‚æœå›ç­”æ²¡æœ‰å¼•ç”¨å…·ä½“æ³•æ¡ï¼Œä¸æ£€æµ‹å¹»è§‰ï¼ˆå¯èƒ½æ˜¯æ¦‚æ‹¬æ€§å›ç­”ï¼‰
        if not answer_articles:
            return False
        
        # æå–æ£€ç´¢æ–‡æ¡£ä¸­çš„æ³•æ¡ç¼–å·
        doc_articles = set()
        doc_full_text = ""
        for doc in retrieved_docs:
            doc_articles.update(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', doc.page_content))
            doc_full_text += doc.page_content
        
        # å…è®¸ground_truthä¸­çš„æ³•æ¡
        if ground_truth:
            gt_articles = set(re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', ground_truth))
            doc_articles = doc_articles | gt_articles
        
        # æ£€æµ‹ï¼šå›ç­”ä¸­è¶…è¿‡åŠæ•°çš„æ³•æ¡åœ¨æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°
        hallucinated_articles = answer_articles - doc_articles
        if len(hallucinated_articles) > len(answer_articles) / 2:
            return True
        
        return False
    
    def _calculate_relevance(
        self,
        answer: str,
        question: str,
        ground_truth: str
    ) -> float:
        """
        è®¡ç®—å›ç­”ç›¸å…³æ€§åˆ†æ•°
        ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯é‡å åº¦
        """
        import jieba
        
        # åˆ†è¯
        q_words = set(jieba.cut(question))
        a_words = set(jieba.cut(answer))
        gt_words = set(jieba.cut(ground_truth)) if ground_truth else set()
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'æœ‰', 'å¯¹', 'è¢«', 'ä¸º', 'ä»¥', 'åŠ'}
        q_words = q_words - stopwords
        a_words = a_words - stopwords
        gt_words = gt_words - stopwords
        
        # è®¡ç®—ä¸é—®é¢˜çš„ç›¸å…³æ€§
        q_overlap = len(q_words & a_words) / max(len(q_words), 1)
        
        # è®¡ç®—ä¸æ ‡å‡†ç­”æ¡ˆçš„ç›¸å…³æ€§
        if gt_words:
            gt_overlap = len(gt_words & a_words) / max(len(gt_words), 1)
        else:
            gt_overlap = 0.5  # æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ—¶ç»™ä¸­ç­‰åˆ†
        
        # ç»¼åˆå¾—åˆ†
        relevance = 0.4 * q_overlap + 0.6 * gt_overlap
        return min(relevance, 1.0)
    
    def run_evaluation(self, samples: List[EvalSample] = None) -> EvalReport:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            samples: è¯„ä¼°æ ·æœ¬åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨å·²åŠ è½½çš„æ ·æœ¬
            
        Returns:
            EvalReport: è¯„ä¼°æŠ¥å‘Š
        """
        if samples is None:
            samples = self.eval_samples
        
        if not samples:
            print("âŒ æ²¡æœ‰è¯„ä¼°æ ·æœ¬ï¼è¯·å…ˆåŠ è½½æ•°æ®ã€‚")
            return None
        
        print(f"\nğŸ§ª å¼€å§‹è¯„ä¼°ï¼Œå…± {len(samples)} ä¸ªæ ·æœ¬...")
        print("=" * 60)
        
        # åˆå§‹åŒ–å¼•æ“
        self.initialize_engine()
        
        self.results = []
        category_results = defaultdict(list)
        
        for i, sample in enumerate(samples, 1):
            print(f"\n[{i}/{len(samples)}] è¯„ä¼°: {sample.question[:30]}...")
            
            attempt = 0
            max_retries = 2
            backoff = 5
            while attempt <= max_retries:
                try:
                    result = self.evaluate_single(sample)
                    self.results.append(result)
                    category_results[sample.category].append(result)
                    
                    status = "âœ…" if result.is_correct else "âŒ"
                    print(f"   {status} æ­£ç¡®æ€§: {result.is_correct}, ç½®ä¿¡åº¦: {result.confidence:.2f}, è€—æ—¶: {result.latency:.2f}s")
                    break
                except Exception as e:
                    if self._is_rate_limit_error(e) and attempt < max_retries:
                        wait_seconds = backoff * (2 ** attempt)
                        print(f"   âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾… {wait_seconds}s åé‡è¯• (ç¬¬ {attempt+1}/{max_retries} æ¬¡)")
                        time.sleep(wait_seconds)
                        attempt += 1
                        continue
                    print(f"   âš ï¸ è¯„ä¼°å¤±è´¥: {e}")
                    break
            
            # æ‰¹æ¬¡é—´éš”
            if i % EVAL_BATCH_SIZE == 0:
                print(f"\n   å·²å®Œæˆ {i}/{len(samples)} ({100*i/len(samples):.1f}%)")
                # é¢å¤–èŠ‚æµï¼šæŒ‰ç…§ RPM è¿½åŠ ç­‰å¾…ï¼Œé¿å…çŸ­æ—¶é—´è¿‡å¤šè¯·æ±‚
                if LLM_MIN_INTERVAL > 0:
                    time.sleep(LLM_MIN_INTERVAL)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        metrics = self._calculate_metrics(self.results)
        
        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        category_metrics = {}
        for category, results in category_results.items():
            category_metrics[category] = self._calculate_metrics(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = EvalReport(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            total_samples=len(samples),
            metrics=metrics,
            category_metrics=category_metrics,
            samples=self.results
        )
        
        return report

    @staticmethod
    def _is_rate_limit_error(err: Exception) -> bool:
        msg = str(err).lower()
        return any(key in msg for key in ["rpm limit", "rate limit", "429", "too many", "exceeded"])
    
    def _calculate_metrics(self, results: List[EvalResult]) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not results:
            return {}
        
        n = len(results)
        
        return {
            "accuracy": sum(1 for r in results if r.is_correct) / n,
            "avg_confidence": sum(r.confidence for r in results) / n,
            "citation_precision": sum(r.citation_precision for r in results) / n,
            "citation_recall": sum(r.citation_recall for r in results) / n,
            "citation_f1": sum(r.citation_f1 for r in results) / n,
            "hallucination_rate": sum(1 for r in results if r.has_hallucination) / n,
            "avg_relevance": sum(r.relevance_score for r in results) / n,
            "avg_latency": sum(r.latency for r in results) / n,
            "total_samples": n
        }
    
    def print_report(self, report: EvalReport):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        print(f"â° è¯„ä¼°æ—¶é—´: {report.timestamp}")
        print(f"ğŸ“ æ ·æœ¬æ•°é‡: {report.total_samples}")

        if not report.metrics:
            print("âš ï¸ æ— å¯ç”¨è¯„ä¼°ç»“æœï¼ˆå…¨éƒ¨æ ·æœ¬å¤±è´¥æˆ–è¢«è·³è¿‡ï¼‰")
            print("=" * 60)
            return

        print("\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print("-" * 40)
        m = report.metrics
        print(f"   å‡†ç¡®ç‡ (Accuracy):     {m['accuracy']:.2%}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦:            {m['avg_confidence']:.2%}")
        print(f"   å¼•ç”¨ç²¾ç¡®ç‡:            {m['citation_precision']:.2%}")
        print(f"   å¼•ç”¨å¬å›ç‡:            {m['citation_recall']:.2%}")
        print(f"   å¼•ç”¨F1:               {m['citation_f1']:.2%}")
        print(f"   å¹»è§‰ç‡:               {m['hallucination_rate']:.2%}")
        print(f"   å¹³å‡ç›¸å…³æ€§:            {m['avg_relevance']:.2%}")
        print(f"   å¹³å‡å“åº”æ—¶é—´:          {m['avg_latency']:.2f}s")
        
        if report.category_metrics:
            print("\nğŸ“‚ åˆ†ç±»æŒ‡æ ‡:")
            print("-" * 40)
            for category, metrics in report.category_metrics.items():
                print(f"\n   ã€{category}ã€‘(n={metrics['total_samples']})")
                print(f"      å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
                print(f"      å¼•ç”¨F1: {metrics['citation_f1']:.2%}")
                print(f"      å¹»è§‰ç‡: {metrics['hallucination_rate']:.2%}")
        
        print("\n" + "=" * 60)
    
    def save_report(self, report: EvalReport, output_path: str = None):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        if output_path is None:
            os.makedirs(REPORTS_PATH, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(REPORTS_PATH, f"eval_report_{timestamp}.json")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        report_dict = {
            "timestamp": report.timestamp,
            "total_samples": report.total_samples,
            "metrics": report.metrics,
            "category_metrics": report.category_metrics,
            "samples": [asdict(s) for s in report.samples]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
        return output_path


def run_baseline_comparison():
    """
    è¿è¡ŒåŸºçº¿å¯¹æ¯”å®éªŒ
    æ¯”è¾ƒä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½
    """
    print("ğŸ”¬ åŸºçº¿å¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    evaluator = JurisEvaluator()
    evaluator.load_eval_data()
    
    # è¿è¡Œè¯„ä¼°
    report = evaluator.run_evaluation()
    
    if report:
        evaluator.print_report(report)
        evaluator.save_report(report)
    
    return report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›ï¸ Juris-RAG è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not SILICONFLOW_API_KEY:
        print("âŒ è¯·å…ˆè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = JurisEvaluator()
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    eval_file = os.path.join(EVAL_DATA_PATH, "eval_set.json")
    evaluator.load_eval_data(eval_file)
    
    # è¿è¡Œè¯„ä¼°
    report = evaluator.run_evaluation()
    
    if report:
        # æ‰“å°æŠ¥å‘Š
        evaluator.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Š
        evaluator.save_report(report)
        
        print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
