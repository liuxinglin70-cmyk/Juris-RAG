# 数据处理前检查清单

## ✅ 数据集验证完成

### 📁 已验证的文件

| 文件名 | 法律名称 | 大小 | 字符数 | 状态 |
|--------|---------|------|--------|------|
| criminal_code.txt | 中华人民共和国刑法 | 220.8 KB | 76,436 | ✅ 通过 |
| civil_code.txt | 中华人民共和国民法典 | 348.4 KB | 119,824 | ✅ 通过 |
| administrative_law.txt | 中华人民共和国行政处罚法 | 32.7 KB | 11,253 | ✅ 通过 |
| labor_law.txt | 中华人民共和国劳动法 | 27.4 KB | 9,455 | ✅ 通过 |
| commercial_law.txt | 中华人民共和国公司法 | 102.4 KB | 35,248 | ✅ 通过 |
| cail_cases_20k.json | CAIL司法案例数据集 | 139.6 MB | 100,000案例 | ✅ 通过 |

**总计**: 6/6 文件验证通过 ✅

---

## 🔍 代码逻辑验证

### 1. 数据处理模块 (src/data_processing.py)

✅ **多领域支持**
- 定义了5个法律领域：刑法、民法、商法、行政法、劳动法
- 每个领域有独立的元数据配置
- 支持按领域创建独立的向量库

✅ **法条加载逻辑**
- `load_statutes()` 方法正确处理法律条文
- 按"第X条"进行智能分割
- 保留章节信息增强检索效果
- 对超长条款进行二次分割（最大800字符）

✅ **案例加载逻辑**
- `load_cail_cases()` 支持JSON数组格式
- 正确提取案情、罪名、法条、判决信息
- 结构化组织案例内容
- 限制加载数量避免内存问题

✅ **文本清洗**
- `clean_text()` 去除多余空白和特殊字符
- 规范化标点符号
- 保持UTF-8编码

✅ **向量化流程**
- 使用 BGE-M3 embedding模型（1024维）
- 批量处理减少API调用
- 错误重试机制
- 为每个领域创建独立的ChromaDB

### 2. RAG引擎模块 (src/rag_engine.py)

✅ **多领域检索**
- 支持跨领域检索或单领域检索
- 法条优先级权重调整（STATUTE_BOOST = 1.5）
- 相似度阈值过滤（0.3）

✅ **长上下文支持**
- Qwen3-8B支持128K上下文
- 保留15轮历史对话
- 智能上下文管理

✅ **置信度机制**
- 低于阈值（0.35）时拒绝回答
- 超出领域范围时明确说明
- 返回不确定响应

✅ **引用来源**
- 每个回答标注来源法条或案例
- 包含章节、条款号信息
- 可追溯性强

### 3. 配置模块 (src/config.py)

✅ **路径配置正确**
```python
DATA_PATH = "data/raw"
DB_PATH = "data/vector_db"
```

✅ **模型配置**
```python
EMBEDDING_MODEL = "BAAI/bge-m3"  # 中文法律领域优秀
LLM_MODEL = "Qwen/Qwen3-8B"       # 128K长上下文
```

✅ **RAG参数合理**
```python
CHUNK_SIZE = 800           # 保持法条完整
CHUNK_OVERLAP = 150        # 适当重叠
RETRIEVAL_TOP_K = 8        # 检索8个文档
CONFIDENCE_THRESHOLD = 0.35 # 置信度阈值
```

✅ **API限流配置**
```python
EMBED_RPM_LIMIT = 1000     # 避免触发限流
EMBED_BATCH_SIZE = 20      # 批量处理
EMBED_SLEEP_SECONDS = 0.1  # 请求间隔
```

---

## 📊 预期的向量化结果

### 单领域模式（仅刑法）
- **法条文档**: ~500个条款
- **案例文档**: 20,000个案例（可配置）
- **总文档数**: ~20,500个
- **向量库大小**: ~50-100 MB

### 多领域模式（推荐）
每个领域独立向量库：

| 领域 | 法条数 | 案例数 | 总文档 | 向量库路径 |
|------|--------|--------|--------|-----------|
| 刑法 | ~500 | 20,000 | ~20,500 | data/vector_db/criminal |
| 民法 | ~1,200 | 0 | ~1,200 | data/vector_db/civil |
| 商法 | ~400 | 0 | ~400 | data/vector_db/commercial |
| 行政法 | ~120 | 0 | ~120 | data/vector_db/administrative |
| 劳动法 | ~100 | 0 | ~100 | data/vector_db/labor |

**总计**: ~22,320个文档，5个独立向量库

---

## 🚀 准备就绪！

### 环境检查
- ✅ Python环境正常
- ✅ 所有依赖包已安装（requirements.txt）
- ✅ SILICONFLOW_API_KEY 已设置
- ✅ 数据文件完整且格式正确
- ✅ 代码逻辑验证通过

### 下一步操作

**开始数据处理和向量化：**

```bash
# 方式1: 多领域模式（推荐）
python src/data_processing.py

# 方式2: 单领域模式（仅刑法）
# 需要修改 data_processing.py 最后一行：
# build_vector_db(multi_domain=False)
```

**预计耗时：**
- 加载数据: 2-3分钟
- 向量化处理: 10-30分钟（取决于API速度和案例数量）
- 保存向量库: 1-2分钟

**注意事项：**
1. 确保网络连接稳定
2. 向量化过程会调用API，需要API密钥有效
3. 如遇到限流错误，脚本会自动重试
4. 向量库会保存在 `data/vector_db/` 目录下
5. 可以随时中断（Ctrl+C），重新运行会继续

### 验证向量化结果

向量化完成后，检查：

```bash
# 查看向量库目录
ls data/vector_db/

# 应该看到：
# criminal/     - 刑法向量库
# civil/        - 民法向量库
# commercial/   - 商法向量库
# administrative/ - 行政法向量库
# labor/        - 劳动法向量库
```

每个目录下应包含：
- `chroma.sqlite3` - ChromaDB数据库文件
- 其他索引文件

---

## 📝 常见问题

**Q1: 向量化失败怎么办？**
- 检查API密钥是否有效
- 检查网络连接
- 查看错误信息，可能是限流问题
- 可以减少 `CAIL_CASE_LIMIT` 的值

**Q2: 内存不足怎么办？**
- 减少案例数量：设置环境变量 `CAIL_CASE_LIMIT=5000`
- 减少批处理大小：`EMBED_BATCH_SIZE=10`

**Q3: 如何只向量化特定领域？**
- 修改 `data_processing.py` 中的 `LEGAL_DOMAINS` 字典
- 注释掉不需要的领域

**Q4: 如何更新向量库？**
- 删除对应领域的向量库目录
- 重新运行 `python src/data_processing.py`

---

## ✅ 最终确认

- [x] 所有数据文件已手动补充完整
- [x] 文件编码为 UTF-8
- [x] 文件内容格式正确
- [x] 代码逻辑验证通过
- [x] 配置参数合理
- [x] API密钥已设置
- [x] 准备开始向量化

**可以安全地运行数据处理了！** 🎉
