"""
Juris-RAG æ•°æ®å¤„ç†æ¨¡å—
è´Ÿè´£æ³•å¾‹æ–‡æœ¬çš„åŠ è½½ã€æ¸…æ´—ã€åˆ†å—å’Œå‘é‡åŒ–
"""
import os
import re
import json
import time
import hashlib
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm

# LangChain ç»„ä»¶
try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain_core.documents import Document
    except ImportError:
        from langchain_classic.schema import Document

# ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆ langchain-chroma
try:
    from langchain_chroma import Chroma  # type: ignore
except ImportError:  # pragma: no cover - å…¼å®¹æ—§ç¯å¢ƒ
    from langchain_community.vectorstores import Chroma

try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DATA_PATH, DB_PATH, EMBEDDING_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, CHUNK_SIZE, CHUNK_OVERLAP, 
        CAIL_CASE_LIMIT, STATUTE_SEPARATORS,
        EMBED_RPM_LIMIT, EMBED_TPM_LIMIT,
        EMBED_BATCH_SIZE, EMBED_SLEEP_SECONDS, EMBED_MAX_RETRIES,
        EMBED_BACKOFF_SECONDS, EMBED_BACKOFF_MAX_SECONDS
    )
    from src.cail_adapter import get_cail_file_path
except ImportError:
    # é»˜è®¤é…ç½®
    DATA_PATH = "./data/raw"
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    CAIL_CASE_LIMIT = int(os.getenv("CAIL_CASE_LIMIT", "100000"))
    STATUTE_SEPARATORS = ["\nç¬¬", "\n\n", "\n", "ã€‚", "ï¼›"]
    EMBED_RPM_LIMIT = int(os.getenv("EMBED_RPM_LIMIT", "2000"))
    EMBED_TPM_LIMIT = int(os.getenv("EMBED_TPM_LIMIT", "500000"))
    EMBED_BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "20"))
    EMBED_SLEEP_SECONDS = float(os.getenv("EMBED_SLEEP_SECONDS", "0.1"))
    EMBED_MAX_RETRIES = 5
    EMBED_BACKOFF_SECONDS = 10
    EMBED_BACKOFF_MAX_SECONDS = 120
    
    def get_cail_file_path():
        from pathlib import Path
        data_dir = Path(DATA_PATH)
        cail_file = data_dir / "cail_cases.json"
        return str(cail_file)


class LegalDataProcessor:
    """æ³•å¾‹æ•°æ®å¤„ç†å™¨ - æ”¯æŒå¤šé¢†åŸŸæ³•å¾‹æ•°æ®"""
    
    # å®šä¹‰æ”¯æŒçš„æ³•å¾‹é¢†åŸŸ
    LEGAL_DOMAINS = {
        'criminal': {
            'name': 'åˆ‘æ³•',
            'file': 'criminal_code.txt',
            'priority': 1,  # æœ€é«˜ä¼˜å…ˆçº§
            'description': 'ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³• - å…³äºçŠ¯ç½ªå’Œåˆ‘ç½šçš„è§„å®š'
        },
        'civil': {
            'name': 'æ°‘æ³•',
            'file': 'civil_code.txt',
            'priority': 2,
            'description': 'ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ - å…³äºæ°‘äº‹æƒåˆ©å’Œä¹‰åŠ¡çš„è§„å®š'
        },
        'commercial': {
            'name': 'å•†æ³•',
            'file': 'commercial_law.txt',
            'priority': 3,
            'description': 'å•†æ³•åŠç›¸å…³æ³•å¾‹ - å…³äºå•†ä¸šã€å…¬å¸ã€è¯åˆ¸ç­‰çš„è§„å®š'
        },
        'administrative': {
            'name': 'è¡Œæ”¿æ³•',
            'file': 'administrative_law.txt',
            'priority': 4,
            'description': 'è¡Œæ”¿æ³•åŠç›¸å…³æ³•å¾‹ - å…³äºè¡Œæ”¿è¡Œä¸ºå’Œè¡Œæ”¿ç®¡ç†çš„è§„å®š'
        },
        'labor': {
            'name': 'åŠ³åŠ¨æ³•',
            'file': 'labor_law.txt',
            'priority': 5,
            'description': 'åŠ³åŠ¨æ³•åŠç›¸å…³æ³•å¾‹ - å…³äºåŠ³åŠ¨æƒåˆ©å’Œä¹‰åŠ¡çš„è§„å®š'
        }
    }
    
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or SILICONFLOW_API_KEY
        self.base_url = base_url or SILICONFLOW_BASE_URL
        
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        # åˆå§‹åŒ–Embeddingæ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=self.base_url,
            openai_api_key=self.api_key
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=STATUTE_SEPARATORS,
            length_function=len
        )
        
        # å­˜å‚¨ä¸åŒé¢†åŸŸçš„å‘é‡åº“
        self.vectorstores = {}
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰"""
        if not text:
            return ""
        
        # æ›¿æ¢å¤šä¸ªç©ºæ ¼/æ¢è¡Œä¸ºå•ä¸ª
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        # è§„èŒƒåŒ–æ ‡ç‚¹
        text = text.replace('ï¼', '.').replace('ï¼Œ', 'ï¼Œ').replace('ã€‚', 'ã€‚')
        
        return text.strip()
    
    def generate_doc_id(self, content: str, source: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID"""
        hash_input = f"{source}:{content[:100]}"
        return hashlib.md5(hash_input.encode()).hexdigest()[:12]
    
    def load_statutes(self, file_path: str, domain_key: str = 'criminal') -> List[Document]:
        """
        åŠ è½½æ³•æ¡æ•°æ® - æŒ‰å•ä¸ªæ¡æ¬¾è¿›è¡Œç»†ç²’åº¦åˆ†å—
        æ¯ä¸ªæ¡æ¬¾å•ç‹¬ä½œä¸ºä¸€ä¸ªæ–‡æ¡£ï¼Œç¡®ä¿æ£€ç´¢ç²¾åº¦
        
        Args:
            file_path: æ³•å¾‹æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            domain_key: æ³•å¾‹é¢†åŸŸæ ‡è¯†ç¬¦ï¼ˆå¦‚'criminal', 'civil'ç­‰ï¼‰
        """
        domain_info = self.LEGAL_DOMAINS.get(domain_key, {})
        domain_name = domain_info.get('name', domain_key)
        
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½ {domain_name} æ³•æ¡: {file_path}")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # é¢„å¤„ç†ï¼šè§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'ã€€', ' ', text)  # å…¨è§’ç©ºæ ¼è½¬åŠè§’
        text = re.sub(r'\t', ' ', text)
        
        # æŒ‰æ¡æ¬¾åˆ†å‰² - ä½¿ç”¨æ­£åˆ™åŒ¹é… "ç¬¬Xæ¡" å¼€å¤´çš„æ®µè½
        # åŒ¹é…æ ¼å¼ï¼šç¬¬Xæ¡ æˆ– ç¬¬Xæ¡ä¹‹X
        article_pattern = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]*)?)\s*'
        
        # åˆ†å‰²æ–‡æœ¬ï¼Œä¿ç•™åˆ†éš”ç¬¦
        parts = re.split(f'({article_pattern})', text)
        
        current_article = None
        current_content = []
        chapter_info = ""  # å½“å‰ç« èŠ‚ä¿¡æ¯
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
            
            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ "ç¬¬å››ç«  ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©ã€æ°‘ä¸»æƒåˆ©ç½ª"ï¼‰
            chapter_match = re.match(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+(?:ç¼–|ç« |èŠ‚)[^\n]*)', part)
            if chapter_match:
                chapter_info = chapter_match.group(1)
                continue
            
            # æ£€æµ‹æ¡æ¬¾å·
            article_match = re.match(article_pattern, part)
            if article_match:
                # ä¿å­˜ä¹‹å‰çš„æ¡æ¬¾
                if current_article and current_content:
                    content_text = ' '.join(current_content)
                    content_text = self.clean_text(content_text)
                    
                    if len(content_text) > 20:  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                        doc_id = self.generate_doc_id(content_text, domain_name)
                        
                        # åœ¨å†…å®¹å‰æ·»åŠ æ¡æ¬¾å·å’Œç« èŠ‚ä¿¡æ¯ï¼Œå¢å¼ºæ£€ç´¢æ•ˆæœ
                        full_content = f"{current_article} {content_text}"
                        if chapter_info:
                            full_content = f"ã€{chapter_info}ã€‘{full_content}"
                        
                        docs.append(Document(
                            page_content=full_content,
                            metadata={
                                "source": domain_info.get('description', "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•"),
                                "domain": domain_key,
                                "domain_name": domain_name,
                                "type": "statute",
                                "article": current_article,
                                "chapter": chapter_info,
                                "doc_id": doc_id,
                                "chunk_index": len(docs)
                            }
                        ))
                
                # å¼€å§‹æ–°æ¡æ¬¾
                current_article = article_match.group(1)
                current_content = []
            else:
                # éæ¡æ¬¾å·çš„å†…å®¹ï¼Œæ·»åŠ åˆ°å½“å‰æ¡æ¬¾
                if current_article:
                    current_content.append(part)
                elif not any(kw in part for kw in ['ç›®å½•', 'ç›®ã€€ã€€å½•', 'ç¬¬ä¸€ç¼–', 'ç¬¬äºŒç¼–']):
                    # å‰è¨€éƒ¨åˆ†ï¼ˆä¿®è®¢å†å²ç­‰ï¼‰ï¼Œä¹Ÿå¯ä»¥ä¿ç•™
                    if len(part) > 100:
                        doc_id = self.generate_doc_id(part[:100], f"{domain_name}å‰è¨€")
                        docs.append(Document(
                            page_content=self.clean_text(part),
                            metadata={
                                "source": domain_info.get('description', domain_name),
                                "domain": domain_key,
                                "domain_name": domain_name,
                                "type": "statute",
                                "article": "å‰è¨€",
                                "chapter": "",
                                "doc_id": doc_id,
                                "chunk_index": len(docs)
                            }
                        ))
        
        # ä¿å­˜æœ€åä¸€ä¸ªæ¡æ¬¾
        if current_article and current_content:
            content_text = ' '.join(current_content)
            content_text = self.clean_text(content_text)
            
            if len(content_text) > 20:
                doc_id = self.generate_doc_id(content_text, "åˆ‘æ³•")
                full_content = f"{current_article} {content_text}"
                if chapter_info:
                    full_content = f"ã€{chapter_info}ã€‘{full_content}"
                
                docs.append(Document(
                    page_content=full_content,
                    metadata={
                        "source": domain_info.get('description', domain_name),
                        "domain": domain_key,
                        "domain_name": domain_name,
                        "type": "statute",
                        "article": current_article,
                        "chapter": chapter_info,
                        "doc_id": doc_id,
                        "chunk_index": len(docs)
                    }
                ))
        
        # å¯¹äºè¿‡é•¿çš„æ¡æ¬¾ï¼Œè¿›è¡ŒäºŒæ¬¡åˆ†å‰²
        final_docs = []
        max_length = 800  # å•ä¸ªchunkæœ€å¤§é•¿åº¦
        
        for doc in docs:
            if len(doc.page_content) > max_length:
                # æŒ‰å¥å­åˆ†å‰²é•¿æ¡æ¬¾
                sentences = re.split(r'([ã€‚ï¼›])', doc.page_content)
                
                current_chunk = ""
                chunk_index = 0
                
                for i in range(0, len(sentences), 2):
                    sentence = sentences[i]
                    punct = sentences[i+1] if i+1 < len(sentences) else ""
                    
                    if len(current_chunk) + len(sentence) + len(punct) > max_length:
                        if current_chunk:
                            new_doc = Document(
                                page_content=current_chunk.strip(),
                                metadata={
                                    **doc.metadata,
                                    "article": f"{doc.metadata['article']}(ç¬¬{chunk_index+1}éƒ¨åˆ†)",
                                    "doc_id": f"{doc.metadata['doc_id']}_{chunk_index}"
                                }
                            )
                            final_docs.append(new_doc)
                            chunk_index += 1
                        current_chunk = sentence + punct
                    else:
                        current_chunk += sentence + punct
                
                if current_chunk.strip():
                    new_doc = Document(
                        page_content=current_chunk.strip(),
                        metadata={
                            **doc.metadata,
                            "article": f"{doc.metadata['article']}(ç¬¬{chunk_index+1}éƒ¨åˆ†)" if chunk_index > 0 else doc.metadata['article'],
                            "doc_id": f"{doc.metadata['doc_id']}_{chunk_index}" if chunk_index > 0 else doc.metadata['doc_id']
                        }
                    )
                    final_docs.append(new_doc)
            else:
                final_docs.append(doc)
        
        print(f"âœ… åŠ è½½æ³•æ¡å®Œæˆï¼Œå…± {len(final_docs)} ä¸ªæ–‡æ¡£å—")
        
        # æ‰“å°ä¸€äº›ç¤ºä¾‹
        sample_articles = ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡", "ç¬¬äºŒç™¾å…­åå››æ¡", "ç¬¬äºŒåæ¡", "ç¬¬å…­åä¸ƒæ¡"]
        print("ğŸ“‹ å…³é”®æ³•æ¡ç¤ºä¾‹:")
        for doc in final_docs[:]:
            if doc.metadata.get("article") in sample_articles:
                print(f"   - {doc.metadata['article']}: {doc.page_content[:60]}...")
        
        return final_docs
    
    # ==================== æ–¹æ¡ˆE: ç½ªåå…³é”®è¯ç´¢å¼• ====================
    def _extract_crime_keywords(self, content: str) -> List[str]:
        """
        ä»æ³•æ¡å†…å®¹ä¸­æå–ç½ªåå…³é”®è¯
        ç”¨äºå¢å¼ºæ£€ç´¢æ•ˆæœ
        """
        crime_patterns = {
            # ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©ç½ª
            "æ•…æ„æ€äºº": ["æ•…æ„æ€äºº", "æ€å®³", "æ€äºº"],
            "æ•…æ„ä¼¤å®³": ["æ•…æ„ä¼¤å®³", "ä¼¤å®³", "è½»ä¼¤", "é‡ä¼¤"],
            "å¼ºå¥¸": ["å¼ºå¥¸", "å¥¸æ·«", "å¦‡å¥³"],
            "ç»‘æ¶": ["ç»‘æ¶", "äººè´¨", "å‹’ç´¢"],
            "æ‹å–": ["æ‹å–", "æ”¶ä¹°", "å¦‡å¥³å„¿ç«¥"],
            "éæ³•æ‹˜ç¦": ["éæ³•æ‹˜ç¦", "å‰¥å¤ºäººèº«è‡ªç”±"],
            
            # ä¾µçŠ¯è´¢äº§ç½ª
            "ç›—çªƒ": ["ç›—çªƒ", "å·ç›—", "çªƒå–"],
            "æŠ¢åŠ«": ["æŠ¢åŠ«", "æš´åŠ›åŠ«å–", "æŒæ¢°æŠ¢åŠ«"],
            "è¯ˆéª—": ["è¯ˆéª—", "éª—å–", "è™šæ„äº‹å®"],
            "æŠ¢å¤º": ["æŠ¢å¤º", "å…¬ç„¶å¤ºå–"],
            "æ•²è¯ˆå‹’ç´¢": ["æ•²è¯ˆå‹’ç´¢", "å¨èƒ", "è¦æŒŸè´¢ç‰©"],
            "ä¾µå ": ["ä¾µå ", "ä»£ä¸ºä¿ç®¡", "æ‹’ä¸é€€è¿˜"],
            
            # å±å®³å…¬å…±å®‰å…¨ç½ª
            "äº¤é€šè‚‡äº‹": ["äº¤é€šè‚‡äº‹", "äº¤é€šäº‹æ•…", "é€ƒé€¸"],
            "å±é™©é©¾é©¶": ["å±é™©é©¾é©¶", "é†‰é…’é©¾é©¶", "è¿½é€ç«é©¶"],
            "æ”¾ç«": ["æ”¾ç«", "çºµç«", "å±å®³å…¬å…±å®‰å…¨"],
            "çˆ†ç‚¸": ["çˆ†ç‚¸", "ç‚¸å¼¹", "å±å®³å…¬å…±å®‰å…¨"],
            
            # å¦¨å®³ç¤¾ä¼šç®¡ç†ç§©åºç½ª
            "èšä¼—æ–—æ®´": ["èšä¼—æ–—æ®´", "æ–—æ®´", "é¦–è¦åˆ†å­"],
            "å¯»è¡…æ»‹äº‹": ["å¯»è¡…æ»‹äº‹", "éšæ„æ®´æ‰“", "å¼ºæ‹¿ç¡¬è¦"],
            "èµŒåš": ["èµŒåš", "å¼€è®¾èµŒåœº"],
            "ä¼ªè¯": ["ä¼ªè¯", "è™šå‡è¯æ˜", "è¯äºº"],
            "åŒ…åº‡": ["åŒ…åº‡", "çªè—", "éšç’"],
            
            # è´ªæ±¡è´¿èµ‚ç½ª
            "è´ªæ±¡": ["è´ªæ±¡", "ä¾µå", "å›½å®¶å·¥ä½œäººå‘˜"],
            "å—è´¿": ["å—è´¿", "æ”¶å—è´¢ç‰©", "è°‹å–åˆ©ç›Š"],
            "è¡Œè´¿": ["è¡Œè´¿", "ç»™äºˆè´¢ç‰©"],
            
            # èµ°ç§è´©æ¯’ç½ª
            "èµ°ç§": ["èµ°ç§", "æ­¦å™¨å¼¹è¯", "æ ¸ææ–™"],
            "è´©æ¯’": ["è´©æ¯’", "æ¯’å“", "èµ°ç§è´©å–è¿è¾“"],
            
            # åˆ‘ç½šåˆ¶åº¦
            "æ­£å½“é˜²å«": ["æ­£å½“é˜²å«", "é˜²å«è¿‡å½“", "ä¸æ³•ä¾µå®³"],
            "ç´§æ€¥é¿é™©": ["ç´§æ€¥é¿é™©", "é¿å…å±é™©"],
            "è‡ªé¦–": ["è‡ªé¦–", "è‡ªåŠ¨æŠ•æ¡ˆ", "å¦‚å®ä¾›è¿°"],
            "ç«‹åŠŸ": ["ç«‹åŠŸ", "é‡å¤§ç«‹åŠŸ", "æ£€ä¸¾æ­å‘"],
            "ç´¯çŠ¯": ["ç´¯çŠ¯", "åˆ‘ç½šæ‰§è¡Œå®Œæ¯•", "äº”å¹´ä»¥å†…"],
            "ç¼“åˆ‘": ["ç¼“åˆ‘", "å®£å‘Šç¼“åˆ‘", "è€ƒéªŒæœŸ"],
            "å‡åˆ‘": ["å‡åˆ‘", "æ‚”æ”¹è¡¨ç°"],
            "å‡é‡Š": ["å‡é‡Š", "æœåˆ‘æœŸé—´"],
        }
        
        found_keywords = []
        content_lower = content.lower()
        
        for crime, patterns in crime_patterns.items():
            for pattern in patterns:
                if pattern in content_lower:
                    found_keywords.append(crime)
                    break
        
        return list(set(found_keywords))
    
    def load_cail_cases(self, file_path: str, limit: int = None) -> List[Document]:
        """
        åŠ è½½CAILæ¡ˆä¾‹æ•°æ®
        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. JSONæ•°ç»„æ ¼å¼: [{...}, {...}]
        2. JSONLæ ¼å¼: æ¯è¡Œä¸€æ¡JSON
        """
        limit = limit or CAIL_CASE_LIMIT
        print(f"âš–ï¸ æ­£åœ¨åŠ è½½ CAIL æ¡ˆä¾‹: {file_path} (é™åˆ¶ {limit} æ¡)")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        # å…ˆå°è¯•ä½œä¸ºJSONæ•°ç»„æ ¼å¼è¯»å–
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if content.startswith('['):
                # JSONæ•°ç»„æ ¼å¼
                print("   [JSONæ•°ç»„æ ¼å¼]")
                cases = json.loads(content)
                
                if isinstance(cases, list):
                    total_cases = len(cases)
                    print(f"   æ£€æµ‹åˆ° {total_cases} æ¡æ¡ˆä¾‹")
                    
                    for case_idx, data in enumerate(tqdm(cases[:limit], desc="åŠ è½½æ¡ˆä¾‹", total=min(len(cases), limit))):
                        try:
                            # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ•°æ®ç»“æ„
                            if isinstance(data, dict):
                                # ç»“æ„1: {"fact": "...", "meta": {...}}
                                fact = data.get('fact', '')
                                meta = data.get('meta', {})
                                
                                # ç»“æ„2: {"fact": "...", "accusation": [...], "law_articles": [...]}
                                if 'accusation' in data and not meta:
                                    meta = {
                                        'accusation': data.get('accusation', []),
                                        'relevant_articles': data.get('law_articles', []),
                                        'term_of_imprisonment': {
                                            'imprisonment': data.get('prison_term', 0),
                                            'death_penalty': False,
                                            'life_imprisonment': False
                                        },
                                        'punish_of_money': data.get('fine', 0)
                                    }
                            else:
                                continue
                            
                            # æå–æ¡ˆæƒ…äº‹å®
                            if not fact or len(fact) < 50:
                                continue
                            
                            fact = self.clean_text(fact)
                            
                            # æå–å…ƒæ•°æ®
                            accusation = meta.get('accusation', [])
                            relevant_articles = meta.get('relevant_articles', [])
                            term_of_imprisonment = meta.get('term_of_imprisonment', {})
                            
                            # æ„é€ ç»“æ„åŒ–å†…å®¹
                            case_content = f"ã€æ¡ˆæƒ…äº‹å®ã€‘\n{fact}"
                            
                            # æ·»åŠ ç½ªåä¿¡æ¯
                            if accusation:
                                acc_str = "ã€".join(accusation) if isinstance(accusation, list) else str(accusation)
                                case_content += f"\nã€æŒ‡æ§ç½ªåã€‘{acc_str}"
                            
                            # æ·»åŠ æ³•æ¡ä¿¡æ¯
                            if relevant_articles:
                                articles_str = "ã€".join(str(a) for a in relevant_articles) if isinstance(relevant_articles, list) else str(relevant_articles)
                                case_content += f"\nã€ç›¸å…³æ³•æ¡ã€‘ç¬¬{articles_str}æ¡"
                            
                            # æ·»åŠ åˆ¤å†³ç»“æœ
                            if term_of_imprisonment:
                                death = term_of_imprisonment.get('death_penalty', False)
                                life = term_of_imprisonment.get('life_imprisonment', False)
                                imprisonment = term_of_imprisonment.get('imprisonment', 0)
                                
                                if death:
                                    sentence = "æ­»åˆ‘"
                                elif life:
                                    sentence = "æ— æœŸå¾’åˆ‘"
                                elif imprisonment > 0:
                                    sentence = f"æœ‰æœŸå¾’åˆ‘{imprisonment}ä¸ªæœˆ"
                                else:
                                    sentence = "å…¶ä»–åˆ‘ç½š"
                                
                                case_content += f"\nã€åˆ¤å†³ç»“æœã€‘{sentence}"
                            
                            doc_id = self.generate_doc_id(fact[:100], "CAIL")
                            
                            docs.append(Document(
                                page_content=case_content,
                                metadata={
                                    "source": "CAIL2018å¸æ³•æ¡ˆä¾‹æ•°æ®é›†",
                                    "type": "case",
                                    "accusation": ",".join(accusation) if accusation else "æœªçŸ¥",
                                    "articles": ",".join(str(a) for a in relevant_articles) if relevant_articles else "æœªçŸ¥",
                                    "doc_id": doc_id,
                                    "case_index": case_idx
                                }
                            ))
                        
                        except Exception as e:
                            continue
                    
                    print(f"âœ… åŠ è½½æ¡ˆä¾‹å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£")
                    return docs
        
        except (json.JSONDecodeError, ValueError):
            pass  # ä¸æ˜¯JSONæ•°ç»„æ ¼å¼ï¼Œå°è¯•JSONLæ ¼å¼
        
        # å¦‚æœä¸æ˜¯JSONæ•°ç»„æ ¼å¼ï¼Œå°è¯•JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€æ¡JSONï¼‰
        print("   [JSONLæ ¼å¼]")
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(tqdm(f, desc="åŠ è½½æ¡ˆä¾‹", total=limit)):
                if line_num >= limit:
                    break
                
                try:
                    data = json.loads(line)
                    
                    # æå–æ¡ˆæƒ…äº‹å®
                    fact = data.get('fact', '')
                    if not fact or len(fact) < 50:
                        continue
                    
                    fact = self.clean_text(fact)
                    
                    # æå–å…ƒæ•°æ®
                    meta = data.get('meta', {})
                    accusation = meta.get('accusation', [])
                    relevant_articles = meta.get('relevant_articles', [])
                    term_of_imprisonment = meta.get('term_of_imprisonment', {})
                    
                    # æ„é€ ç»“æ„åŒ–å†…å®¹
                    case_content = f"ã€æ¡ˆæƒ…äº‹å®ã€‘\n{fact}"
                    
                    if accusation:
                        case_content += f"\nã€æŒ‡æ§ç½ªåã€‘{'ã€'.join(accusation)}"
                    
                    if term_of_imprisonment:
                        death = term_of_imprisonment.get('death_penalty', False)
                        life = term_of_imprisonment.get('life_imprisonment', False)
                        imprisonment = term_of_imprisonment.get('imprisonment', 0)
                        
                        if death:
                            sentence = "æ­»åˆ‘"
                        elif life:
                            sentence = "æ— æœŸå¾’åˆ‘"
                        elif imprisonment > 0:
                            sentence = f"æœ‰æœŸå¾’åˆ‘{imprisonment}ä¸ªæœˆ"
                        else:
                            sentence = "å…¶ä»–åˆ‘ç½š"
                        
                        case_content += f"\nã€åˆ¤å†³ç»“æœã€‘{sentence}"
                    
                    doc_id = self.generate_doc_id(fact, "CAIL")
                    
                    docs.append(Document(
                        page_content=case_content,
                        metadata={
                            "source": "CAIL2018å¸æ³•æ¡ˆä¾‹æ•°æ®é›†",
                            "type": "case",
                            "accusation": ",".join(accusation) if accusation else "æœªçŸ¥",
                            "articles": ",".join(str(a) for a in relevant_articles),
                            "doc_id": doc_id,
                            "case_index": line_num
                        }
                    ))
                
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    continue
        
        print(f"âœ… åŠ è½½æ¡ˆä¾‹å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£")
        return docs
    
    def load_qa_pairs(self, file_path: str) -> List[Document]:
        """
        åŠ è½½QAå¯¹æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        æ ¼å¼: JSON Linesï¼Œæ¯è¡Œ {"question": "...", "answer": "..."}
        """
        print(f"â“ æ­£åœ¨åŠ è½½QAå¯¹: {file_path}")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                try:
                    data = json.loads(line)
                    question = self.clean_text(data.get('question', ''))
                    answer = self.clean_text(data.get('answer', ''))
                    
                    if question and answer:
                        content = f"ã€é—®é¢˜ã€‘{question}\nã€å›ç­”ã€‘{answer}"
                        doc_id = self.generate_doc_id(content, "QA")
                        
                        docs.append(Document(
                            page_content=content,
                            metadata={
                                "source": "æ³•å¾‹QAæ•°æ®é›†",
                                "type": "qa",
                                "doc_id": doc_id,
                                "qa_index": line_num
                            }
                        ))
                except:
                    continue
        
        print(f"âœ… åŠ è½½QAå¯¹å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£")
        return docs
    
    def build_vector_db(self, docs: List[Document], batch_size: int = EMBED_BATCH_SIZE) -> Chroma:
        """
        æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        ä½¿ç”¨æ‰¹é‡å¤„ç†é¿å…APIè¶…æ—¶
        """
        return self.build_vector_db_with_path(docs, DB_PATH, batch_size)
    
    def build_vector_db_with_path(self, docs: List[Document], db_path: str, batch_size: int = EMBED_BATCH_SIZE) -> Chroma:
        """
        æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæŒ‡å®šè·¯å¾„ï¼‰
        ç”¨äºæ”¯æŒå¤šé¢†åŸŸçš„ç‹¬ç«‹å‘é‡åº“
        
        Args:
            docs: å¾…å‘é‡åŒ–çš„æ–‡æ¡£åˆ—è¡¨
            db_path: æ•°æ®åº“ä¿å­˜è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if not docs:
            raise ValueError("âŒ æ²¡æœ‰æ–‡æ¡£å¯ä¾›å‘é‡åŒ–ï¼")
        
        print(f"ğŸ“¦ å‡†å¤‡å‘é‡åŒ– {len(docs)} æ¡æ–‡æ¡£åˆ° {db_path}...")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(db_path, exist_ok=True)
        
        # åˆ é™¤æ—§æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…ç‰ˆæœ¬å†²çª
        if os.path.exists(db_path):
            try:
                import shutil
                print(f"ğŸ—‘ï¸ æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“ {db_path}...")
                shutil.rmtree(db_path)
                time.sleep(0.5)  # ç­‰å¾…æ–‡ä»¶ç³»ç»Ÿå®Œæˆåˆ é™¤
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†æ—§æ•°æ®åº“å¤±è´¥: {e}")
        
        os.makedirs(db_path, exist_ok=True)
        
        vectorstore = None
        
        # æ‰¹é‡å¤„ç†
        def is_rate_limit_error(err: Exception) -> bool:
            message = str(err).lower()
            return ("rate limit" in message or "rpm limit" in message or "429" in message or "too many" in message)

        def get_batch_sleep_seconds(batch_docs) -> float:
            rpm_wait = 0.0
            if EMBED_RPM_LIMIT > 0:
                rpm_wait = 60.0 / EMBED_RPM_LIMIT
            tpm_wait = 0.0
            if EMBED_TPM_LIMIT > 0:
                approx_tokens = sum(len(doc.page_content) for doc in batch_docs)
                tpm_wait = (approx_tokens / EMBED_TPM_LIMIT) * 60.0
            return max(EMBED_SLEEP_SECONDS, rpm_wait, tpm_wait)

        for i in tqdm(range(0, len(docs), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
            batch = docs[i:i + batch_size]
            retries = 0
            
            while True:
                try:
                    if vectorstore is None:
                        # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„å‘é‡åº“
                        vectorstore = Chroma.from_documents(
                            documents=batch,
                            embedding=self.embeddings,
                            persist_directory=db_path
                        )
                    else:
                        # åç»­æ‰¹ï¼šæ·»åŠ åˆ°ç°æœ‰å‘é‡åº“
                        vectorstore.add_documents(batch)
                    
                    # é¿å…APIé€Ÿç‡é™åˆ¶
                    sleep_seconds = get_batch_sleep_seconds(batch)
                    if sleep_seconds > 0:
                        time.sleep(sleep_seconds)
                    break
                    
                except Exception as e:
                    if is_rate_limit_error(e):
                        retries += 1
                        if retries > EMBED_MAX_RETRIES:
                            raise RuntimeError(
                                "è§¦å‘RPMé™åˆ¶ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚"
                                "è¯·å®Œæˆè´¦å·å®åè®¤è¯æˆ–å¢å¤§ç­‰å¾…æ—¶é—´åé‡è¯•ã€‚"
                            ) from e
                        backoff = min(EMBED_BACKOFF_SECONDS * (2 ** (retries - 1)), EMBED_BACKOFF_MAX_SECONDS)
                        print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} è§¦å‘é™é€Ÿï¼Œç­‰å¾… {backoff:.0f}s åé‡è¯•...")
                        time.sleep(backoff)
                        continue
                    
                    print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} å¤„ç†å¤±è´¥: {e}")
                    time.sleep(2)  # å‡ºé”™åå¤šç­‰å¾…
                    break
        
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼å·²ä¿å­˜è‡³ {DB_PATH}")
        return vectorstore
    
    def get_statistics(self, docs: List[Document]) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_docs": len(docs),
            "by_type": {},
            "avg_length": 0,
            "total_chars": 0
        }
        
        for doc in docs:
            doc_type = doc.metadata.get("type", "unknown")
            stats["by_type"][doc_type] = stats["by_type"].get(doc_type, 0) + 1
            stats["total_chars"] += len(doc.page_content)
        
        stats["avg_length"] = stats["total_chars"] / len(docs) if docs else 0
        
        return stats


def build_vector_db(multi_domain: bool = True):
    """ä¸»å‡½æ•°ï¼šæ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒå•é¢†åŸŸæˆ–å¤šé¢†åŸŸï¼‰
    
    Args:
        multi_domain: æ˜¯å¦ä½¿ç”¨å¤šé¢†åŸŸç³»ç»Ÿï¼Œé»˜è®¤ä¸ºTrue
    """
    processor = LegalDataProcessor()
    
    if multi_domain:
        print("\nğŸŒ å¤šé¢†åŸŸæ¨¡å¼å¯åŠ¨")
        print("="*60)
        
        # ä¸ºæ¯ä¸ªé¢†åŸŸå»ºç«‹ç‹¬ç«‹çš„å‘é‡åº“
        for domain_key, domain_info in processor.LEGAL_DOMAINS.items():
            domain_name = domain_info['name']
            domain_file = domain_info['file']
            
            file_path = os.path.join(DATA_PATH, domain_file)
            
            if not os.path.exists(file_path):
                print(f"âš ï¸  {domain_name} æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            print(f"\nğŸ“š åŠ è½½ {domain_name}...")
            
            domain_docs = []
            
            # åŠ è½½è¯¥é¢†åŸŸçš„æ³•å¾‹æ–‡æœ¬
            domain_docs.extend(processor.load_statutes(file_path, domain_key=domain_key))
            
            # ä»…ä¸ºåˆ‘æ³•é¢†åŸŸåŠ è½½æ¡ˆä¾‹å’ŒQAå¯¹
            if domain_key == 'criminal':
                cail_path = get_cail_file_path()
                domain_docs.extend(processor.load_cail_cases(cail_path, limit=CAIL_CASE_LIMIT))
                
                qa_path = os.path.join(DATA_PATH, "legal_qa.json")
                if os.path.exists(qa_path):
                    domain_docs.extend(processor.load_qa_pairs(qa_path))
            
            if domain_docs:
                stats = processor.get_statistics(domain_docs)
                print(f"  âœ… {domain_name}: {stats['total_docs']} æ–‡æ¡£, {stats['total_chars']:,} å­—ç¬¦")
                
                # ä¸ºè¯¥é¢†åŸŸåˆ›å»ºç‹¬ç«‹çš„å‘é‡åº“
                domain_db_path = os.path.join(DB_PATH, domain_key)
                processor.build_vector_db_with_path(domain_docs, domain_db_path)
                processor.vectorstores[domain_key] = domain_db_path
            else:
                print(f"  âš ï¸  {domain_name}: æ— æ•°æ®")
        
        print("\nâœ… å¤šé¢†åŸŸå‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print(f"   å·²æ„å»ºçš„é¢†åŸŸ: {list(processor.vectorstores.keys())}")
        
    else:
        # å•é¢†åŸŸæ¨¡å¼ï¼ˆä¿ç•™å‘åå…¼å®¹æ€§ï¼‰
        print("\nğŸ” å•é¢†åŸŸæ¨¡å¼å¯åŠ¨ - ä»…åŠ è½½åˆ‘æ³•")
        print("="*60)
        
        all_docs = []
        
        # åŠ è½½æ³•æ¡
        statute_path = os.path.join(DATA_PATH, "criminal_code.txt")
        statute_docs = processor.load_statutes(statute_path)
        all_docs.extend(statute_docs)
        
        # åŠ è½½CAILæ¡ˆä¾‹
        cail_path = get_cail_file_path()
        case_docs = processor.load_cail_cases(cail_path, limit=CAIL_CASE_LIMIT)
        all_docs.extend(case_docs)
        
        # åŠ è½½QAå¯¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        qa_path = os.path.join(DATA_PATH, "legal_qa.json")
        if os.path.exists(qa_path):
            qa_docs = processor.load_qa_pairs(qa_path)
            all_docs.extend(qa_docs)
        
        if not all_docs:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ data/raw ç›®å½•ã€‚")
            return
        
        # 2. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_statistics(all_docs)
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_docs']}")
        print(f"   æŒ‰ç±»å‹åˆ†å¸ƒ: {stats['by_type']}")
        print(f"   å¹³å‡é•¿åº¦: {stats['avg_length']:.1f} å­—ç¬¦")
        print(f"   æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
        
        # 3. æ„å»ºå‘é‡æ•°æ®åº“
        processor.build_vector_db(all_docs)


if __name__ == "__main__":
    if not SILICONFLOW_API_KEY:
        print("âŒ è¯·å…ˆè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼")
        print("   Windows: set SILICONFLOW_API_KEY=your_key")
        print("   Linux/Mac: export SILICONFLOW_API_KEY=your_key")
    else:
        build_vector_db(multi_domain=True)  # å¯ç”¨å¤šé¢†åŸŸæ¨¡å¼
