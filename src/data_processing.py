"""
Juris-RAG æ•°æ®å¤„ç†æ¨¡å—
è´Ÿè´£æ³•å¾‹æ–‡æœ¬çš„åŠ è½½ã€æ¸…æ´—ã€åˆ†å—å’Œå‘é‡åŒ–
"""
import os
import re
import json
import time
import hashlib
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm

# LangChain ç»„ä»¶
try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain_classic.schema import Document
from langchain_community.vectorstores import Chroma
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DATA_PATH, DB_PATH, EMBEDDING_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, CHUNK_SIZE, CHUNK_OVERLAP, 
        CAIL_CASE_LIMIT, STATUTE_SEPARATORS,
        EMBED_RPM_LIMIT, EMBED_TPM_LIMIT,
        EMBED_BATCH_SIZE, EMBED_SLEEP_SECONDS, EMBED_MAX_RETRIES,
        EMBED_BACKOFF_SECONDS, EMBED_BACKOFF_MAX_SECONDS
    )
    from src.cail_adapter import get_cail_file_path
except ImportError:
    # é»˜è®¤é…ç½®
    DATA_PATH = "./data/raw"
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    CAIL_CASE_LIMIT = 20000
    STATUTE_SEPARATORS = ["\nç¬¬", "\n\n", "\n", "ã€‚", "ï¼›"]
    EMBED_RPM_LIMIT = 2000
    EMBED_TPM_LIMIT = 500000
    EMBED_BATCH_SIZE = 20
    EMBED_SLEEP_SECONDS = 0.1
    EMBED_MAX_RETRIES = 5
    EMBED_BACKOFF_SECONDS = 10
    EMBED_BACKOFF_MAX_SECONDS = 120
    
    def get_cail_file_path():
        from pathlib import Path
        data_dir = Path(DATA_PATH)
        trimmed = data_dir / "cail_cases_20k.json"
        original = data_dir / "cail_cases.json"
        return str(trimmed if trimmed.exists() else original)


class LegalDataProcessor:
    """æ³•å¾‹æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or SILICONFLOW_API_KEY
        self.base_url = base_url or SILICONFLOW_BASE_URL
        
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        # åˆå§‹åŒ–Embeddingæ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=self.base_url,
            openai_api_key=self.api_key
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=STATUTE_SEPARATORS,
            length_function=len
        )
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦ç­‰"""
        if not text:
            return ""
        
        # æ›¿æ¢å¤šä¸ªç©ºæ ¼/æ¢è¡Œä¸ºå•ä¸ª
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        # è§„èŒƒåŒ–æ ‡ç‚¹
        text = text.replace('ï¼', '.').replace('ï¼Œ', 'ï¼Œ').replace('ã€‚', 'ã€‚')
        
        return text.strip()
    
    def generate_doc_id(self, content: str, source: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID"""
        hash_input = f"{source}:{content[:100]}"
        return hashlib.md5(hash_input.encode()).hexdigest()[:12]
    
    def load_statutes(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ³•æ¡æ•°æ®
        æ”¯æŒæŒ‰æ¡æ¬¾è¿›è¡Œæ™ºèƒ½åˆ†å‰²
        """
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ³•æ¡: {file_path}")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # æ¸…æ´—æ–‡æœ¬
        text = self.clean_text(text)
        
        # å°è¯•æŒ‰æ³•æ¡ç¼–å·åˆ†å‰²
        # åŒ¹é… "ç¬¬Xæ¡" æˆ– "ç¬¬XXæ¡" æ ¼å¼
        article_pattern = r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡[ä¹‹çš„]?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]*)'
        
        # ä½¿ç”¨åˆ†å‰²å™¨åˆ†å‰²
        chunks = self.text_splitter.split_text(text)
        
        for i, chunk in enumerate(chunks):
            # æå–æ³•æ¡ç¼–å·ï¼ˆå¦‚æœæœ‰ï¼‰
            article_match = re.search(article_pattern, chunk)
            article_num = article_match.group(1) if article_match else f"æ®µè½{i+1}"
            
            doc_id = self.generate_doc_id(chunk, "åˆ‘æ³•")
            
            docs.append(Document(
                page_content=chunk,
                metadata={
                    "source": "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•",
                    "type": "statute",
                    "article": article_num,
                    "doc_id": doc_id,
                    "chunk_index": i
                }
            ))
        
        print(f"âœ… åŠ è½½æ³•æ¡å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£å—")
        return docs
    
    def load_cail_cases(self, file_path: str, limit: int = None) -> List[Document]:
        """
        åŠ è½½CAILæ¡ˆä¾‹æ•°æ®
        æå–æ¡ˆæƒ…äº‹å®ã€ç½ªåã€ç›¸å…³æ³•æ¡ç­‰ä¿¡æ¯
        """
        limit = limit or CAIL_CASE_LIMIT
        print(f"âš–ï¸ æ­£åœ¨åŠ è½½ CAIL æ¡ˆä¾‹: {file_path} (é™åˆ¶ {limit} æ¡)")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(tqdm(f, desc="åŠ è½½æ¡ˆä¾‹", total=limit)):
                if line_num >= limit:
                    break
                
                try:
                    data = json.loads(line)
                    
                    # æå–æ¡ˆæƒ…äº‹å®
                    fact = data.get('fact', '')
                    if not fact or len(fact) < 50:  # è¿‡æ»¤è¿‡çŸ­çš„æ¡ˆæƒ…
                        continue
                    
                    fact = self.clean_text(fact)
                    
                    # æå–å…ƒæ•°æ®
                    meta = data.get('meta', {})
                    accusation = meta.get('accusation', [])
                    relevant_articles = meta.get('relevant_articles', [])
                    term_of_imprisonment = meta.get('term_of_imprisonment', {})
                    
                    # æ„é€ ç»“æ„åŒ–å†…å®¹
                    case_content = f"ã€æ¡ˆæƒ…äº‹å®ã€‘\n{fact}"
                    
                    # å¦‚æœæœ‰åˆ¤å†³ç»“æœï¼Œä¹ŸåŠ å…¥
                    if term_of_imprisonment:
                        death = term_of_imprisonment.get('death_penalty', False)
                        life = term_of_imprisonment.get('life_imprisonment', False)
                        imprisonment = term_of_imprisonment.get('imprisonment', 0)
                        
                        if death:
                            sentence = "æ­»åˆ‘"
                        elif life:
                            sentence = "æ— æœŸå¾’åˆ‘"
                        elif imprisonment > 0:
                            sentence = f"æœ‰æœŸå¾’åˆ‘{imprisonment}ä¸ªæœˆ"
                        else:
                            sentence = "å…¶ä»–åˆ‘ç½š"
                        
                        case_content += f"\nã€åˆ¤å†³ç»“æœã€‘{sentence}"
                    
                    doc_id = self.generate_doc_id(fact, "CAIL")
                    
                    docs.append(Document(
                        page_content=case_content,
                        metadata={
                            "source": "CAIL2018å¸æ³•æ¡ˆä¾‹æ•°æ®é›†",
                            "type": "case",
                            "accusation": ",".join(accusation) if accusation else "æœªçŸ¥",
                            "articles": ",".join(str(a) for a in relevant_articles),
                            "doc_id": doc_id,
                            "case_index": line_num
                        }
                    ))
                    
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†ç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {e}")
                    continue
        
        print(f"âœ… åŠ è½½æ¡ˆä¾‹å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£")
        return docs
    
    def load_qa_pairs(self, file_path: str) -> List[Document]:
        """
        åŠ è½½QAå¯¹æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        æ ¼å¼: JSON Linesï¼Œæ¯è¡Œ {"question": "...", "answer": "..."}
        """
        print(f"â“ æ­£åœ¨åŠ è½½QAå¯¹: {file_path}")
        docs = []
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                try:
                    data = json.loads(line)
                    question = self.clean_text(data.get('question', ''))
                    answer = self.clean_text(data.get('answer', ''))
                    
                    if question and answer:
                        content = f"ã€é—®é¢˜ã€‘{question}\nã€å›ç­”ã€‘{answer}"
                        doc_id = self.generate_doc_id(content, "QA")
                        
                        docs.append(Document(
                            page_content=content,
                            metadata={
                                "source": "æ³•å¾‹QAæ•°æ®é›†",
                                "type": "qa",
                                "doc_id": doc_id,
                                "qa_index": line_num
                            }
                        ))
                except:
                    continue
        
        print(f"âœ… åŠ è½½QAå¯¹å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æ¡£")
        return docs
    
    def build_vector_db(self, docs: List[Document], batch_size: int = EMBED_BATCH_SIZE) -> Chroma:
        """
        æ„å»ºå‘é‡æ•°æ®åº“
        ä½¿ç”¨æ‰¹é‡å¤„ç†é¿å…APIè¶…æ—¶
        """
        if not docs:
            raise ValueError("âŒ æ²¡æœ‰æ–‡æ¡£å¯ä¾›å‘é‡åŒ–ï¼")
        
        print(f"ğŸ“¦ å‡†å¤‡å‘é‡åŒ– {len(docs)} æ¡æ–‡æ¡£...")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(DB_PATH, exist_ok=True)
        
        # åˆ é™¤æ—§æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(DB_PATH) and os.listdir(DB_PATH):
            import shutil
            print("ğŸ—‘ï¸ æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“...")
            shutil.rmtree(DB_PATH)
            os.makedirs(DB_PATH)
        
        vectorstore = None
        
        # æ‰¹é‡å¤„ç†
        def is_rate_limit_error(err: Exception) -> bool:
            message = str(err).lower()
            return ("rate limit" in message or "rpm limit" in message or "429" in message or "too many" in message)

        def get_batch_sleep_seconds(batch_docs) -> float:
            rpm_wait = 0.0
            if EMBED_RPM_LIMIT > 0:
                rpm_wait = 60.0 / EMBED_RPM_LIMIT
            tpm_wait = 0.0
            if EMBED_TPM_LIMIT > 0:
                approx_tokens = sum(len(doc.page_content) for doc in batch_docs)
                tpm_wait = (approx_tokens / EMBED_TPM_LIMIT) * 60.0
            return max(EMBED_SLEEP_SECONDS, rpm_wait, tpm_wait)

        for i in tqdm(range(0, len(docs), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
            batch = docs[i:i + batch_size]
            retries = 0
            
            while True:
                try:
                    if vectorstore is None:
                        # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ–°çš„å‘é‡åº“
                        vectorstore = Chroma.from_documents(
                            documents=batch,
                            embedding=self.embeddings,
                            persist_directory=DB_PATH
                        )
                    else:
                        # åç»­æ‰¹ï¼šæ·»åŠ åˆ°ç°æœ‰å‘é‡åº“
                        vectorstore.add_documents(batch)
                    
                    # é¿å…APIé€Ÿç‡é™åˆ¶
                    sleep_seconds = get_batch_sleep_seconds(batch)
                    if sleep_seconds > 0:
                        time.sleep(sleep_seconds)
                    break
                    
                except Exception as e:
                    if is_rate_limit_error(e):
                        retries += 1
                        if retries > EMBED_MAX_RETRIES:
                            raise RuntimeError(
                                "è§¦å‘RPMé™åˆ¶ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚"
                                "è¯·å®Œæˆè´¦å·å®åè®¤è¯æˆ–å¢å¤§ç­‰å¾…æ—¶é—´åé‡è¯•ã€‚"
                            ) from e
                        backoff = min(EMBED_BACKOFF_SECONDS * (2 ** (retries - 1)), EMBED_BACKOFF_MAX_SECONDS)
                        print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} è§¦å‘é™é€Ÿï¼Œç­‰å¾… {backoff:.0f}s åé‡è¯•...")
                        time.sleep(backoff)
                        continue
                    
                    print(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} å¤„ç†å¤±è´¥: {e}")
                    time.sleep(2)  # å‡ºé”™åå¤šç­‰å¾…
                    break
        
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼å·²ä¿å­˜è‡³ {DB_PATH}")
        return vectorstore
    
    def get_statistics(self, docs: List[Document]) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_docs": len(docs),
            "by_type": {},
            "avg_length": 0,
            "total_chars": 0
        }
        
        for doc in docs:
            doc_type = doc.metadata.get("type", "unknown")
            stats["by_type"][doc_type] = stats["by_type"].get(doc_type, 0) + 1
            stats["total_chars"] += len(doc.page_content)
        
        stats["avg_length"] = stats["total_chars"] / len(docs) if docs else 0
        
        return stats


def build_vector_db():
    """ä¸»å‡½æ•°ï¼šæ„å»ºå‘é‡æ•°æ®åº“"""
    processor = LegalDataProcessor()
    
    # 1. åŠ è½½å„ç±»æ•°æ®
    all_docs = []
    
    # åŠ è½½æ³•æ¡
    statute_path = os.path.join(DATA_PATH, "criminal_code.txt")
    statute_docs = processor.load_statutes(statute_path)
    all_docs.extend(statute_docs)
    
    # åŠ è½½CAILæ¡ˆä¾‹
    cail_path = get_cail_file_path()
    case_docs = processor.load_cail_cases(cail_path, limit=CAIL_CASE_LIMIT)
    all_docs.extend(case_docs)
    
    # åŠ è½½QAå¯¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    qa_path = os.path.join(DATA_PATH, "legal_qa.json")
    if os.path.exists(qa_path):
        qa_docs = processor.load_qa_pairs(qa_path)
        all_docs.extend(qa_docs)
    
    if not all_docs:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ data/raw ç›®å½•ã€‚")
        return
    
    # 2. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_statistics(all_docs)
    print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_docs']}")
    print(f"   æŒ‰ç±»å‹åˆ†å¸ƒ: {stats['by_type']}")
    print(f"   å¹³å‡é•¿åº¦: {stats['avg_length']:.1f} å­—ç¬¦")
    print(f"   æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
    
    # 3. æ„å»ºå‘é‡æ•°æ®åº“
    processor.build_vector_db(all_docs)


if __name__ == "__main__":
    if not SILICONFLOW_API_KEY:
        print("âŒ è¯·å…ˆè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ï¼")
        print("   Windows: set SILICONFLOW_API_KEY=your_key")
        print("   Linux/Mac: export SILICONFLOW_API_KEY=your_key")
    else:
        build_vector_db()
