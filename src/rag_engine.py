"""
Juris-RAG æ ¸å¿ƒå¼•æ“æ¨¡å—
æ”¯æŒå¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ã€å¼•ç”¨æ¥æºæ˜¾ç¤ºã€æ‹’ç»ä¸ç¡®å®šå›ç­”
"""
import os
import re
from typing import List, Dict, Tuple, Optional, Generator
from dataclasses import dataclass

from langchain_community.vectorstores import Chroma
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# å°è¯•å¯¼å…¥é“¾ç»„ä»¶ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
try:
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError:
    try:
        from langchain_core.runnables import RunnablePassthrough
        # å¦‚æœæ²¡æœ‰ä¼ ç»Ÿchainsï¼Œä½¿ç”¨ç®€åŒ–å®ç°
        create_history_aware_retriever = None
        create_retrieval_chain = None
        create_stuff_documents_chain = None
    except ImportError:
        pass

try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain_community.docstore.document import Document

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DB_PATH, EMBEDDING_MODEL, LLM_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, RETRIEVAL_TOP_K, RETRIEVAL_SCORE_THRESHOLD,
        LLM_TEMPERATURE, LLM_MAX_TOKENS, MAX_HISTORY_TURNS,
        CONFIDENCE_THRESHOLD, UNCERTAIN_RESPONSE
    )
except ImportError:
    # é»˜è®¤å›é€€é…ç½®
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    LLM_MODEL = "Qwen/Qwen3-8B"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    RETRIEVAL_TOP_K = 8
    RETRIEVAL_SCORE_THRESHOLD = 0.2
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2048
    MAX_HISTORY_TURNS = 10
    CONFIDENCE_THRESHOLD = 0.4
    UNCERTAIN_RESPONSE = "æ ¹æ®ç°æœ‰æ³•å¾‹æ•°æ®åº“ï¼Œæˆ‘æ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚"

# ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹é…ç½® ====================
# éåˆ‘æ³•é¢†åŸŸå…³é”®è¯ - æ£€æµ‹åˆ°è¿™äº›è¯æ—¶è§¦å‘è¶…èŒƒå›´æ‹’ç»
OUT_OF_SCOPE_KEYWORDS = [
    # æ°‘æ³•ç›¸å…³
    "æ°‘æ³•å…¸", "åˆåŒæ³•", "å©šå§»æ³•", "ç»§æ‰¿æ³•", "ç‰©æƒæ³•", "ä¾µæƒè´£ä»»",
    "æ°‘äº‹çº çº·", "ç¦»å©š", "æŠšå…»æƒ", "é—äº§ç»§æ‰¿", "æˆ¿äº§çº çº·", "å€ºåŠ¡çº çº·",
    "å€Ÿæ¬¾åˆåŒ", "ç§ŸèµåˆåŒ", "ä¹°å–åˆåŒ", "åŠ³åŠ¡åˆåŒ",
    # å•†æ³•ç›¸å…³  
    "å…¬å¸æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ç¥¨æ®æ³•", "ç ´äº§æ³•",
    "è‚¡ç¥¨", "åŸºé‡‘", "æŠ•èµ„ç†è´¢", "ä¸Šå¸‚å…¬å¸", "è‘£äº‹ä¼š", "è‚¡ä¸œ",
    "å•†ä¸šç§˜å¯†", "çŸ¥è¯†äº§æƒ", "ä¸“åˆ©", "å•†æ ‡", "è‘—ä½œæƒ",
    # è¡Œæ”¿æ³•ç›¸å…³
    "è¡Œæ”¿å¤„ç½š", "è¡Œæ”¿å¤è®®", "è¡Œæ”¿è¯‰è®¼", "æ‹†è¿", "åœŸåœ°å¾æ”¶",
    "è¡Œæ”¿è®¸å¯", "è¡Œæ”¿å¼ºåˆ¶", "å…¬åŠ¡å‘˜", "äº‹ä¸šç¼–",
    # åŠ³åŠ¨æ³•ç›¸å…³
    "åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "ç¤¾ä¿", "å·¥ä¼¤", "åŠ³åŠ¨ä»²è£",
    "åŠ ç­è´¹", "å¹´å‡", "è¾é€€èµ”å¿", "äº”é™©ä¸€é‡‘",
    # å…¶ä»–éåˆ‘æ³•
    "ç¨æ³•", "æµ·å…³", "ç¯ä¿æ³•", "é£Ÿå“å®‰å…¨",
    "åŒ»ç–—çº çº·", "åŒ»æ‚£å…³ç³»", "äº¤é€šäº‹æ•…èµ”å¿"
]

# è¶…èŒƒå›´æ‹’ç»å“åº”æ¨¡æ¿
OUT_OF_SCOPE_RESPONSE = """æŠ±æ­‰ï¼Œæ‚¨çš„é—®é¢˜æ¶‰åŠ**{detected_domain}**é¢†åŸŸï¼Œä¸åœ¨æœ¬ç³»ç»Ÿçš„æœåŠ¡èŒƒå›´å†…ã€‚

**æœ¬ç³»ç»Ÿä¸“æ³¨äºä¸­å›½åˆ‘æ³•é¢†åŸŸ**ï¼ŒåŒ…æ‹¬ï¼š
- å„ç±»åˆ‘äº‹çŠ¯ç½ªçš„è®¤å®šä¸é‡åˆ‘
- åˆ‘äº‹è´£ä»»å¹´é¾„ã€è‡ªé¦–ã€ç«‹åŠŸç­‰æƒ…èŠ‚
- æ­£å½“é˜²å«ã€ç´§æ€¥é¿é™©ç­‰å…è´£äº‹ç”±
- åˆ‘äº‹æ¡ˆä¾‹çš„åˆ¤å†³å‚è€ƒ

**å»ºè®®**ï¼š
- æ°‘äº‹é—®é¢˜è¯·å’¨è¯¢æ°‘äº‹å¾‹å¸ˆæˆ–æŸ¥é˜…æ°‘æ³•å…¸
- å•†äº‹é—®é¢˜è¯·å’¨è¯¢å…¬å¸æ³•/è¯åˆ¸æ³•å¾‹å¸ˆ
- åŠ³åŠ¨é—®é¢˜è¯·å’¨è¯¢åŠ³åŠ¨ä»²è£éƒ¨é—¨æˆ–åŠ³åŠ¨å¾‹å¸ˆ
- è¡Œæ”¿é—®é¢˜è¯·å’¨è¯¢è¡Œæ”¿æ³•å¾‹å¸ˆæˆ–ç›¸å…³æ”¿åºœéƒ¨é—¨

å¦‚æœæ‚¨æœ‰**åˆ‘æ³•ç›¸å…³é—®é¢˜**ï¼Œæ¬¢è¿ç»§ç»­å’¨è¯¢ï¼"""

# æœ€ä½ç›¸å…³æ€§é˜ˆå€¼ - ä½äºæ­¤å€¼è§†ä¸ºè¶…èŒƒå›´
MIN_RELEVANCE_THRESHOLD = 0.65


@dataclass
class Citation:
    """å¼•ç”¨æ¥æºæ•°æ®ç±»"""
    source: str
    doc_type: str
    content: str
    relevance_score: float
    metadata: Dict


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç±»"""
    answer: str
    citations: List[Citation]
    confidence: float
    is_uncertain: bool
    retrieved_docs: List[Document]


class JurisRAGEngine:
    """æ³•å¾‹RAGå¼•æ“ - æ”¯æŒå¤šé¢†åŸŸ"""
    
    def _try_load_multi_domain_vectorstores(self) -> bool:
        """
        å°è¯•åŠ è½½å¤šé¢†åŸŸå‘é‡åº“
        
        Returns:
            True å¦‚æœæˆåŠŸåŠ è½½å¤šé¢†åŸŸï¼ŒFalse å¦åˆ™
        """
        legal_domains = {
            'criminal': 'åˆ‘æ³•',
            'civil': 'æ°‘æ³•',
            'commercial': 'å•†æ³•',
            'administrative': 'è¡Œæ”¿æ³•',
            'labor': 'åŠ³åŠ¨æ³•'
        }
        
        self.vectorstores_multi = {}
        loaded_count = 0
        
        for domain_key, domain_name in legal_domains.items():
            domain_db_path = os.path.join(DB_PATH, domain_key)
            if os.path.exists(domain_db_path):
                try:
                    vs = Chroma(
                        persist_directory=domain_db_path,
                        embedding_function=self.embeddings
                    )
                    self.vectorstores_multi[domain_key] = {
                        'vectorstore': vs,
                        'retriever': vs.as_retriever(search_type="similarity", search_kwargs={"k": RETRIEVAL_TOP_K * 2}),
                        'name': domain_name
                    }
                    loaded_count += 1
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åŠ è½½ {domain_name} å‘é‡åº“: {e}")
        
        if loaded_count > 0:
            print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªé¢†åŸŸçš„å‘é‡åº“")
            return True
        return False
    
    def __init__(self, streaming: bool = True):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        """
        if not SILICONFLOW_API_KEY:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        self.streaming = streaming
        self.chat_history: List[Tuple[str, str]] = []
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embeddings()
        self._init_vectorstore()
        self._init_llm()
        self._init_chains()
    
    # ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ ====================
    def _detect_out_of_scope(self, query: str) -> Tuple[bool, str]:
        """
        æ£€æµ‹é—®é¢˜æ˜¯å¦è¶…å‡ºåˆ‘æ³•èŒƒå›´
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦è¶…èŒƒå›´, æ£€æµ‹åˆ°çš„é¢†åŸŸ)
        """
        query_lower = query.lower()
        
        # æ£€æµ‹åˆ°çš„éåˆ‘æ³•é¢†åŸŸ
        detected_domains = []
        
        # æŒ‰é¢†åŸŸåˆ†ç»„æ£€æµ‹
        domain_keywords = {
            "æ°‘æ³•/åˆåŒæ³•": ["æ°‘æ³•å…¸", "åˆåŒæ³•", "å©šå§»æ³•", "ç»§æ‰¿æ³•", "ç‰©æƒæ³•", "ä¾µæƒè´£ä»»",
                         "æ°‘äº‹çº çº·", "ç¦»å©š", "æŠšå…»æƒ", "é—äº§ç»§æ‰¿", "æˆ¿äº§çº çº·", "å€ºåŠ¡çº çº·",
                         "å€Ÿæ¬¾åˆåŒ", "ç§ŸèµåˆåŒ", "ä¹°å–åˆåŒ", "åŠ³åŠ¡åˆåŒ"],
            "å•†æ³•/å…¬å¸æ³•": ["å…¬å¸æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ç¥¨æ®æ³•", "ç ´äº§æ³•",
                         "è‚¡ç¥¨", "åŸºé‡‘", "æŠ•èµ„ç†è´¢", "ä¸Šå¸‚å…¬å¸", "è‘£äº‹ä¼š", "è‚¡ä¸œ",
                         "å•†ä¸šç§˜å¯†", "çŸ¥è¯†äº§æƒ", "ä¸“åˆ©", "å•†æ ‡", "è‘—ä½œæƒ"],
            "è¡Œæ”¿æ³•": ["è¡Œæ”¿å¤„ç½š", "è¡Œæ”¿å¤è®®", "è¡Œæ”¿è¯‰è®¼", "æ‹†è¿", "åœŸåœ°å¾æ”¶",
                     "è¡Œæ”¿è®¸å¯", "è¡Œæ”¿å¼ºåˆ¶", "å…¬åŠ¡å‘˜", "äº‹ä¸šç¼–"],
            "åŠ³åŠ¨æ³•": ["åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "ç¤¾ä¿", "å·¥ä¼¤", "åŠ³åŠ¨ä»²è£",
                     "åŠ ç­è´¹", "å¹´å‡", "è¾é€€èµ”å¿", "äº”é™©ä¸€é‡‘"],
            "å…¶ä»–éåˆ‘æ³•": ["ç¨æ³•", "æµ·å…³", "ç¯ä¿æ³•", "é£Ÿå“å®‰å…¨",
                        "åŒ»ç–—çº çº·", "åŒ»æ‚£å…³ç³»", "äº¤é€šäº‹æ•…èµ”å¿"]
        }
        
        for domain, keywords in domain_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    detected_domains.append(domain)
                    break
        
        if detected_domains:
            # å»é‡å¹¶è¿”å›ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„é¢†åŸŸ
            return True, detected_domains[0]
        
        return False, ""
    
    def _is_low_relevance(self, docs: List[Document]) -> bool:
        """
        æ£€æµ‹æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§æ˜¯å¦è¿‡ä½
        å¦‚æœæ‰€æœ‰æ–‡æ¡£çš„ç›¸å…³æ€§éƒ½ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºè¶…å‡ºèŒƒå›´
        
        Args:
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            
        Returns:
            bool: æ˜¯å¦ç›¸å…³æ€§è¿‡ä½
        """
        if not docs:
            return True
        
        # è®¡ç®—æ³•æ¡æ–‡æ¡£çš„å¹³å‡ç›¸å…³æ€§
        statute_scores = []
        for doc in docs:
            if doc.metadata.get("type") == "statute":
                score = doc.metadata.get("relevance_score", 1.0)
                # ChromaDBåˆ†æ•°è¶Šä½è¶Šç›¸å…³
                relevance = 1 - min(score / 2, 1.0)
                statute_scores.append(relevance)
        
        if not statute_scores:
            # æ²¡æœ‰æ£€ç´¢åˆ°æ³•æ¡ï¼Œå¯èƒ½æ˜¯è¶…èŒƒå›´
            return True
        
        avg_relevance = sum(statute_scores) / len(statute_scores)
        return avg_relevance < MIN_RELEVANCE_THRESHOLD
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY
        )
    
    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError(
                f"âŒ å‘é‡åº“ä¸å­˜åœ¨: {DB_PATH}\n"
                f"   è¯·å…ˆè¿è¡Œ: python -m src.data_processing"
            )
        
        # å°è¯•åŠ è½½å¤šé¢†åŸŸå‘é‡åº“
        self.multi_domain_mode = self._try_load_multi_domain_vectorstores()
        
        if self.multi_domain_mode:
            print("âœ… å¤šé¢†åŸŸæ¨¡å¼å·²å¯åŠ¨")
        else:
            # å›é€€åˆ°å•é¢†åŸŸæ¨¡å¼ï¼ˆåˆ‘æ³•ï¼‰
            print("âš ï¸  æœªæ£€æµ‹åˆ°å¤šé¢†åŸŸå‘é‡åº“ï¼Œä½¿ç”¨å•é¢†åŸŸæ¨¡å¼ï¼ˆåˆ‘æ³•ï¼‰")
            self.vectorstore = Chroma(
                persist_directory=DB_PATH,
                embedding_function=self.embeddings
            )
            
            # ä½¿ç”¨æ›´å®½æ¾çš„ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œåç»­é€šè¿‡åå¤„ç†è¿‡æ»¤
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": RETRIEVAL_TOP_K * 2  # æ£€ç´¢æ›´å¤šï¼Œåå¤„ç†ç­›é€‰
                }
            )
    
    def _extract_crime_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–ç½ªåå…³é”®è¯ï¼Œè¿”å›å¤šä¸ªå¢å¼ºæŸ¥è¯¢
        æ–¹æ¡ˆCå¢å¼ºï¼šæ‰©å±•ç½ªåæ˜ å°„ï¼Œæ”¯æŒæ›´å¤šç½ªå
        """
        # å¸¸è§ç½ªåå…³é”®è¯æ˜ å°„ - åŒ…å«æ¡æ¬¾å·å’Œæ ¸å¿ƒæè¿°è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        crime_mappings = {
            # ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©ç½ª
            "æ•…æ„æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘ æ— æœŸå¾’åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ© æ•…æ„æ€äºº"],
            "æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©"],
            "æ•…æ„ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³ è½»ä¼¤ é‡ä¼¤ è‡´äººæ­»äº¡"],
            "ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³"],
            "å¼ºå¥¸": ["ç¬¬äºŒç™¾ä¸‰åå…­æ¡ å¼ºå¥¸ æš´åŠ› èƒè¿« å¦‡å¥³", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ© å¼ºå¥¸ç½ª"],
            "ç»‘æ¶": ["ç¬¬äºŒç™¾ä¸‰åä¹æ¡ ç»‘æ¶ å‹’ç´¢è´¢ç‰© äººè´¨"],
            "æ‹å–": ["ç¬¬äºŒç™¾å››åæ¡ æ‹å–å¦‡å¥³å„¿ç«¥"],
            "éæ³•æ‹˜ç¦": ["ç¬¬äºŒç™¾ä¸‰åå…«æ¡ éæ³•æ‹˜ç¦ å‰¥å¤ºäººèº«è‡ªç”±"],
            
            # ä¾µçŠ¯è´¢äº§ç½ª
            "ç›—çªƒ": ["ç¬¬äºŒç™¾å…­åå››æ¡ ç›—çªƒ æ•°é¢è¾ƒå¤§ æ•°é¢å·¨å¤§ ä¾µçŠ¯è´¢äº§ç½ª"],
            "æŠ¢åŠ«": ["ç¬¬äºŒç™¾å…­åä¸‰æ¡ æŠ¢åŠ« æš´åŠ› èƒè¿« ä¾µçŠ¯è´¢äº§ç½ª"],
            "è¯ˆéª—": ["ç¬¬äºŒç™¾å…­åå…­æ¡ è¯ˆéª— æ•°é¢è¾ƒå¤§ æ•°é¢å·¨å¤§"],
            "æŠ¢å¤º": ["ç¬¬äºŒç™¾å…­åä¸ƒæ¡ æŠ¢å¤º å…¬ç„¶å¤ºå–"],
            "æ•²è¯ˆå‹’ç´¢": ["ç¬¬äºŒç™¾ä¸ƒåå››æ¡ æ•²è¯ˆå‹’ç´¢ å¨èƒ è¦æŒŸ"],
            "ä¾µå ": ["ç¬¬äºŒç™¾ä¸ƒåæ¡ ä¾µå  ä»£ä¸ºä¿ç®¡"],
            "æŒªç”¨": ["ç¬¬äºŒç™¾ä¸ƒåäºŒæ¡ æŒªç”¨èµ„é‡‘"],
            
            # å±å®³å…¬å…±å®‰å…¨ç½ª
            "äº¤é€šè‚‡äº‹": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ äº¤é€šè‚‡äº‹ é€ƒé€¸ é‡å¤§äº‹æ•… å±å®³å…¬å…±å®‰å…¨"],
            "é†‰é©¾": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ é†‰é…’é©¾é©¶"],
            "å±é™©é©¾é©¶": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ é†‰é…’ è¿½é€ç«é©¶"],
            "æ”¾ç«": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ æ”¾ç«ç½ª å±å®³å…¬å…±å®‰å…¨"],
            "çˆ†ç‚¸": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ çˆ†ç‚¸ç½ª å±å®³å…¬å…±å®‰å…¨"],
            
            # å¦¨å®³ç¤¾ä¼šç®¡ç†ç§©åºç½ª
            "èšä¼—æ–—æ®´": ["ç¬¬äºŒç™¾ä¹åäºŒæ¡ èšä¼—æ–—æ®´ é¦–è¦åˆ†å­ ç§¯æå‚åŠ "],
            "å¯»è¡…æ»‹äº‹": ["ç¬¬äºŒç™¾ä¹åä¸‰æ¡ å¯»è¡…æ»‹äº‹ éšæ„æ®´æ‰“ è¿½é€æ‹¦æˆª"],
            "èµŒåš": ["ç¬¬ä¸‰ç™¾é›¶ä¸‰æ¡ èµŒåšç½ª å¼€è®¾èµŒåœº"],
            "ä¼ªè¯": ["ç¬¬ä¸‰ç™¾é›¶äº”æ¡ ä¼ªè¯ç½ª è™šå‡è¯æ˜ è¯äºº"],
            "åŒ…åº‡": ["ç¬¬ä¸‰ç™¾ä¸€åæ¡ åŒ…åº‡ç½ª çªè— éšç’"],
            "å¦¨å®³å…¬åŠ¡": ["ç¬¬äºŒç™¾ä¸ƒåä¸ƒæ¡ å¦¨å®³å…¬åŠ¡ æš´åŠ› å¨èƒ"],
            
            # è´ªæ±¡è´¿èµ‚ç½ª
            "è´ªæ±¡": ["ç¬¬ä¸‰ç™¾å…«åäºŒæ¡ è´ªæ±¡ç½ª å›½å®¶å·¥ä½œäººå‘˜ ä¾µå"],
            "å—è´¿": ["ç¬¬ä¸‰ç™¾å…«åäº”æ¡ å—è´¿ç½ª å›½å®¶å·¥ä½œäººå‘˜ è°‹å–åˆ©ç›Š"],
            "è¡Œè´¿": ["ç¬¬ä¸‰ç™¾å…«åä¹æ¡ è¡Œè´¿ç½ª ç»™äºˆè´¢ç‰©"],
            "æŒªç”¨å…¬æ¬¾": ["ç¬¬ä¸‰ç™¾å…«åå››æ¡ æŒªç”¨å…¬æ¬¾ å½’ä¸ªäººä½¿ç”¨"],
            
            # èµ°ç§è´©æ¯’ç½ª
            "æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ èµ°ç§è´©å–è¿è¾“åˆ¶é€ æ¯’å“ èµ°ç§ç½ª"],
            "è´©æ¯’": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ è´©å–æ¯’å“ èµ°ç§è¿è¾“åˆ¶é€ "],
            "èµ°ç§": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§ç½ª æ­¦å™¨å¼¹è¯ æ ¸ææ–™ å‡å¸", "ç¬¬ä¸€ç™¾äº”åä¸‰æ¡ èµ°ç§æ™®é€šè´§ç‰©"],
            
            # åˆ‘ç½šåˆ¶åº¦
            "æ­£å½“é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“ ä¸è´Ÿåˆ‘äº‹è´£ä»» ä¸æ³•ä¾µå®³"],
            "é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“"],
            "ç´§æ€¥é¿é™©": ["ç¬¬äºŒåä¸€æ¡ ç´§æ€¥é¿é™© é¿å…å±é™©"],
            "è‡ªé¦–": ["ç¬¬å…­åä¸ƒæ¡ è‡ªé¦– ä»è½»å¤„ç½š å‡è½»å¤„ç½š è‡ªåŠ¨æŠ•æ¡ˆ"],
            "ç«‹åŠŸ": ["ç¬¬å…­åå…«æ¡ ç«‹åŠŸ é‡å¤§ç«‹åŠŸ å‡è½»å¤„ç½š"],
            "ç´¯çŠ¯": ["ç¬¬å…­åäº”æ¡ ç´¯çŠ¯ ä»é‡å¤„ç½š äº”å¹´ä»¥å†…"],
            "ç¼“åˆ‘": ["ç¬¬ä¸ƒåäºŒæ¡ ç¼“åˆ‘ å®£å‘Šç¼“åˆ‘ ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘", "ç¬¬ä¸ƒåä¸‰æ¡ ç¼“åˆ‘è€ƒéªŒæœŸ"],
            "å‡åˆ‘": ["ç¬¬ä¸ƒåå…«æ¡ å‡åˆ‘ æ‚”æ”¹è¡¨ç° ç«‹åŠŸè¡¨ç°"],
            "å‡é‡Š": ["ç¬¬å…«åä¸€æ¡ å‡é‡Š æœåˆ‘æœŸé—´ ä¸è‡´å†å±å®³ç¤¾ä¼š"],
            "æœªæˆå¹´": ["ç¬¬åä¸ƒæ¡ æœªæˆå¹´äºº åˆ‘äº‹è´£ä»»å¹´é¾„ ä»è½»å‡è½»"],
            "åˆ‘äº‹è´£ä»»å¹´é¾„": ["ç¬¬åä¸ƒæ¡ åˆ‘äº‹è´£ä»»å¹´é¾„ åå››å‘¨å² åå…­å‘¨å²"],
            "ä»è½»": ["ç¬¬å…­åä¸ƒæ¡ ä»è½»å¤„ç½š", "ç¬¬åä¸ƒæ¡ ä»è½»å‡è½»"],
            "å‡è½»": ["ç¬¬å…­åä¸‰æ¡ å‡è½»å¤„ç½š æ³•å®šåˆ‘ä»¥ä¸‹"],
            "ä»é‡": ["ç¬¬å…­åäº”æ¡ ä»é‡å¤„ç½š"],
            "å…±åŒçŠ¯ç½ª": ["ç¬¬äºŒåäº”æ¡ å…±åŒçŠ¯ç½ª äºŒäººä»¥ä¸Šå…±åŒæ•…æ„"],
            "ä¸»çŠ¯": ["ç¬¬äºŒåå…­æ¡ ä¸»çŠ¯ ç»„ç»‡é¢†å¯¼ ä¸»è¦ä½œç”¨"],
            "ä»çŠ¯": ["ç¬¬äºŒåä¸ƒæ¡ ä»çŠ¯ æ¬¡è¦è¾…åŠ©ä½œç”¨"],
        }
        
        enhanced_queries = [query]  # åŸå§‹æŸ¥è¯¢å§‹ç»ˆä¿ç•™
        matched_keywords = []
        
        # åŒ¹é…æ‰€æœ‰ç›¸å…³å…³é”®è¯
        for keyword, expansions in crime_mappings.items():
            if keyword in query:
                enhanced_queries.extend(expansions)
                matched_keywords.append(keyword)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œå°è¯•é€šç”¨æ³•å¾‹æŸ¥è¯¢å¢å¼º
        if not matched_keywords:
            enhanced_queries.append(f"åˆ‘æ³• {query} å¤„ç½š")
            enhanced_queries.append(f"{query} æœ‰æœŸå¾’åˆ‘ ç½šé‡‘")
        
        return enhanced_queries
    
    def _hybrid_retrieve(self, query: str, k: int = RETRIEVAL_TOP_K) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼šåˆ†åˆ«æ£€ç´¢æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œç„¶ååˆå¹¶
        ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼º + å…³é”®è¯è¿‡æ»¤æé«˜æ³•æ¡æ£€ç´¢ç²¾åº¦
        
        ChromaDBçš„åˆ†æ•°è¶Šä½è¡¨ç¤ºè¶Šç›¸å…³ï¼ˆL2è·ç¦»ï¼‰
        """
        statute_docs = []
        case_docs = []
        seen_doc_ids = set()
        
        statute_k = max(4, k // 2 + 1)  # æ³•æ¡æ•°é‡
        case_k = k - statute_k + 2  # æ¡ˆä¾‹æ•°é‡
        
        # 1. æ³•æ¡æ£€ç´¢ï¼šä½¿ç”¨å¤šä¸ªå¢å¼ºæŸ¥è¯¢
        enhanced_queries = self._extract_crime_keywords(query)
        
        for eq in enhanced_queries:
            try:
                results = self.vectorstore.similarity_search_with_score(
                    eq,
                    k=statute_k * 2,
                    filter={"type": "statute"}
                )
                
                for doc, score in results:
                    doc_id = doc.metadata.get("doc_id", id(doc))
                    if doc_id not in seen_doc_ids:
                        seen_doc_ids.add(doc_id)
                        doc.metadata["relevance_score"] = score
                        statute_docs.append(doc)
                        
            except Exception as e:
                print(f"[è­¦å‘Š] æ³•æ¡æ£€ç´¢å¤±è´¥: {e}")
        
        # 2. æ¡ˆä¾‹æ£€ç´¢ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢
        try:
            case_results = self.vectorstore.similarity_search_with_score(
                query, 
                k=case_k * 2,
                filter={"type": "case"}
            )
            
            for doc, score in case_results:
                doc_id = doc.metadata.get("doc_id", id(doc))
                if doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    doc.metadata["relevance_score"] = score
                    case_docs.append(doc)
                    
        except Exception as e:
            print(f"[è­¦å‘Š] æ¡ˆä¾‹æ£€ç´¢å¤±è´¥: {e}")
        
        # 3. æ–¹æ¡ˆCå¢å¼ºï¼šå…³é”®è¯é‡æ’åº + è¯­ä¹‰ç›¸å…³æ€§èåˆ
        def get_keyword_score(doc):
            """è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ† - å¢å¼ºç‰ˆ"""
            base_score = doc.metadata.get("relevance_score", 999)
            content = doc.page_content.lower()
            query_lower = query.lower()
            
            # æ‰©å±•çš„å…³é”®è¯åˆ—è¡¨ï¼ˆè¦†ç›–æ›´å¤šç½ªåï¼‰
            crime_keywords = [
                # ä¾µçŠ¯äººèº«æƒåˆ©
                "æ•…æ„æ€äºº", "æ•…æ„ä¼¤å®³", "å¼ºå¥¸", "ç»‘æ¶", "æ‹å–", "éæ³•æ‹˜ç¦",
                # ä¾µçŠ¯è´¢äº§
                "ç›—çªƒ", "æŠ¢åŠ«", "è¯ˆéª—", "æŠ¢å¤º", "æ•²è¯ˆå‹’ç´¢", "ä¾µå ", "æŒªç”¨",
                # å±å®³å…¬å…±å®‰å…¨
                "äº¤é€šè‚‡äº‹", "å±é™©é©¾é©¶", "é†‰é©¾", "æ”¾ç«", "çˆ†ç‚¸",
                # å¦¨å®³ç¤¾ä¼šç®¡ç†
                "èšä¼—æ–—æ®´", "å¯»è¡…æ»‹äº‹", "èµŒåš", "ä¼ªè¯", "åŒ…åº‡", "å¦¨å®³å…¬åŠ¡",
                # è´ªæ±¡è´¿èµ‚
                "è´ªæ±¡", "å—è´¿", "è¡Œè´¿", "æŒªç”¨å…¬æ¬¾",
                # æ¯’å“çŠ¯ç½ª
                "æ¯’å“", "è´©æ¯’", "èµ°ç§",
                # åˆ‘ç½šåˆ¶åº¦
                "æ­£å½“é˜²å«", "ç´§æ€¥é¿é™©", "è‡ªé¦–", "ç«‹åŠŸ", "ç´¯çŠ¯",
                "ç¼“åˆ‘", "å‡åˆ‘", "å‡é‡Š", "æœªæˆå¹´", "å…±åŒçŠ¯ç½ª",
                "ä»è½»", "å‡è½»", "ä»é‡", "ä¸»çŠ¯", "ä»çŠ¯"
            ]
            
            bonus = 0
            matched_count = 0
            
            # ç»Ÿè®¡åŒ¹é…çš„å…³é”®è¯æ•°é‡
            for kw in crime_keywords:
                if kw in query_lower and kw in content:
                    matched_count += 1
                    bonus -= 0.5  # æ¯ä¸ªåŒ¹é…å…³é”®è¯é™ä½0.5åˆ†
            
            # é¢å¤–å¥–åŠ±ï¼šå¤šä¸ªå…³é”®è¯åŒ¹é…
            if matched_count >= 2:
                bonus -= 0.5  # é¢å¤–å¥–åŠ±
            
            # ç²¾ç¡®æ¡æ¬¾åŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            article_nums = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', query)
            for num in article_nums:
                if num in content:
                    bonus -= 2.0  # ç²¾ç¡®æ¡æ¬¾åŒ¹é…
            
            # æŸ¥è¯¢è¯ç›´æ¥å‡ºç°åœ¨å†…å®¹ä¸­
            query_words = [w for w in query_lower.split() if len(w) >= 2]
            for word in query_words:
                if word in content:
                    bonus -= 0.3
            
            return base_score + bonus
        
        # å¯¹æ³•æ¡è¿›è¡Œé‡æ’åº
        statute_docs.sort(key=get_keyword_score)
        case_docs.sort(key=lambda d: d.metadata.get("relevance_score", 999))
        
        # 4. åˆå¹¶ç»“æœï¼šæ³•æ¡ä¼˜å…ˆ
        final_docs = []
        final_docs.extend(statute_docs[:statute_k])
        final_docs.extend(case_docs[:case_k])
        
        # 5. å›é€€æ£€ç´¢
        if len(final_docs) == 0:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            for doc, score in results:
                doc.metadata["relevance_score"] = score
                final_docs.append(doc)
        
        return final_docs[:k]
    
    def _init_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY,
            streaming=self.streaming
        )
    
    def _init_chains(self):
        """åˆå§‹åŒ–RAGé“¾ - ç®€åŒ–ç‰ˆï¼ˆä¸ä¾èµ–ä¼ ç»Ÿchainsï¼‰"""
        # ç”±äºä½¿ç”¨äº†æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä¸å†éœ€è¦ä¼ ç»Ÿçš„chains
        # ç›´æ¥åœ¨queryæ–¹æ³•ä¸­å¤„ç†æ£€ç´¢å’Œç”Ÿæˆ
        self.history_aware_retriever = None
        self.rag_chain = None
    
    def _format_chat_history(self) -> List:
        """æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºLangChainæ¶ˆæ¯æ ¼å¼"""
        messages = []
        for human, ai in self.chat_history[-MAX_HISTORY_TURNS:]:
            messages.append(HumanMessage(content=human))
            messages.append(AIMessage(content=ai))
        return messages
    
    def _extract_citations(self, docs: List[Document]) -> List[Citation]:
        """ä»æ£€ç´¢æ–‡æ¡£ä¸­æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, doc in enumerate(docs):
            # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–ç›¸ä¼¼åº¦åˆ†æ•°
            score = (
                doc.metadata.get("relevance_score") or
                doc.metadata.get("score") or
                doc.metadata.get("_score") or
                0.7  # é»˜è®¤ç»™äºˆä¸­ç­‰ç›¸å…³æ€§
            )
            
            # æ”¹è¿›æ¥æºæ ‡æ³¨ - åŒ…å«æ›´å¤šå…ƒæ•°æ®ä¿¡æ¯
            source_parts = [doc.metadata.get("source", "æœªçŸ¥æ¥æº")]
            
            # æ·»åŠ ç±»å‹ä¿¡æ¯
            doc_type = doc.metadata.get("type", "unknown")
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                if article:
                    source_parts.append(f"({article})")
            elif doc_type == "case":
                accusation = doc.metadata.get("accusation", "")
                case_id = doc.metadata.get("case_id", "")
                if accusation:
                    source_parts.append(f"ã€{accusation}ã€‘")
                if case_id:
                    source_parts.append(f"(æ¡ˆå·:{case_id})")
            
            source_display = "".join(source_parts)
            
            citation = Citation(
                source=source_display,
                doc_type=doc_type,
                content=doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                relevance_score=float(score),
                metadata=doc.metadata
            )
            citations.append(citation)
        return citations

    def _attach_similarity_scores(self, query: str, docs: List[Document]) -> List[Document]:
        """ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£è¡¥å……ç›¸ä¼¼åº¦åˆ†æ•°ã€‚"""
        if not docs:
            return docs
        try:
            # é¢„å–æ›´å¤šå€™é€‰ï¼Œä»¥ä¾¿è¦†ç›– context ä¸­çš„æ–‡æ¡£
            k = max(len(docs), RETRIEVAL_TOP_K * 2)
            scored = self.vectorstore.similarity_search_with_score(query, k=k)
            score_map = {}
            for d, score in scored:
                doc_id = d.metadata.get("doc_id")
                if doc_id:
                    score_map[doc_id] = score
            for doc in docs:
                doc_id = doc.metadata.get("doc_id")
                if doc_id and doc_id in score_map:
                    doc.metadata["relevance_score"] = score_map[doc_id]
            return docs
        except Exception:
            return docs
    
    def _calculate_confidence(self, docs: List[Document]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0
        
        # ChromaDBçš„åˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼Œéœ€è¦è½¬æ¢
        scores = []
        has_statute = False
        
        for doc in docs:
            raw_score = doc.metadata.get("relevance_score", 1.0)
            # è½¬æ¢ä¸º0-1çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼‰
            relevance = max(0, 1 - raw_score / 2)
            scores.append(relevance)
            
            if doc.metadata.get("type") == "statute":
                has_statute = True
        
        max_score = max(scores) if scores else 0.0
        avg_score = sum(scores) / len(scores) if scores else 0.0
        
        # å¦‚æœæœ‰æ³•æ¡æ–‡æ¡£ï¼Œç½®ä¿¡åº¦æå‡
        statute_bonus = 0.1 if has_statute else 0
        
        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.4 * max_score + 0.4 * avg_score + 0.2 * min(len(docs) / RETRIEVAL_TOP_K, 1.0) + statute_bonus
        
        return round(min(confidence, 0.95), 2)
    
    def query(self, question: str) -> RAGResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ + è¶…èŒƒå›´æ£€æµ‹ï¼‰
        
        æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ - æ£€æµ‹éåˆ‘æ³•é¢†åŸŸé—®é¢˜å¹¶æ‹’ç»
        æ–¹æ¡ˆC: æ··åˆæ£€ç´¢ + é‡æ’åº
        æ–¹æ¡ˆD: ä¼˜åŒ–æç¤ºè¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            RAGResponse: åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯
        """
        # ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ ====================
        is_out_of_scope, detected_domain = self._detect_out_of_scope(question)
        
        if is_out_of_scope:
            # ç”Ÿæˆè¶…èŒƒå›´æ‹’ç»å“åº”
            out_of_scope_answer = OUT_OF_SCOPE_RESPONSE.format(detected_domain=detected_domain)
            self.chat_history.append((question, out_of_scope_answer))
            return RAGResponse(
                answer=out_of_scope_answer,
                citations=[],
                confidence=0.1,  # ä½ç½®ä¿¡åº¦è¡¨ç¤ºè¿™æ˜¯æ‹’ç»å›ç­”
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ ¼å¼åŒ–å†å²å¯¹è¯
        chat_history = self._format_chat_history()
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥è·å–æ–‡æ¡£
        docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K)
        
        # æ£€æµ‹æ£€ç´¢ç»“æœç›¸å…³æ€§æ˜¯å¦è¿‡ä½ï¼ˆç¬¬äºŒé“é˜²çº¿ï¼‰
        if self._is_low_relevance(docs):
            low_relevance_answer = """æŠ±æ­‰ï¼Œåœ¨ç°æœ‰çš„åˆ‘æ³•æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜é«˜åº¦ç›¸å…³çš„æ³•æ¡ã€‚

**å¯èƒ½çš„åŸå› **ï¼š
1. é—®é¢˜æ¶‰åŠçš„å…·ä½“æ³•å¾‹è§„å®šä¸åœ¨å½“å‰çŸ¥è¯†åº“è¦†ç›–èŒƒå›´å†…
2. é—®é¢˜è¡¨è¿°å¯èƒ½éœ€è¦æ›´å…·ä½“çš„æ³•å¾‹æœ¯è¯­
3. è¯¥é—®é¢˜å¯èƒ½æ¶‰åŠå…¶ä»–æ³•å¾‹é¢†åŸŸ

**å»ºè®®**ï¼š
- è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„æ³•å¾‹æœ¯è¯­æè¿°é—®é¢˜
- å¦‚æœæ¶‰åŠå…·ä½“æ¡ˆä»¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåˆ‘äº‹å¾‹å¸ˆ
- å¦‚æœ‰å…¶ä»–åˆ‘æ³•ç›¸å…³é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­å’¨è¯¢"""
            self.chat_history.append((question, low_relevance_answer))
            return RAGResponse(
                answer=low_relevance_answer,
                citations=[],
                confidence=0.2,
                is_uncertain=True,
                retrieved_docs=docs
            )
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆæ–‡æ¡£
        if not docs:
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(docs, 1):
            doc_type = doc.metadata.get("type", "unknown")
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ³•æ¡ã€‘{source} {article}\n{doc.page_content}")
            else:
                accusation = doc.metadata.get("accusation", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ¡ˆä¾‹ã€‘{source}ï¼ˆ{accusation}ï¼‰\n{doc.page_content}")
        
        context_text = "\n\n".join(context_parts)
        
        # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›ç­”
        from langchain_core.messages import HumanMessage, SystemMessage
        
        # ==================== æ–¹æ¡ˆD: ä¼˜åŒ–æç¤ºè¯ ====================
        qa_prompt = f"""ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„**ä¸­å›½åˆ‘æ³•**é—®ç­”AIã€‚ä½ åªå›ç­”åˆ‘æ³•ç›¸å…³é—®é¢˜ã€‚

ã€ç³»ç»Ÿè¯´æ˜ã€‘
æœ¬ç³»ç»Ÿä¸“æ³¨äºä¸­å›½åˆ‘æ³•é¢†åŸŸï¼ŒåŒ…æ‹¬ï¼š
- å„ç±»åˆ‘äº‹çŠ¯ç½ªçš„è®¤å®šä¸é‡åˆ‘ï¼ˆå¦‚æ•…æ„æ€äººã€ç›—çªƒã€è¯ˆéª—ç­‰ï¼‰
- åˆ‘äº‹è´£ä»»å¹´é¾„ã€è‡ªé¦–ã€ç«‹åŠŸã€ç´¯çŠ¯ç­‰é‡åˆ‘æƒ…èŠ‚
- æ­£å½“é˜²å«ã€ç´§æ€¥é¿é™©ç­‰å…è´£äº‹ç”±
- åˆ‘äº‹æ¡ˆä¾‹çš„åˆ¤å†³å‚è€ƒ

ã€å›ç­”åŸåˆ™ã€‘
1. **ä¸¥æ ¼åŸºäºæ£€ç´¢å†…å®¹**ï¼šåªä½¿ç”¨æ£€ç´¢åˆ°çš„æ³•æ¡å’Œæ¡ˆä¾‹å›ç­”ï¼Œä¸ç¼–é€ 
2. **æ³•æ¡ä¼˜å…ˆ**ï¼šå¦‚æ£€ç´¢åˆ°ã€Šåˆ‘æ³•ã€‹æ¡æ–‡ï¼Œå¿…é¡»ä¼˜å…ˆå¼•ç”¨æ³•æ¡åŸæ–‡
3. **å‡†ç¡®å¼•ç”¨**ï¼šæ³•æ¡ç¼–å·å’Œå†…å®¹å¿…é¡»ä¸æ£€ç´¢æ–‡æ¡£å®Œå…¨ä¸€è‡´
4. **è¯šå®å›ç­”**ï¼šå¦‚æ£€ç´¢å†…å®¹ä¸åŒ…å«ç›¸å…³è§„å®šï¼Œæ˜ç¡®è¯´æ˜"æœªæ£€ç´¢åˆ°ç›¸å…³æ³•æ¡"

ã€å›ç­”æ ¼å¼ã€‘
**ç›´æ¥å›ç­”**ï¼šç”¨1-2å¥è¯æ¦‚æ‹¬æ ¸å¿ƒç»“è®ºï¼ˆåŸºäºæ³•æ¡ï¼‰

**æ³•å¾‹ä¾æ®**ï¼š
- å¼•ç”¨æ£€ç´¢åˆ°çš„æ³•æ¡åŸæ–‡ï¼Œæ ‡æ³¨[æ¥æºX]
- å¿…é¡»åŒ…å«å®Œæ•´çš„æ¡æ¬¾å·ï¼ˆå¦‚"ç¬¬äºŒç™¾ä¸‰åäºŒæ¡"ï¼‰

**æ¡ˆä¾‹å‚è€ƒ**ï¼ˆå¦‚æœ‰ï¼‰ï¼š
- ç®€è¦è¯´æ˜ç›¸å…³æ¡ˆä¾‹çš„åˆ¤å†³ç»“æœ

**æç¤º**ï¼šè¯´æ˜æ³¨æ„äº‹é¡¹æˆ–å»ºè®®å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆ

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

è¯·åŸºäºä¸Šè¿°æ£€ç´¢å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ£€ç´¢å†…å®¹ä¸åŒ…å«ç­”æ¡ˆæ‰€éœ€ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜ã€‚"""
        
        messages = [HumanMessage(content=qa_prompt)]
        
        response = self.llm.invoke(messages)
        answer = response.content
        
        # æå–å¼•ç”¨
        citations = self._extract_citations(docs)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(docs)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸ç¡®å®šå›ç­”
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        # å¦‚æœç½®ä¿¡åº¦è¿‡ä½ï¼Œä½¿ç”¨ä¸ç¡®å®šå›ç­”æ¨¡æ¿
        if is_uncertain and len(docs) == 0:
            answer = UNCERTAIN_RESPONSE
        
        # æ›´æ–°å¯¹è¯å†å²
        self.chat_history.append((question, answer))
        
        return RAGResponse(
            answer=answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def query_stream(self, question: str) -> Generator[str, None, RAGResponse]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ + è¶…èŒƒå›´æ£€æµ‹ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            str: ç­”æ¡ˆç‰‡æ®µ
            
        Returns:
            RAGResponse: å®Œæ•´å“åº”ï¼ˆåœ¨ç”Ÿæˆå™¨ç»“æŸæ—¶ï¼‰
        """
        # è¶…èŒƒå›´æ£€æµ‹
        is_out_of_scope, detected_domain = self._detect_out_of_scope(question)
        
        if is_out_of_scope:
            out_of_scope_answer = OUT_OF_SCOPE_RESPONSE.format(detected_domain=detected_domain)
            yield out_of_scope_answer
            self.chat_history.append((question, out_of_scope_answer))
            return RAGResponse(
                answer=out_of_scope_answer,
                citations=[],
                confidence=0.1,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢
        docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K)
        
        # æ£€æµ‹ç›¸å…³æ€§
        if self._is_low_relevance(docs):
            low_relevance_answer = """æŠ±æ­‰ï¼Œåœ¨ç°æœ‰çš„åˆ‘æ³•æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜é«˜åº¦ç›¸å…³çš„æ³•æ¡ã€‚å»ºè®®å’¨è¯¢ä¸“ä¸šåˆ‘äº‹å¾‹å¸ˆã€‚"""
            yield low_relevance_answer
            self.chat_history.append((question, low_relevance_answer))
            return RAGResponse(
                answer=low_relevance_answer,
                citations=[],
                confidence=0.2,
                is_uncertain=True,
                retrieved_docs=docs
            )
        
        citations = self._extract_citations(docs)
        confidence = self._calculate_confidence(docs)
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        if is_uncertain and len(docs) == 0:
            yield UNCERTAIN_RESPONSE
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(docs, 1):
            doc_type = doc.metadata.get("type", "unknown")
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ³•æ¡ã€‘{source} {article}\n{doc.page_content}")
            else:
                accusation = doc.metadata.get("accusation", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ¡ˆä¾‹ã€‘{source}ï¼ˆ{accusation}ï¼‰\n{doc.page_content}")
        
        context_text = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯
        qa_prompt = f"""ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„ä¸­å›½åˆ‘æ³•é—®ç­”AIã€‚

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

è¯·åŸºäºæ£€ç´¢å†…å®¹å›ç­”é—®é¢˜ï¼Œä¼˜å…ˆå¼•ç”¨æ³•æ¡åŸæ–‡ã€‚"""
        
        # æµå¼ç”Ÿæˆå›ç­”
        full_answer = ""
        for chunk in self.llm.stream([HumanMessage(content=qa_prompt)]):
            if chunk.content:
                full_answer += chunk.content
                yield chunk.content
        
        self.chat_history.append((question, full_answer))
        
        return RAGResponse(
            answer=full_answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
    
    def get_history(self) -> List[Tuple[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.chat_history.copy()
    
    def search_similar(self, query: str, k: int = 5) -> List[Document]:
        """
        ç›´æ¥æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆä¸ç»è¿‡LLMï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            
        Returns:
            List[Document]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        return self.vectorstore.similarity_search(query, k=k)


# ä¾¿æ·å‡½æ•°ï¼šè·å–é»˜è®¤RAGå¼•æ“å®ä¾‹
_default_engine: Optional[JurisRAGEngine] = None

def get_rag_engine(streaming: bool = True) -> JurisRAGEngine:
    """è·å–RAGå¼•æ“å•ä¾‹"""
    global _default_engine
    if _default_engine is None:
        _default_engine = JurisRAGEngine(streaming=streaming)
    return _default_engine


def get_retriever():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–æ£€ç´¢å™¨"""
    engine = get_rag_engine()
    return engine.retriever


def get_rag_chain():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–RAGé“¾"""
    engine = get_rag_engine()
    return engine.rag_chain


# --- å‘½ä»¤è¡Œæµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Juris-RAG å¼•æ“...")
    
    try:
        engine = JurisRAGEngine(streaming=False)
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼\n")
        
        # æµ‹è¯•é—®é¢˜åˆ—è¡¨
        test_questions = [
            "æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
            "å¦‚æœæ˜¯æƒ…èŠ‚è¾ƒè½»çš„å‘¢ï¼Ÿ",  # æµ‹è¯•å¤šè½®å¯¹è¯
            "ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for q in test_questions:
            print(f"ğŸ‘¤ ç”¨æˆ·: {q}")
            print("-" * 50)
            
            response = engine.query(q)
            
            print(f"ğŸ¤– åŠ©æ‰‹: {response.answer}")
            print(f"\nğŸ“Š ç½®ä¿¡åº¦: {response.confidence}")
            print(f"â“ ä¸ç¡®å®šå›ç­”: {response.is_uncertain}")
            
            if response.citations:
                print("\nğŸ“š å¼•ç”¨æ¥æº:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.source} ({citation.doc_type})")
                    if citation.metadata.get("accusation"):
                        print(f"       ç½ªå: {citation.metadata['accusation']}")
            
            print("\n" + "=" * 60 + "\n")
            
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬æ„å»ºå‘é‡åº“ã€‚")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
