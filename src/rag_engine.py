"""
Juris-RAG æ ¸å¿ƒå¼•æ“æ¨¡å—
æ”¯æŒå¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ã€å¼•ç”¨æ¥æºæ˜¾ç¤ºã€æ‹’ç»ä¸ç¡®å®šå›ç­”
"""
import os
import re
from typing import List, Dict, Tuple, Optional, Generator
from dataclasses import dataclass

from langchain_community.vectorstores import Chroma
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
try:
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError:  # langchain>=1.0 moved legacy chains to langchain_classic
    from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain_classic.chains.combine_documents import create_stuff_documents_chain
try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain_classic.schema import Document

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DB_PATH, EMBEDDING_MODEL, LLM_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, RETRIEVAL_TOP_K, RETRIEVAL_SCORE_THRESHOLD,
        LLM_TEMPERATURE, LLM_MAX_TOKENS, MAX_HISTORY_TURNS,
        CONFIDENCE_THRESHOLD, UNCERTAIN_RESPONSE
    )
except ImportError:
    # é»˜è®¤å›é€€é…ç½®
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    LLM_MODEL = "Qwen/Qwen3-8B"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    RETRIEVAL_TOP_K = 8
    RETRIEVAL_SCORE_THRESHOLD = 0.2
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2048
    MAX_HISTORY_TURNS = 10
    CONFIDENCE_THRESHOLD = 0.4
    UNCERTAIN_RESPONSE = "æ ¹æ®ç°æœ‰æ³•å¾‹æ•°æ®åº“ï¼Œæˆ‘æ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚"


@dataclass
class Citation:
    """å¼•ç”¨æ¥æºæ•°æ®ç±»"""
    source: str
    doc_type: str
    content: str
    relevance_score: float
    metadata: Dict


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç±»"""
    answer: str
    citations: List[Citation]
    confidence: float
    is_uncertain: bool
    retrieved_docs: List[Document]


class JurisRAGEngine:
    """æ³•å¾‹RAGå¼•æ“"""
    
    def __init__(self, streaming: bool = True):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        """
        if not SILICONFLOW_API_KEY:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        self.streaming = streaming
        self.chat_history: List[Tuple[str, str]] = []
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embeddings()
        self._init_vectorstore()
        self._init_llm()
        self._init_chains()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY
        )
    
    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError(
                f"âŒ å‘é‡åº“ä¸å­˜åœ¨: {DB_PATH}\n"
                f"   è¯·å…ˆè¿è¡Œ: python -m src.data_processing"
            )
        
        self.vectorstore = Chroma(
            persist_directory=DB_PATH,
            embedding_function=self.embeddings
        )
        
        # ä½¿ç”¨æ›´å®½æ¾çš„ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œåç»­é€šè¿‡åå¤„ç†è¿‡æ»¤
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": RETRIEVAL_TOP_K * 2  # æ£€ç´¢æ›´å¤šï¼Œåå¤„ç†ç­›é€‰
            }
        )
    
    def _extract_crime_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–ç½ªåå…³é”®è¯ï¼Œè¿”å›å¤šä¸ªå¢å¼ºæŸ¥è¯¢
        """
        # å¸¸è§ç½ªåå…³é”®è¯æ˜ å°„ - åŒ…å«æ¡æ¬¾å·å’Œæ ¸å¿ƒæè¿°è¯
        crime_mappings = {
            "æ•…æ„æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ© æ•…æ„æ€äºº"],
            "æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©"],
            "æ•…æ„ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³ è½»ä¼¤ é‡ä¼¤"],
            "ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³"],
            "ç›—çªƒ": ["ç¬¬äºŒç™¾å…­åå››æ¡ ç›—çªƒ æ•°é¢è¾ƒå¤§ ä¾µçŠ¯è´¢äº§ç½ª"],
            "æŠ¢åŠ«": ["ç¬¬äºŒç™¾å…­åä¸‰æ¡ æŠ¢åŠ« æš´åŠ› ä¾µçŠ¯è´¢äº§ç½ª"],
            "è¯ˆéª—": ["ç¬¬äºŒç™¾å…­åå…­æ¡ è¯ˆéª— æ•°é¢è¾ƒå¤§"],
            "æ­£å½“é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“ ä¸è´Ÿåˆ‘äº‹è´£ä»»"],
            "é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“"],
            "è‡ªé¦–": ["ç¬¬å…­åä¸ƒæ¡ è‡ªé¦– ä»è½»å¤„ç½š å‡è½»å¤„ç½š"],
            "ç´¯çŠ¯": ["ç¬¬å…­åäº”æ¡ ç´¯çŠ¯ ä»é‡å¤„ç½š"],
            "æœªæˆå¹´": ["ç¬¬åä¸ƒæ¡ æœªæˆå¹´äºº åˆ‘äº‹è´£ä»»å¹´é¾„"],
            "äº¤é€šè‚‡äº‹": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ äº¤é€šè‚‡äº‹ é€ƒé€¸ å±å®³å…¬å…±å®‰å…¨"],
            "é†‰é©¾": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶"],
            "å±é™©é©¾é©¶": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ é†‰é…’"],
            "è´ªæ±¡": ["ç¬¬ä¸‰ç™¾å…«åäºŒæ¡ è´ªæ±¡ å›½å®¶å·¥ä½œäººå‘˜"],
            "å—è´¿": ["ç¬¬ä¸‰ç™¾å…«åäº”æ¡ å—è´¿ å›½å®¶å·¥ä½œäººå‘˜"],
            "æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ èµ°ç§ è´©å– è¿è¾“ åˆ¶é€ æ¯’å“"],
            "å¼ºå¥¸": ["ç¬¬äºŒç™¾ä¸‰åå…­æ¡ å¼ºå¥¸ æš´åŠ›"],
            "ç»‘æ¶": ["ç¬¬äºŒç™¾ä¸‰åä¹æ¡ ç»‘æ¶ å‹’ç´¢è´¢ç‰©"],
            "æŠ¢å¤º": ["ç¬¬äºŒç™¾å…­åä¸ƒæ¡ æŠ¢å¤º"],
            "æ•²è¯ˆå‹’ç´¢": ["ç¬¬äºŒç™¾ä¸ƒåå››æ¡ æ•²è¯ˆå‹’ç´¢"],
            "ä¾µå ": ["ç¬¬äºŒç™¾ä¸ƒåæ¡ ä¾µå "],
        }
        
        enhanced_queries = [query]  # åŸå§‹æŸ¥è¯¢å§‹ç»ˆä¿ç•™
        
        for keyword, expansions in crime_mappings.items():
            if keyword in query:
                enhanced_queries.extend(expansions)
                break
        
        return enhanced_queries
    
    def _hybrid_retrieve(self, query: str, k: int = RETRIEVAL_TOP_K) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼šåˆ†åˆ«æ£€ç´¢æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œç„¶ååˆå¹¶
        ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼º + å…³é”®è¯è¿‡æ»¤æé«˜æ³•æ¡æ£€ç´¢ç²¾åº¦
        
        ChromaDBçš„åˆ†æ•°è¶Šä½è¡¨ç¤ºè¶Šç›¸å…³ï¼ˆL2è·ç¦»ï¼‰
        """
        statute_docs = []
        case_docs = []
        seen_doc_ids = set()
        
        statute_k = max(4, k // 2 + 1)  # æ³•æ¡æ•°é‡
        case_k = k - statute_k + 2  # æ¡ˆä¾‹æ•°é‡
        
        # 1. æ³•æ¡æ£€ç´¢ï¼šä½¿ç”¨å¤šä¸ªå¢å¼ºæŸ¥è¯¢
        enhanced_queries = self._extract_crime_keywords(query)
        
        for eq in enhanced_queries:
            try:
                results = self.vectorstore.similarity_search_with_score(
                    eq,
                    k=statute_k * 2,
                    filter={"type": "statute"}
                )
                
                for doc, score in results:
                    doc_id = doc.metadata.get("doc_id", id(doc))
                    if doc_id not in seen_doc_ids:
                        seen_doc_ids.add(doc_id)
                        doc.metadata["relevance_score"] = score
                        statute_docs.append(doc)
                        
            except Exception as e:
                print(f"[è­¦å‘Š] æ³•æ¡æ£€ç´¢å¤±è´¥: {e}")
        
        # 2. æ¡ˆä¾‹æ£€ç´¢ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢
        try:
            case_results = self.vectorstore.similarity_search_with_score(
                query, 
                k=case_k * 2,
                filter={"type": "case"}
            )
            
            for doc, score in case_results:
                doc_id = doc.metadata.get("doc_id", id(doc))
                if doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    doc.metadata["relevance_score"] = score
                    case_docs.append(doc)
                    
        except Exception as e:
            print(f"[è­¦å‘Š] æ¡ˆä¾‹æ£€ç´¢å¤±è´¥: {e}")
        
        # 3. å…³é”®è¯é‡æ’åºï¼šä¼˜å…ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯çš„æ³•æ¡
        def get_keyword_score(doc):
            """è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ†"""
            base_score = doc.metadata.get("relevance_score", 999)
            content = doc.page_content.lower()
            
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
            keywords = ["æ•…æ„æ€äºº", "ç›—çªƒ", "æŠ¢åŠ«", "è¯ˆéª—", "æ­£å½“é˜²å«", "è‡ªé¦–", 
                       "äº¤é€šè‚‡äº‹", "æ•…æ„ä¼¤å®³", "å¼ºå¥¸", "ç»‘æ¶", "æœªæˆå¹´"]
            
            bonus = 0
            for kw in keywords:
                if kw in query and kw in content:
                    bonus -= 1.0  # å¤§å¹…æé«˜æ’åï¼ˆé™ä½åˆ†æ•°ï¼‰
                    break
            
            # é¢å¤–å¥–åŠ±ï¼šæŸ¥è¯¢ä¸­çš„æ•°å­—è¯ï¼ˆå¦‚"232æ¡"ï¼‰
            article_nums = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', query)
            for num in article_nums:
                if num in content:
                    bonus -= 2.0  # ç²¾ç¡®æ¡æ¬¾åŒ¹é…ï¼Œæœ€é«˜ä¼˜å…ˆçº§
            
            return base_score + bonus
        
        # å¯¹æ³•æ¡è¿›è¡Œé‡æ’åº
        statute_docs.sort(key=get_keyword_score)
        case_docs.sort(key=lambda d: d.metadata.get("relevance_score", 999))
        
        # 4. åˆå¹¶ç»“æœï¼šæ³•æ¡ä¼˜å…ˆ
        final_docs = []
        final_docs.extend(statute_docs[:statute_k])
        final_docs.extend(case_docs[:case_k])
        
        # 5. å›é€€æ£€ç´¢
        if len(final_docs) == 0:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            for doc, score in results:
                doc.metadata["relevance_score"] = score
                final_docs.append(doc)
        
        return final_docs[:k]
    
    def _init_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY,
            streaming=self.streaming
        )
    
    def _init_chains(self):
        """åˆå§‹åŒ–RAGé“¾"""
        # 1. å†å²å¯¹è¯é‡å†™é“¾ - å°†ä¾èµ–ä¸Šä¸‹æ–‡çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹é—®é¢˜
        contextualize_q_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜æ”¹å†™åŠ©æ‰‹ã€‚ç»™å®šä¸€æ®µèŠå¤©å†å²å’Œç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ
è¯·åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦å¼•ç”¨äº†å†å²ä¿¡æ¯ï¼ˆå¦‚"å®ƒ"ã€"è¿™ä¸ªæ¡ˆå­"ã€"ä¸Šé¢æåˆ°çš„"ç­‰ï¼‰ã€‚

å¦‚æœæ˜¯ï¼Œè¯·å°†é—®é¢˜é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€æ— éœ€ä¸Šä¸‹æ–‡å³å¯ç†è§£çš„é—®é¢˜ã€‚
å¦‚æœé—®é¢˜å·²ç»æ˜¯ç‹¬ç«‹çš„ï¼Œè¯·åŸæ ·è¿”å›ã€‚

åªè¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Šã€‚"""
        
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])
        
        self.history_aware_retriever = create_history_aware_retriever(
            self.llm, self.retriever, contextualize_q_prompt
        )
        
        # 2. æ³•å¾‹é—®ç­”é“¾ - æ ¸å¿ƒPromptï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        qa_system_prompt = """ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„ä¸­å›½åˆ‘æ³•é—®ç­”AIã€‚åŸºäºæ£€ç´¢åˆ°çš„æ³•æ¡å’Œæ¡ˆä¾‹å›ç­”é—®é¢˜ã€‚

ã€å›ç­”åŸåˆ™ã€‘
1. **ä¼˜å…ˆä½¿ç”¨æ³•æ¡**ï¼šå¦‚æœæ£€ç´¢åˆ°äº†ã€Šåˆ‘æ³•ã€‹æ¡æ–‡ï¼Œå¿…é¡»ä¼˜å…ˆå¼•ç”¨æ³•æ¡å†…å®¹
2. **æ¡ˆä¾‹ä½œä¸ºè¡¥å……**ï¼šæ¡ˆä¾‹ç”¨äºè¯´æ˜å®é™…åˆ¤å†³æƒ…å†µï¼Œä½†ä¸èƒ½æ›¿ä»£æ³•æ¡
3. **å¦‚å®ä½œç­”**ï¼šåªåŸºäºæ£€ç´¢å†…å®¹å›ç­”ï¼Œæ— ç›¸å…³å†…å®¹åˆ™æ˜ç¡®è¯´æ˜

ã€å›ç­”æ ¼å¼ã€‘
**ç›´æ¥å›ç­”**ï¼šå…ˆç”¨1-2å¥è¯æ¦‚æ‹¬ç­”æ¡ˆï¼ˆåŸºäºæ³•æ¡ï¼‰

**æ³•å¾‹ä¾æ®**ï¼š
- å¼•ç”¨æ£€ç´¢åˆ°çš„æ³•æ¡åŸæ–‡ï¼Œæ ‡æ³¨[æ¥æºX]
- å¦‚æœ‰å¤šä¸ªç›¸å…³æ¡æ¬¾ï¼Œåˆ†åˆ«åˆ—å‡º

**æ¡ˆä¾‹å‚è€ƒ**ï¼ˆå¦‚æœ‰ï¼‰ï¼š
- ç®€è¦è¯´æ˜ç›¸å…³æ¡ˆä¾‹çš„åˆ¤å†³ç»“æœ

**æç¤º**ï¼šè¯´æ˜å¯èƒ½çš„å±€é™æˆ–å»ºè®®

ã€é‡è¦è§„åˆ™ã€‘
- æ£€ç´¢åˆ°æ³•æ¡å†…å®¹æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡
- æ³•æ¡ç¼–å·å’Œå†…å®¹å¿…é¡»ä¸æ£€ç´¢æ–‡æ¡£å®Œå…¨ä¸€è‡´
- å¦‚æœæ£€ç´¢å†…å®¹ä¸åŒ…å«é—®é¢˜æ‰€é—®çš„ç½ªå/æƒ…å†µï¼Œè¯·ç›´æ¥è¯´æ˜"æ£€ç´¢å†…å®¹ä¸­æœªæ‰¾åˆ°ç›¸å…³è§„å®š"
- ä¸è¦ç¼–é€ æˆ–æ¨æµ‹æ³•æ¡å†…å®¹

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context}"""
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])
        
        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)
        
        # 3. ç»„åˆæœ€ç»ˆRAGé“¾
        self.rag_chain = create_retrieval_chain(
            self.history_aware_retriever,
            question_answer_chain
        )
    
    def _format_chat_history(self) -> List:
        """æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºLangChainæ¶ˆæ¯æ ¼å¼"""
        messages = []
        for human, ai in self.chat_history[-MAX_HISTORY_TURNS:]:
            messages.append(HumanMessage(content=human))
            messages.append(AIMessage(content=ai))
        return messages
    
    def _extract_citations(self, docs: List[Document]) -> List[Citation]:
        """ä»æ£€ç´¢æ–‡æ¡£ä¸­æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, doc in enumerate(docs):
            # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–ç›¸ä¼¼åº¦åˆ†æ•°
            score = (
                doc.metadata.get("relevance_score") or
                doc.metadata.get("score") or
                doc.metadata.get("_score") or
                0.7  # é»˜è®¤ç»™äºˆä¸­ç­‰ç›¸å…³æ€§
            )
            
            # æ”¹è¿›æ¥æºæ ‡æ³¨ - åŒ…å«æ›´å¤šå…ƒæ•°æ®ä¿¡æ¯
            source_parts = [doc.metadata.get("source", "æœªçŸ¥æ¥æº")]
            
            # æ·»åŠ ç±»å‹ä¿¡æ¯
            doc_type = doc.metadata.get("type", "unknown")
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                if article:
                    source_parts.append(f"({article})")
            elif doc_type == "case":
                accusation = doc.metadata.get("accusation", "")
                case_id = doc.metadata.get("case_id", "")
                if accusation:
                    source_parts.append(f"ã€{accusation}ã€‘")
                if case_id:
                    source_parts.append(f"(æ¡ˆå·:{case_id})")
            
            source_display = "".join(source_parts)
            
            citation = Citation(
                source=source_display,
                doc_type=doc_type,
                content=doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                relevance_score=float(score),
                metadata=doc.metadata
            )
            citations.append(citation)
        return citations

    def _attach_similarity_scores(self, query: str, docs: List[Document]) -> List[Document]:
        """ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£è¡¥å……ç›¸ä¼¼åº¦åˆ†æ•°ã€‚"""
        if not docs:
            return docs
        try:
            # é¢„å–æ›´å¤šå€™é€‰ï¼Œä»¥ä¾¿è¦†ç›– context ä¸­çš„æ–‡æ¡£
            k = max(len(docs), RETRIEVAL_TOP_K * 2)
            scored = self.vectorstore.similarity_search_with_score(query, k=k)
            score_map = {}
            for d, score in scored:
                doc_id = d.metadata.get("doc_id")
                if doc_id:
                    score_map[doc_id] = score
            for doc in docs:
                doc_id = doc.metadata.get("doc_id")
                if doc_id and doc_id in score_map:
                    doc.metadata["relevance_score"] = score_map[doc_id]
            return docs
        except Exception:
            return docs
    
    def _calculate_confidence(self, docs: List[Document]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0
        
        # ChromaDBçš„åˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼Œéœ€è¦è½¬æ¢
        scores = []
        has_statute = False
        
        for doc in docs:
            raw_score = doc.metadata.get("relevance_score", 1.0)
            # è½¬æ¢ä¸º0-1çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼‰
            relevance = max(0, 1 - raw_score / 2)
            scores.append(relevance)
            
            if doc.metadata.get("type") == "statute":
                has_statute = True
        
        max_score = max(scores) if scores else 0.0
        avg_score = sum(scores) / len(scores) if scores else 0.0
        
        # å¦‚æœæœ‰æ³•æ¡æ–‡æ¡£ï¼Œç½®ä¿¡åº¦æå‡
        statute_bonus = 0.1 if has_statute else 0
        
        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.4 * max_score + 0.4 * avg_score + 0.2 * min(len(docs) / RETRIEVAL_TOP_K, 1.0) + statute_bonus
        
        return round(min(confidence, 0.95), 2)
    
    def query(self, question: str) -> RAGResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            RAGResponse: åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯
        """
        # æ ¼å¼åŒ–å†å²å¯¹è¯
        chat_history = self._format_chat_history()
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥è·å–æ–‡æ¡£
        docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K)
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆæ–‡æ¡£
        if not docs:
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(docs, 1):
            doc_type = doc.metadata.get("type", "unknown")
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ³•æ¡ã€‘{source} {article}\n{doc.page_content}")
            else:
                accusation = doc.metadata.get("accusation", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ¡ˆä¾‹ã€‘{source}ï¼ˆ{accusation}ï¼‰\n{doc.page_content}")
        
        context_text = "\n\n".join(context_parts)
        
        # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›ç­”
        from langchain_core.messages import HumanMessage, SystemMessage
        
        qa_prompt = f"""ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„ä¸­å›½åˆ‘æ³•é—®ç­”AIã€‚åŸºäºæ£€ç´¢åˆ°çš„æ³•æ¡å’Œæ¡ˆä¾‹å›ç­”é—®é¢˜ã€‚

ã€å›ç­”åŸåˆ™ã€‘
1. **ä¼˜å…ˆä½¿ç”¨æ³•æ¡**ï¼šå¦‚æœæ£€ç´¢åˆ°äº†ã€Šåˆ‘æ³•ã€‹æ¡æ–‡ï¼Œå¿…é¡»ä¼˜å…ˆå¼•ç”¨æ³•æ¡å†…å®¹
2. **æ¡ˆä¾‹ä½œä¸ºè¡¥å……**ï¼šæ¡ˆä¾‹ç”¨äºè¯´æ˜å®é™…åˆ¤å†³æƒ…å†µï¼Œä½†ä¸èƒ½æ›¿ä»£æ³•æ¡
3. **å¦‚å®ä½œç­”**ï¼šåªåŸºäºæ£€ç´¢å†…å®¹å›ç­”ï¼Œæ— ç›¸å…³å†…å®¹åˆ™æ˜ç¡®è¯´æ˜

ã€å›ç­”æ ¼å¼ã€‘
**ç›´æ¥å›ç­”**ï¼šå…ˆç”¨1-2å¥è¯æ¦‚æ‹¬ç­”æ¡ˆï¼ˆåŸºäºæ³•æ¡ï¼‰

**æ³•å¾‹ä¾æ®**ï¼š
- å¼•ç”¨æ£€ç´¢åˆ°çš„æ³•æ¡åŸæ–‡ï¼Œæ ‡æ³¨[æ¥æºX]
- å¦‚æœ‰å¤šä¸ªç›¸å…³æ¡æ¬¾ï¼Œåˆ†åˆ«åˆ—å‡º

**æ¡ˆä¾‹å‚è€ƒ**ï¼ˆå¦‚æœ‰ç›¸å…³æ¡ˆä¾‹ï¼‰ï¼š
- ç®€è¦è¯´æ˜ç›¸å…³æ¡ˆä¾‹çš„åˆ¤å†³ç»“æœ

**æç¤º**ï¼šè¯´æ˜å¯èƒ½çš„å±€é™æˆ–å»ºè®®

ã€é‡è¦è§„åˆ™ã€‘
- æ£€ç´¢åˆ°æ³•æ¡å†…å®¹æ—¶ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨åŸæ–‡
- æ³•æ¡ç¼–å·å’Œå†…å®¹å¿…é¡»ä¸æ£€ç´¢æ–‡æ¡£å®Œå…¨ä¸€è‡´
- å¦‚æœæ£€ç´¢å†…å®¹ä¸åŒ…å«é—®é¢˜æ‰€é—®çš„ç½ªå/æƒ…å†µï¼Œè¯·ç›´æ¥è¯´æ˜"æ£€ç´¢å†…å®¹ä¸­æœªæ‰¾åˆ°ç›¸å…³è§„å®š"

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}"""
        
        messages = [HumanMessage(content=qa_prompt)]
        
        response = self.llm.invoke(messages)
        answer = response.content
        
        # æå–å¼•ç”¨
        citations = self._extract_citations(docs)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(docs)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸ç¡®å®šå›ç­”
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        # å¦‚æœç½®ä¿¡åº¦è¿‡ä½ï¼Œä½¿ç”¨ä¸ç¡®å®šå›ç­”æ¨¡æ¿
        if is_uncertain and len(docs) == 0:
            answer = UNCERTAIN_RESPONSE
        
        # æ›´æ–°å¯¹è¯å†å²
        self.chat_history.append((question, answer))
        
        return RAGResponse(
            answer=answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def query_stream(self, question: str) -> Generator[str, None, RAGResponse]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            str: ç­”æ¡ˆç‰‡æ®µ
            
        Returns:
            RAGResponse: å®Œæ•´å“åº”ï¼ˆåœ¨ç”Ÿæˆå™¨ç»“æŸæ—¶ï¼‰
        """
        chat_history = self._format_chat_history()
        
        # å…ˆè·å–æ£€ç´¢ç»“æœ
        docs = self.history_aware_retriever.invoke({
            "input": question,
            "chat_history": chat_history
        })
        docs = self._attach_similarity_scores(question, docs)
        
        citations = self._extract_citations(docs)
        confidence = self._calculate_confidence(docs)
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        if is_uncertain and len(docs) == 0:
            yield UNCERTAIN_RESPONSE
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æµå¼ç”Ÿæˆå›ç­”
        full_answer = ""
        for chunk in self.rag_chain.stream({
            "input": question,
            "chat_history": chat_history
        }):
            if "answer" in chunk:
                full_answer += chunk["answer"]
                yield chunk["answer"]
        
        self.chat_history.append((question, full_answer))
        
        return RAGResponse(
            answer=full_answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
    
    def get_history(self) -> List[Tuple[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.chat_history.copy()
    
    def search_similar(self, query: str, k: int = 5) -> List[Document]:
        """
        ç›´æ¥æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆä¸ç»è¿‡LLMï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            
        Returns:
            List[Document]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        return self.vectorstore.similarity_search(query, k=k)


# ä¾¿æ·å‡½æ•°ï¼šè·å–é»˜è®¤RAGå¼•æ“å®ä¾‹
_default_engine: Optional[JurisRAGEngine] = None

def get_rag_engine(streaming: bool = True) -> JurisRAGEngine:
    """è·å–RAGå¼•æ“å•ä¾‹"""
    global _default_engine
    if _default_engine is None:
        _default_engine = JurisRAGEngine(streaming=streaming)
    return _default_engine


def get_retriever():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–æ£€ç´¢å™¨"""
    engine = get_rag_engine()
    return engine.retriever


def get_rag_chain():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–RAGé“¾"""
    engine = get_rag_engine()
    return engine.rag_chain


# --- å‘½ä»¤è¡Œæµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Juris-RAG å¼•æ“...")
    
    try:
        engine = JurisRAGEngine(streaming=False)
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼\n")
        
        # æµ‹è¯•é—®é¢˜åˆ—è¡¨
        test_questions = [
            "æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
            "å¦‚æœæ˜¯æƒ…èŠ‚è¾ƒè½»çš„å‘¢ï¼Ÿ",  # æµ‹è¯•å¤šè½®å¯¹è¯
            "ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for q in test_questions:
            print(f"ğŸ‘¤ ç”¨æˆ·: {q}")
            print("-" * 50)
            
            response = engine.query(q)
            
            print(f"ğŸ¤– åŠ©æ‰‹: {response.answer}")
            print(f"\nğŸ“Š ç½®ä¿¡åº¦: {response.confidence}")
            print(f"â“ ä¸ç¡®å®šå›ç­”: {response.is_uncertain}")
            
            if response.citations:
                print("\nğŸ“š å¼•ç”¨æ¥æº:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.source} ({citation.doc_type})")
                    if citation.metadata.get("accusation"):
                        print(f"       ç½ªå: {citation.metadata['accusation']}")
            
            print("\n" + "=" * 60 + "\n")
            
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬æ„å»ºå‘é‡åº“ã€‚")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
