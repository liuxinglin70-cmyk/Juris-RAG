"""
Juris-RAG æ ¸å¿ƒå¼•æ“æ¨¡å—
æ”¯æŒå¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ã€å¼•ç”¨æ¥æºæ˜¾ç¤ºã€æ‹’ç»ä¸ç¡®å®šå›ç­”
"""
import os
from typing import List, Dict, Tuple, Optional, Generator
from dataclasses import dataclass

from langchain_community.vectorstores import Chroma
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
try:
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError:  # langchain>=1.0 moved legacy chains to langchain_classic
    from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain_classic.chains.combine_documents import create_stuff_documents_chain
try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain_classic.schema import Document

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DB_PATH, EMBEDDING_MODEL, LLM_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, RETRIEVAL_TOP_K, RETRIEVAL_SCORE_THRESHOLD,
        LLM_TEMPERATURE, LLM_MAX_TOKENS, MAX_HISTORY_TURNS,
        CONFIDENCE_THRESHOLD, UNCERTAIN_RESPONSE
    )
except ImportError:
    # é»˜è®¤å›é€€é…ç½®
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    LLM_MODEL = "Qwen/Qwen2.5-7B-Instruct"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    RETRIEVAL_TOP_K = 5
    RETRIEVAL_SCORE_THRESHOLD = 0.3
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2048
    MAX_HISTORY_TURNS = 10
    CONFIDENCE_THRESHOLD = 0.4
    UNCERTAIN_RESPONSE = "æ ¹æ®ç°æœ‰æ³•å¾‹æ•°æ®åº“ï¼Œæˆ‘æ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚"


@dataclass
class Citation:
    """å¼•ç”¨æ¥æºæ•°æ®ç±»"""
    source: str
    doc_type: str
    content: str
    relevance_score: float
    metadata: Dict


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç±»"""
    answer: str
    citations: List[Citation]
    confidence: float
    is_uncertain: bool
    retrieved_docs: List[Document]


class JurisRAGEngine:
    """æ³•å¾‹RAGå¼•æ“"""
    
    def __init__(self, streaming: bool = True):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        """
        if not SILICONFLOW_API_KEY:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        self.streaming = streaming
        self.chat_history: List[Tuple[str, str]] = []
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embeddings()
        self._init_vectorstore()
        self._init_llm()
        self._init_chains()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY
        )
    
    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError(
                f"âŒ å‘é‡åº“ä¸å­˜åœ¨: {DB_PATH}\n"
                f"   è¯·å…ˆè¿è¡Œ: python -m src.data_processing"
            )
        
        self.vectorstore = Chroma(
            persist_directory=DB_PATH,
            embedding_function=self.embeddings
        )
        
        # é…ç½®æ£€ç´¢å™¨
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": RETRIEVAL_TOP_K,
                "score_threshold": RETRIEVAL_SCORE_THRESHOLD
            }
        )
    
    def _init_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY,
            streaming=self.streaming
        )
    
    def _init_chains(self):
        """åˆå§‹åŒ–RAGé“¾"""
        # 1. å†å²å¯¹è¯é‡å†™é“¾ - å°†ä¾èµ–ä¸Šä¸‹æ–‡çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹é—®é¢˜
        contextualize_q_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜æ”¹å†™åŠ©æ‰‹ã€‚ç»™å®šä¸€æ®µèŠå¤©å†å²å’Œç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ
è¯·åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦å¼•ç”¨äº†å†å²ä¿¡æ¯ï¼ˆå¦‚"å®ƒ"ã€"è¿™ä¸ªæ¡ˆå­"ã€"ä¸Šé¢æåˆ°çš„"ç­‰ï¼‰ã€‚

å¦‚æœæ˜¯ï¼Œè¯·å°†é—®é¢˜é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€æ— éœ€ä¸Šä¸‹æ–‡å³å¯ç†è§£çš„é—®é¢˜ã€‚
å¦‚æœé—®é¢˜å·²ç»æ˜¯ç‹¬ç«‹çš„ï¼Œè¯·åŸæ ·è¿”å›ã€‚

åªè¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Šã€‚"""
        
        contextualize_q_prompt = ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])
        
        self.history_aware_retriever = create_history_aware_retriever(
            self.llm, self.retriever, contextualize_q_prompt
        )
        
        # 2. æ³•å¾‹é—®ç­”é“¾ - æ ¸å¿ƒPrompt
        qa_system_prompt = """ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„ä¸­å›½æ³•å¾‹é—®ç­”AIã€‚ä½ çš„èŒè´£æ˜¯åŸºäºæ£€ç´¢åˆ°çš„æ³•å¾‹æ–‡æ¡£ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„æ³•å¾‹å’¨è¯¢ã€‚

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. **ä¸¥æ ¼åŸºäºè¯æ®**ï¼šåªèƒ½æ ¹æ®ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œç»ä¸ç¼–é€ æˆ–æ¨æµ‹
2. **æ˜ç¡®å¼•ç”¨æ¥æº**ï¼šæ¯ä¸ªé‡è¦è®ºè¿°åå¿…é¡»æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º [æ¥æºX]
3. **æ‰¿è®¤ä¸ç¡®å®šæ€§**ï¼šå¦‚æœæ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜

ã€å›ç­”æ ¼å¼è¦æ±‚ã€‘
1. å…ˆç»™å‡ºç›´æ¥å›ç­”ï¼ˆ1-2å¥è¯æ¦‚æ‹¬ï¼‰
2. å†åˆ†ç‚¹è¯¦ç»†è¯´æ˜ï¼ˆå¦‚æ¶‰åŠæ³•æ¡ï¼Œé€æ¡å¼•ç”¨ï¼›å¦‚æ¶‰åŠæ¡ˆä¾‹ï¼Œè¯´æ˜åˆ¤ä¾‹ï¼‰
3. æœ€åç»™å‡ºæ³¨æ„äº‹é¡¹æˆ–å»ºè®®

ã€å¼•ç”¨æ ¼å¼ã€‘
- æ³•æ¡å¼•ç”¨ï¼šæ ¹æ®ã€Šåˆ‘æ³•ã€‹ç¬¬Xæ¡è§„å®šï¼Œ... [æ¥æº1]
- æ¡ˆä¾‹å¼•ç”¨ï¼šåœ¨ç±»ä¼¼æ¡ˆä¾‹ä¸­ï¼Œ... [æ¥æº2]

ã€ç‰¹æ®Šæƒ…å†µå¤„ç†ã€‘
- å¦‚æœé—®é¢˜è¶…å‡ºæ³•å¾‹èŒƒå›´ï¼Œç¤¼è²Œè¯´æ˜å¹¶å»ºè®®å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆ
- å¦‚æœæ£€ç´¢ç»“æœä¸ç›¸å…³æˆ–ä¸å……åˆ†ï¼Œç›´æ¥è¯´"æ ¹æ®ç°æœ‰æ³•å¾‹æ•°æ®åº“ï¼Œæ— æ³•å‡†ç¡®å›ç­”æ­¤é—®é¢˜"
- ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„æ³•æ¡æˆ–æ¡ˆä¾‹

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context}"""
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ])
        
        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)
        
        # 3. ç»„åˆæœ€ç»ˆRAGé“¾
        self.rag_chain = create_retrieval_chain(
            self.history_aware_retriever,
            question_answer_chain
        )
    
    def _format_chat_history(self) -> List:
        """æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºLangChainæ¶ˆæ¯æ ¼å¼"""
        messages = []
        for human, ai in self.chat_history[-MAX_HISTORY_TURNS:]:
            messages.append(HumanMessage(content=human))
            messages.append(AIMessage(content=ai))
        return messages
    
    def _extract_citations(self, docs: List[Document]) -> List[Citation]:
        """ä»æ£€ç´¢æ–‡æ¡£ä¸­æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, doc in enumerate(docs):
            citation = Citation(
                source=doc.metadata.get("source", "æœªçŸ¥æ¥æº"),
                doc_type=doc.metadata.get("type", "unknown"),
                content=doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                relevance_score=doc.metadata.get("relevance_score", 0.0),
                metadata=doc.metadata
            )
            citations.append(citation)
        return citations
    
    def _calculate_confidence(self, docs: List[Document]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0
        
        # åŸºäºæ£€ç´¢æ–‡æ¡£æ•°é‡å’Œç›¸å…³æ€§è®¡ç®—ç½®ä¿¡åº¦
        doc_count_score = min(len(docs) / RETRIEVAL_TOP_K, 1.0)
        
        # å¦‚æœæœ‰ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨å¹³å‡åˆ†æ•°
        scores = [doc.metadata.get("relevance_score", 0.5) for doc in docs]
        avg_score = sum(scores) / len(scores) if scores else 0.5
        
        # ç»¼åˆç½®ä¿¡åº¦
        confidence = 0.4 * doc_count_score + 0.6 * avg_score
        return round(confidence, 2)
    
    def query(self, question: str) -> RAGResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            RAGResponse: åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯
        """
        # æ ¼å¼åŒ–å†å²å¯¹è¯
        chat_history = self._format_chat_history()
        
        # è°ƒç”¨RAGé“¾
        response = self.rag_chain.invoke({
            "input": question,
            "chat_history": chat_history
        })
        
        answer = response.get("answer", "")
        docs = response.get("context", [])
        
        # æå–å¼•ç”¨
        citations = self._extract_citations(docs)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(docs)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸ç¡®å®šå›ç­”
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        # å¦‚æœç½®ä¿¡åº¦è¿‡ä½ï¼Œä½¿ç”¨ä¸ç¡®å®šå›ç­”æ¨¡æ¿
        if is_uncertain and len(docs) == 0:
            answer = UNCERTAIN_RESPONSE
        
        # æ›´æ–°å¯¹è¯å†å²
        self.chat_history.append((question, answer))
        
        return RAGResponse(
            answer=answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def query_stream(self, question: str) -> Generator[str, None, RAGResponse]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            str: ç­”æ¡ˆç‰‡æ®µ
            
        Returns:
            RAGResponse: å®Œæ•´å“åº”ï¼ˆåœ¨ç”Ÿæˆå™¨ç»“æŸæ—¶ï¼‰
        """
        chat_history = self._format_chat_history()
        
        # å…ˆè·å–æ£€ç´¢ç»“æœ
        docs = self.history_aware_retriever.invoke({
            "input": question,
            "chat_history": chat_history
        })
        
        citations = self._extract_citations(docs)
        confidence = self._calculate_confidence(docs)
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        if is_uncertain and len(docs) == 0:
            yield UNCERTAIN_RESPONSE
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æµå¼ç”Ÿæˆå›ç­”
        full_answer = ""
        for chunk in self.rag_chain.stream({
            "input": question,
            "chat_history": chat_history
        }):
            if "answer" in chunk:
                full_answer += chunk["answer"]
                yield chunk["answer"]
        
        self.chat_history.append((question, full_answer))
        
        return RAGResponse(
            answer=full_answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
    
    def get_history(self) -> List[Tuple[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.chat_history.copy()
    
    def search_similar(self, query: str, k: int = 5) -> List[Document]:
        """
        ç›´æ¥æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆä¸ç»è¿‡LLMï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            
        Returns:
            List[Document]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        return self.vectorstore.similarity_search(query, k=k)


# ä¾¿æ·å‡½æ•°ï¼šè·å–é»˜è®¤RAGå¼•æ“å®ä¾‹
_default_engine: Optional[JurisRAGEngine] = None

def get_rag_engine(streaming: bool = True) -> JurisRAGEngine:
    """è·å–RAGå¼•æ“å•ä¾‹"""
    global _default_engine
    if _default_engine is None:
        _default_engine = JurisRAGEngine(streaming=streaming)
    return _default_engine


def get_retriever():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–æ£€ç´¢å™¨"""
    engine = get_rag_engine()
    return engine.retriever


def get_rag_chain():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–RAGé“¾"""
    engine = get_rag_engine()
    return engine.rag_chain


# --- å‘½ä»¤è¡Œæµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Juris-RAG å¼•æ“...")
    
    try:
        engine = JurisRAGEngine(streaming=False)
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼\n")
        
        # æµ‹è¯•é—®é¢˜åˆ—è¡¨
        test_questions = [
            "æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
            "å¦‚æœæ˜¯æƒ…èŠ‚è¾ƒè½»çš„å‘¢ï¼Ÿ",  # æµ‹è¯•å¤šè½®å¯¹è¯
            "ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for q in test_questions:
            print(f"ğŸ‘¤ ç”¨æˆ·: {q}")
            print("-" * 50)
            
            response = engine.query(q)
            
            print(f"ğŸ¤– åŠ©æ‰‹: {response.answer}")
            print(f"\nğŸ“Š ç½®ä¿¡åº¦: {response.confidence}")
            print(f"â“ ä¸ç¡®å®šå›ç­”: {response.is_uncertain}")
            
            if response.citations:
                print("\nğŸ“š å¼•ç”¨æ¥æº:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.source} ({citation.doc_type})")
                    if citation.metadata.get("accusation"):
                        print(f"       ç½ªå: {citation.metadata['accusation']}")
            
            print("\n" + "=" * 60 + "\n")
            
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬æ„å»ºå‘é‡åº“ã€‚")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
