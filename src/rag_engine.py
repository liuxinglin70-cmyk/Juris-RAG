"""
Juris-RAG æ ¸å¿ƒå¼•æ“æ¨¡å—
æ”¯æŒå¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ã€å¼•ç”¨æ¥æºæ˜¾ç¤ºã€æ‹’ç»ä¸ç¡®å®šå›ç­”
"""
import os
import re
from typing import List, Dict, Tuple, Optional, Generator
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time

# ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆ langchain-chromaï¼Œå…¼å®¹æ—§ç‰ˆ langchain-community
try:
    from langchain_chroma import Chroma  # type: ignore
except ImportError:  # pragma: no cover - å…¼å®¹æ—§ç¯å¢ƒ
    from langchain_community.vectorstores import Chroma
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:  # fallback for older installs
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# å°è¯•å¯¼å…¥é“¾ç»„ä»¶ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
try:
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError:
    try:
        from langchain_core.runnables import RunnablePassthrough
        # å¦‚æœæ²¡æœ‰ä¼ ç»Ÿchainsï¼Œä½¿ç”¨ç®€åŒ–å®ç°
        create_history_aware_retriever = None
        create_retrieval_chain = None
        create_stuff_documents_chain = None
    except ImportError:
        pass

try:
    from langchain_core.documents import Document
except ImportError:  # fallback for older langchain versions
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain_community.docstore.document import Document

# å¯¼å…¥é…ç½®
try:
    from src.config import (
        DB_PATH, EMBEDDING_MODEL, LLM_MODEL, SILICONFLOW_API_KEY,
        SILICONFLOW_BASE_URL, RETRIEVAL_TOP_K, RETRIEVAL_SCORE_THRESHOLD,
        LLM_TEMPERATURE, LLM_MAX_TOKENS, MAX_HISTORY_TURNS,
        CONFIDENCE_THRESHOLD, UNCERTAIN_RESPONSE,
        ENABLE_RERANKER, RERANKER_TOP_K, RERANKER_THRESHOLD,
        ENABLE_CACHE, CACHE_MAX_SIZE, ENABLE_HALLUCINATION_CHECK,
        SELECTIVE_HALLUCINATION_CHECK, HALLUCINATION_CHECK_HIGH_RISK,
        HALLUCINATION_CHECK_HIGH_RISK_USE_LLM, HALLUCINATION_CHECK_NORMAL,
        HALLUCINATION_CHECK_TIMEOUT, ENABLE_PARALLEL_RETRIEVAL,
        MAX_PARALLEL_WORKERS, PARALLEL_RETRIEVAL_TIMEOUT,
        ENABLE_STREAM_GENERATION, STREAM_CHUNK_SIZE
    )
except ImportError:
    # é»˜è®¤å›é€€é…ç½®
    DB_PATH = "./data/vector_db"
    EMBEDDING_MODEL = "BAAI/bge-m3"
    LLM_MODEL = "Qwen/Qwen3-8B"
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
    SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
    RETRIEVAL_TOP_K = 8
    RETRIEVAL_SCORE_THRESHOLD = 0.2
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2048
    MAX_HISTORY_TURNS = 10
    CONFIDENCE_THRESHOLD = 0.4
    UNCERTAIN_RESPONSE = "æ ¹æ®ç°æœ‰æ³•å¾‹æ•°æ®åº“ï¼Œæˆ‘æ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚"
    ENABLE_RERANKER = True
    RERANKER_TOP_K = 5
    RERANKER_THRESHOLD = 0.4
    ENABLE_CACHE = True
    CACHE_MAX_SIZE = 100
    ENABLE_HALLUCINATION_CHECK = True

# å¯¼å…¥Rerankeræ¨¡å—
try:
    from src.reranker import (
        get_reranker, rerank_documents, check_scope, check_hallucination,
        TTLCache, RerankedDocument, ScopeCheckResult, HallucinationCheckResult
    )
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False
    print("âš ï¸ Rerankeræ¨¡å—æœªåŠ è½½ï¼Œå°†ä½¿ç”¨åŸºç¡€æ£€ç´¢ç­–ç•¥")

# ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹é…ç½® ====================
# éåˆ‘æ³•é¢†åŸŸå…³é”®è¯ - æ£€æµ‹åˆ°è¿™äº›è¯æ—¶è§¦å‘è¶…èŒƒå›´æ‹’ç»
OUT_OF_SCOPE_KEYWORDS = [
    # æ°‘æ³•ç›¸å…³
    "æ°‘æ³•å…¸", "åˆåŒæ³•", "å©šå§»æ³•", "ç»§æ‰¿æ³•", "ç‰©æƒæ³•", "ä¾µæƒè´£ä»»",
    "æ°‘äº‹çº çº·", "ç¦»å©š", "æŠšå…»æƒ", "é—äº§ç»§æ‰¿", "æˆ¿äº§çº çº·", "å€ºåŠ¡çº çº·",
    "å€Ÿæ¬¾åˆåŒ", "ç§ŸèµåˆåŒ", "ä¹°å–åˆåŒ", "åŠ³åŠ¡åˆåŒ",
    # å•†æ³•ç›¸å…³  
    "å…¬å¸æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ç¥¨æ®æ³•", "ç ´äº§æ³•",
    "è‚¡ç¥¨", "åŸºé‡‘", "æŠ•èµ„ç†è´¢", "ä¸Šå¸‚å…¬å¸", "è‘£äº‹ä¼š", "è‚¡ä¸œ",
    "å•†ä¸šç§˜å¯†", "çŸ¥è¯†äº§æƒ", "ä¸“åˆ©", "å•†æ ‡", "è‘—ä½œæƒ",
    # è¡Œæ”¿æ³•ç›¸å…³
    "è¡Œæ”¿å¤„ç½š", "è¡Œæ”¿å¤è®®", "è¡Œæ”¿è¯‰è®¼", "æ‹†è¿", "åœŸåœ°å¾æ”¶",
    "è¡Œæ”¿è®¸å¯", "è¡Œæ”¿å¼ºåˆ¶", "å…¬åŠ¡å‘˜", "äº‹ä¸šç¼–",
    # åŠ³åŠ¨æ³•ç›¸å…³
    "åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "ç¤¾ä¿", "å·¥ä¼¤", "åŠ³åŠ¨ä»²è£",
    "åŠ ç­è´¹", "å¹´å‡", "è¾é€€èµ”å¿", "äº”é™©ä¸€é‡‘",
    # å…¶ä»–éåˆ‘æ³•
    "ç¨æ³•", "æµ·å…³", "ç¯ä¿æ³•", "é£Ÿå“å®‰å…¨",
    "åŒ»ç–—çº çº·", "åŒ»æ‚£å…³ç³»", "äº¤é€šäº‹æ•…èµ”å¿"
]

# è¶…èŒƒå›´æ‹’ç»å“åº”æ¨¡æ¿
OUT_OF_SCOPE_RESPONSE = """æŠ±æ­‰ï¼Œæ‚¨çš„é—®é¢˜æ¶‰åŠ**{detected_domain}**é¢†åŸŸï¼Œä¸åœ¨æœ¬ç³»ç»Ÿçš„æœåŠ¡èŒƒå›´å†…ã€‚

**æœ¬ç³»ç»Ÿä¸“æ³¨äºä¸­å›½åˆ‘æ³•é¢†åŸŸ**ï¼ŒåŒ…æ‹¬ï¼š
- å„ç±»åˆ‘äº‹çŠ¯ç½ªçš„è®¤å®šä¸é‡åˆ‘
- åˆ‘äº‹è´£ä»»å¹´é¾„ã€è‡ªé¦–ã€ç«‹åŠŸç­‰æƒ…èŠ‚
- æ­£å½“é˜²å«ã€ç´§æ€¥é¿é™©ç­‰å…è´£äº‹ç”±
- åˆ‘äº‹æ¡ˆä¾‹çš„åˆ¤å†³å‚è€ƒ

**å»ºè®®**ï¼š
- æ°‘äº‹é—®é¢˜è¯·å’¨è¯¢æ°‘äº‹å¾‹å¸ˆæˆ–æŸ¥é˜…æ°‘æ³•å…¸
- å•†äº‹é—®é¢˜è¯·å’¨è¯¢å…¬å¸æ³•/è¯åˆ¸æ³•å¾‹å¸ˆ
- åŠ³åŠ¨é—®é¢˜è¯·å’¨è¯¢åŠ³åŠ¨ä»²è£éƒ¨é—¨æˆ–åŠ³åŠ¨å¾‹å¸ˆ
- è¡Œæ”¿é—®é¢˜è¯·å’¨è¯¢è¡Œæ”¿æ³•å¾‹å¸ˆæˆ–ç›¸å…³æ”¿åºœéƒ¨é—¨

å¦‚æœæ‚¨æœ‰**åˆ‘æ³•ç›¸å…³é—®é¢˜**ï¼Œæ¬¢è¿ç»§ç»­å’¨è¯¢ï¼"""

# æœ€ä½ç›¸å…³æ€§é˜ˆå€¼ - ä½äºæ­¤å€¼è§†ä¸ºè¶…èŒƒå›´
MIN_RELEVANCE_THRESHOLD = 0.65


@dataclass
class Citation:
    """å¼•ç”¨æ¥æºæ•°æ®ç±»"""
    source: str
    doc_type: str
    content: str
    relevance_score: float
    metadata: Dict


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç±»"""
    answer: str
    citations: List[Citation]
    confidence: float
    is_uncertain: bool
    retrieved_docs: List[Document]


class JurisRAGEngine:
    """æ³•å¾‹RAGå¼•æ“ - æ”¯æŒå¤šé¢†åŸŸ"""
    
    def _try_load_multi_domain_vectorstores(self) -> bool:
        """
        å°è¯•åŠ è½½å¤šé¢†åŸŸå‘é‡åº“
        
        Returns:
            True å¦‚æœæˆåŠŸåŠ è½½å¤šé¢†åŸŸï¼ŒFalse å¦åˆ™
        """
        legal_domains = {
            'criminal': 'åˆ‘æ³•',
            'civil': 'æ°‘æ³•',
            'commercial': 'å•†æ³•',
            'administrative': 'è¡Œæ”¿æ³•',
            'labor': 'åŠ³åŠ¨æ³•'
        }
        
        self.vectorstores_multi = {}
        loaded_count = 0
        
        for domain_key, domain_name in legal_domains.items():
            domain_db_path = os.path.join(DB_PATH, domain_key)
            if os.path.exists(domain_db_path):
                try:
                    vs = Chroma(
                        persist_directory=domain_db_path,
                        embedding_function=self.embeddings
                    )
                    self.vectorstores_multi[domain_key] = {
                        'vectorstore': vs,
                        'retriever': vs.as_retriever(search_type="similarity", search_kwargs={"k": RETRIEVAL_TOP_K * 2}),
                        'name': domain_name
                    }
                    loaded_count += 1
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åŠ è½½ {domain_name} å‘é‡åº“: {e}")
        
        if loaded_count > 0:
            print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªé¢†åŸŸçš„å‘é‡åº“")
            return True
        return False
    
    def __init__(self, streaming: bool = True):
        """
        åˆå§‹åŒ–RAGå¼•æ“
        
        Args:
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
        """
        if not SILICONFLOW_API_KEY:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼")
        
        self.streaming = streaming
        self.chat_history: List[Tuple[str, str]] = []
        self.vectorstore = None  # é˜²æ­¢AttributeError
        self.multi_domain_mode = False
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_embeddings()
        self._init_vectorstore()
        self._init_llm()
        self._init_chains()
    
    # ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ ====================
    def _detect_out_of_scope(self, query: str) -> Tuple[bool, str]:
        """
        æ£€æµ‹é—®é¢˜æ˜¯å¦è¶…å‡ºåˆ‘æ³•èŒƒå›´
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦è¶…èŒƒå›´, æ£€æµ‹åˆ°çš„é¢†åŸŸ)
        """
        query_lower = query.lower()
        
        # æ£€æµ‹åˆ°çš„éåˆ‘æ³•é¢†åŸŸ
        detected_domains = []
        
        # æŒ‰é¢†åŸŸåˆ†ç»„æ£€æµ‹
        domain_keywords = {
            "æ°‘æ³•/åˆåŒæ³•": ["æ°‘æ³•å…¸", "åˆåŒæ³•", "å©šå§»æ³•", "ç»§æ‰¿æ³•", "ç‰©æƒæ³•", "ä¾µæƒè´£ä»»",
                         "æ°‘äº‹çº çº·", "ç¦»å©š", "æŠšå…»æƒ", "é—äº§ç»§æ‰¿", "æˆ¿äº§çº çº·", "å€ºåŠ¡çº çº·",
                         "å€Ÿæ¬¾åˆåŒ", "ç§ŸèµåˆåŒ", "ä¹°å–åˆåŒ", "åŠ³åŠ¡åˆåŒ"],
            "å•†æ³•/å…¬å¸æ³•": ["å…¬å¸æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ç¥¨æ®æ³•", "ç ´äº§æ³•",
                         "è‚¡ç¥¨", "åŸºé‡‘", "æŠ•èµ„ç†è´¢", "ä¸Šå¸‚å…¬å¸", "è‘£äº‹ä¼š", "è‚¡ä¸œ",
                         "å•†ä¸šç§˜å¯†", "çŸ¥è¯†äº§æƒ", "ä¸“åˆ©", "å•†æ ‡", "è‘—ä½œæƒ"],
            "è¡Œæ”¿æ³•": ["è¡Œæ”¿å¤„ç½š", "è¡Œæ”¿å¤è®®", "è¡Œæ”¿è¯‰è®¼", "æ‹†è¿", "åœŸåœ°å¾æ”¶",
                     "è¡Œæ”¿è®¸å¯", "è¡Œæ”¿å¼ºåˆ¶", "å…¬åŠ¡å‘˜", "äº‹ä¸šç¼–"],
            "åŠ³åŠ¨æ³•": ["åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "ç¤¾ä¿", "å·¥ä¼¤", "åŠ³åŠ¨ä»²è£",
                     "åŠ ç­è´¹", "å¹´å‡", "è¾é€€èµ”å¿", "äº”é™©ä¸€é‡‘"],
            "å…¶ä»–éåˆ‘æ³•": ["ç¨æ³•", "æµ·å…³", "ç¯ä¿æ³•", "é£Ÿå“å®‰å…¨",
                        "åŒ»ç–—çº çº·", "åŒ»æ‚£å…³ç³»", "äº¤é€šäº‹æ•…èµ”å¿"]
        }
        
        for domain, keywords in domain_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    detected_domains.append(domain)
                    break
        
        if detected_domains:
            # å»é‡å¹¶è¿”å›ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„é¢†åŸŸ
            return True, detected_domains[0]
        
        return False, ""
    
    def _is_low_relevance(self, docs: List[Document]) -> bool:
        """
        æ£€æµ‹æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§æ˜¯å¦è¿‡ä½
        å¦‚æœæ‰€æœ‰æ–‡æ¡£çš„ç›¸å…³æ€§éƒ½ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºè¶…å‡ºèŒƒå›´
        
        Args:
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            
        Returns:
            bool: æ˜¯å¦ç›¸å…³æ€§è¿‡ä½
        """
        if not docs:
            return True
        
        # è®¡ç®—æ³•æ¡æ–‡æ¡£çš„å¹³å‡ç›¸å…³æ€§
        statute_scores = []
        for doc in docs:
            if doc.metadata.get("type") == "statute":
                score = doc.metadata.get("relevance_score", 1.0)
                # ChromaDBåˆ†æ•°è¶Šä½è¶Šç›¸å…³
                relevance = 1 - min(score / 2, 1.0)
                statute_scores.append(relevance)
        
        if not statute_scores:
            # æ²¡æœ‰æ£€ç´¢åˆ°æ³•æ¡ï¼Œå¯èƒ½æ˜¯è¶…èŒƒå›´
            return True
        
        avg_relevance = sum(statute_scores) / len(statute_scores)
        return avg_relevance < MIN_RELEVANCE_THRESHOLD
    
    def _should_check_hallucination(self, is_out_of_scope: bool, docs: List[Document]) -> Tuple[bool, bool]:
        """
        ã€æ–¹æ¡ˆDã€‘æ··åˆæ£€æµ‹ç­–ç•¥ï¼šæ ¹æ®é—®é¢˜ç±»å‹é‡‡ç”¨ä¸åŒçš„æ£€æµ‹æ–¹æ³•
        
        Args:
            is_out_of_scope: æ˜¯å¦è¢«æ£€æµ‹ä¸ºè¶…èŒƒå›´é—®é¢˜
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Tuple[bool, bool]: (æ˜¯å¦åº”è¯¥æ£€æµ‹, æ˜¯å¦ä½¿ç”¨LLMæ·±åº¦æ£€æµ‹)
            - (False, False): ä¸æ£€æµ‹ï¼ˆæ­£å¸¸é—®é¢˜ï¼Œå¿«é€Ÿæ¨¡å¼ï¼‰
            - (True, False): æœ¬åœ°å¿«é€Ÿæ£€æµ‹ï¼ˆä¸æ¨èï¼Œæ˜“è¯¯æŠ¥ï¼‰
            - (True, True): LLMæ·±åº¦æ£€æµ‹ï¼ˆé«˜é£é™©é—®é¢˜ï¼Œå‡†ç¡®æ¨¡å¼ï¼‰
        """
        if not SELECTIVE_HALLUCINATION_CHECK:
            # å¦‚æœç¦ç”¨é€‰æ‹©æ€§æ£€æµ‹ï¼Œä½¿ç”¨å…¨å±€é…ç½®
            return ENABLE_HALLUCINATION_CHECK, False
        
        # ğŸ”´ è¶…èŒƒå›´é—®é¢˜ï¼šä¸éœ€è¦å¹»è§‰æ£€æµ‹ï¼ˆå·²è¢«å¿«é€Ÿæ‹’ç­”ï¼‰
        if is_out_of_scope:
            return False, False
        
        # ğŸŸ¡ é«˜é£é™©æƒ…å†µ1ï¼šæ£€ç´¢æ–‡æ¡£è¿‡å°‘ï¼ˆ<2ä¸ªï¼‰â†’ LLMæ·±åº¦æ£€æµ‹
        if len(docs) < 2:
            return HALLUCINATION_CHECK_HIGH_RISK, HALLUCINATION_CHECK_HIGH_RISK_USE_LLM
        
        # ğŸŸ¡ é«˜é£é™©æƒ…å†µ2ï¼šç›¸å…³æ€§è¿‡ä½ â†’ LLMæ·±åº¦æ£€æµ‹
        if self._is_low_relevance(docs):
            return HALLUCINATION_CHECK_HIGH_RISK, HALLUCINATION_CHECK_HIGH_RISK_USE_LLM
        
        # âœ… æ­£å¸¸æƒ…å†µï¼šå……è¶³çš„é«˜è´¨é‡æ–‡æ¡£ â†’ ä¸æ£€æµ‹ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        return HALLUCINATION_CHECK_NORMAL, False
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY
        )
    
    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError(
                f"âŒ å‘é‡åº“ä¸å­˜åœ¨: {DB_PATH}\n"
                f"   è¯·å…ˆè¿è¡Œ: python -m src.data_processing"
            )
        
        # å°è¯•åŠ è½½å¤šé¢†åŸŸå‘é‡åº“
        self.multi_domain_mode = self._try_load_multi_domain_vectorstores()
        
        if self.multi_domain_mode:
            print("âœ… å¤šé¢†åŸŸæ¨¡å¼å·²å¯åŠ¨")
        else:
            # å›é€€åˆ°å•é¢†åŸŸæ¨¡å¼ï¼ˆåˆ‘æ³•ï¼‰
            print("âš ï¸  æœªæ£€æµ‹åˆ°å¤šé¢†åŸŸå‘é‡åº“ï¼Œä½¿ç”¨å•é¢†åŸŸæ¨¡å¼ï¼ˆåˆ‘æ³•ï¼‰")
            self.vectorstore = Chroma(
                persist_directory=DB_PATH,
                embedding_function=self.embeddings
            )
            
            # ä½¿ç”¨æ›´å®½æ¾çš„ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œåç»­é€šè¿‡åå¤„ç†è¿‡æ»¤
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": RETRIEVAL_TOP_K * 2  # æ£€ç´¢æ›´å¤šï¼Œåå¤„ç†ç­›é€‰
                }
            )
    
    def _extract_crime_keywords(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–ç½ªåå…³é”®è¯ï¼Œè¿”å›å¤šä¸ªå¢å¼ºæŸ¥è¯¢
        æ–¹æ¡ˆCå¢å¼ºï¼šæ‰©å±•ç½ªåæ˜ å°„ï¼Œæ”¯æŒæ›´å¤šç½ªåï¼ˆåŒ…æ‹¬å°‘è§ç½ªåï¼‰
        """
        # å¸¸è§ç½ªåå…³é”®è¯æ˜ å°„ - åŒ…å«æ¡æ¬¾å·å’Œæ ¸å¿ƒæè¿°è¯ï¼ˆå…¨é¢æ‰©å±•ç‰ˆï¼‰
        crime_mappings = {
            # ==================== ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©ç½ªï¼ˆç¬¬å››ç« ï¼‰====================
            "æ•…æ„æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘ æ— æœŸå¾’åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ© æ•…æ„æ€äºº"],
            "æ€äºº": ["ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äºº æ­»åˆ‘", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ©"],
            "è¿‡å¤±è‡´äººæ­»äº¡": ["ç¬¬äºŒç™¾ä¸‰åä¸‰æ¡ è¿‡å¤±è‡´äººæ­»äº¡ ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹"],
            "æ•…æ„ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³ è½»ä¼¤ é‡ä¼¤ è‡´äººæ­»äº¡"],
            "ä¼¤å®³": ["ç¬¬äºŒç™¾ä¸‰åå››æ¡ æ•…æ„ä¼¤å®³"],
            "è¿‡å¤±è‡´äººé‡ä¼¤": ["ç¬¬äºŒç™¾ä¸‰åäº”æ¡ è¿‡å¤±è‡´äººé‡ä¼¤"],
            "å¼ºå¥¸": ["ç¬¬äºŒç™¾ä¸‰åå…­æ¡ å¼ºå¥¸ æš´åŠ› èƒè¿« å¦‡å¥³ å¥¸æ·«å¹¼å¥³", "ä¾µçŠ¯å…¬æ°‘äººèº«æƒåˆ© å¼ºå¥¸ç½ª ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹"],
            "å¼ºå¥¸ç½ª": ["ç¬¬äºŒç™¾ä¸‰åå…­æ¡ å¼ºå¥¸ æš´åŠ› èƒè¿« å¦‡å¥³", "å¥¸æ·«ä¸æ»¡åå››å‘¨å²å¹¼å¥³ åå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘"],
            "å¥¸æ·«å¹¼å¥³": ["ç¬¬äºŒç™¾ä¸‰åå…­æ¡ å¥¸æ·«ä¸æ»¡åå››å‘¨å² ä»é‡å¤„ç½š"],
            "å¼ºåˆ¶çŒ¥äºµ": ["ç¬¬äºŒç™¾ä¸‰åä¸ƒæ¡ å¼ºåˆ¶çŒ¥äºµ ä¾®è¾±å¦‡å¥³ èšä¼—"],
            "çŒ¥äºµ": ["ç¬¬äºŒç™¾ä¸‰åä¸ƒæ¡ å¼ºåˆ¶çŒ¥äºµ ä¾®è¾± çŒ¥äºµå„¿ç«¥"],
            "çŒ¥äºµå„¿ç«¥": ["ç¬¬äºŒç™¾ä¸‰åä¸ƒæ¡ çŒ¥äºµå„¿ç«¥ ä»é‡å¤„ç½š"],
            "éæ³•æ‹˜ç¦": ["ç¬¬äºŒç™¾ä¸‰åå…«æ¡ éæ³•æ‹˜ç¦ å‰¥å¤ºäººèº«è‡ªç”±"],
            "ç»‘æ¶": ["ç¬¬äºŒç™¾ä¸‰åä¹æ¡ ç»‘æ¶ å‹’ç´¢è´¢ç‰© äººè´¨ åå¹´ä»¥ä¸Š"],
            "ç»‘æ¶ç½ª": ["ç¬¬äºŒç™¾ä¸‰åä¹æ¡ ç»‘æ¶ å‹’ç´¢è´¢ç‰© æ€å®³è¢«ç»‘æ¶äºº æ­»åˆ‘"],
            "æ‹å–": ["ç¬¬äºŒç™¾å››åæ¡ æ‹å–å¦‡å¥³å„¿ç«¥ äº”å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹"],
            "æ‹å–å¦‡å¥³": ["ç¬¬äºŒç™¾å››åæ¡ æ‹å–å¦‡å¥³ å„¿ç«¥ äººå£è´©å–"],
            "æ‹å–å„¿ç«¥": ["ç¬¬äºŒç™¾å››åæ¡ æ‹å–å„¿ç«¥ äº”å¹´ä»¥ä¸Š"],
            "æ”¶ä¹°è¢«æ‹å–": ["ç¬¬äºŒç™¾å››åä¸€æ¡ æ”¶ä¹°è¢«æ‹å–çš„å¦‡å¥³å„¿ç«¥"],
            "æ‹éª—å„¿ç«¥": ["ç¬¬äºŒç™¾å…­åäºŒæ¡ æ‹éª—å„¿ç«¥ äº”å¹´ä»¥ä¸‹"],
            "ä¾®è¾±": ["ç¬¬äºŒç™¾å››åå…­æ¡ ä¾®è¾±ç½ª è¯½è°¤ç½ª"],
            "è¯½è°¤": ["ç¬¬äºŒç™¾å››åå…­æ¡ è¯½è°¤ç½ª æé€ äº‹å®"],
            "åˆ‘è®¯é€¼ä¾›": ["ç¬¬äºŒç™¾å››åä¸ƒæ¡ åˆ‘è®¯é€¼ä¾› æš´åŠ›å–è¯"],
            "è™å¾…": ["ç¬¬äºŒç™¾å…­åæ¡ è™å¾…ç½ª å®¶åº­æˆå‘˜"],
            "é—å¼ƒ": ["ç¬¬äºŒç™¾å…­åä¸€æ¡ é—å¼ƒç½ª è´Ÿæœ‰æ‰¶å…»ä¹‰åŠ¡"],
            "é‡å©š": ["ç¬¬äºŒç™¾äº”åå…«æ¡ é‡å©šç½ª æœ‰é…å¶è€Œé‡å©š"],
            
            # ==================== ä¾µçŠ¯è´¢äº§ç½ªï¼ˆç¬¬äº”ç« ï¼‰====================
            "æŠ¢åŠ«": ["ç¬¬äºŒç™¾å…­åä¸‰æ¡ æŠ¢åŠ« æš´åŠ› èƒè¿« ä¾µçŠ¯è´¢äº§ç½ª ä¸‰å¹´ä»¥ä¸Š"],
            "æŠ¢åŠ«ç½ª": ["ç¬¬äºŒç™¾å…­åä¸‰æ¡ æŠ¢åŠ« å…¥æˆ·æŠ¢åŠ« æŠ¢åŠ«é“¶è¡Œ åå¹´ä»¥ä¸Š"],
            "ç›—çªƒ": ["ç¬¬äºŒç™¾å…­åå››æ¡ ç›—çªƒ æ•°é¢è¾ƒå¤§ æ•°é¢å·¨å¤§ ä¾µçŠ¯è´¢äº§ç½ª"],
            "ç›—çªƒç½ª": ["ç¬¬äºŒç™¾å…­åå››æ¡ ç›—çªƒ å…¥æˆ·ç›—çªƒ æ‰’çªƒ å¤šæ¬¡ç›—çªƒ"],
            "è¯ˆéª—": ["ç¬¬äºŒç™¾å…­åå…­æ¡ è¯ˆéª— æ•°é¢è¾ƒå¤§ æ•°é¢å·¨å¤§"],
            "è¯ˆéª—ç½ª": ["ç¬¬äºŒç™¾å…­åå…­æ¡ è¯ˆéª— è™šæ„äº‹å® éšç’çœŸç›¸"],
            "æŠ¢å¤º": ["ç¬¬äºŒç™¾å…­åä¸ƒæ¡ æŠ¢å¤º å…¬ç„¶å¤ºå–"],
            "æŠ¢å¤ºç½ª": ["ç¬¬äºŒç™¾å…­åä¸ƒæ¡ æŠ¢å¤º æºå¸¦å‡¶å™¨æŠ¢å¤º"],
            "èšä¼—å“„æŠ¢": ["ç¬¬äºŒç™¾å…­åå…«æ¡ èšä¼—å“„æŠ¢ å…¬ç§è´¢ç‰©"],
            "ä¾µå ": ["ç¬¬äºŒç™¾ä¸ƒåæ¡ ä¾µå  ä»£ä¸ºä¿ç®¡ æ‹’ä¸é€€è¿˜"],
            "èŒåŠ¡ä¾µå ": ["ç¬¬äºŒç™¾ä¸ƒåä¸€æ¡ èŒåŠ¡ä¾µå  å…¬å¸ä¼ä¸š"],
            "æŒªç”¨èµ„é‡‘": ["ç¬¬äºŒç™¾ä¸ƒåäºŒæ¡ æŒªç”¨èµ„é‡‘ å½’ä¸ªäººä½¿ç”¨"],
            "æ•²è¯ˆå‹’ç´¢": ["ç¬¬äºŒç™¾ä¸ƒåå››æ¡ æ•²è¯ˆå‹’ç´¢ å¨èƒ è¦æŒŸ"],
            "æ•²è¯ˆå‹’ç´¢ç½ª": ["ç¬¬äºŒç™¾ä¸ƒåå››æ¡ æ•²è¯ˆå‹’ç´¢ æ•°é¢è¾ƒå¤§"],
            "æ•…æ„æ¯åè´¢ç‰©": ["ç¬¬äºŒç™¾ä¸ƒåäº”æ¡ æ•…æ„æ¯åè´¢ç‰©"],
            "ç ´åç”Ÿäº§ç»è¥": ["ç¬¬äºŒç™¾ä¸ƒåå…­æ¡ ç ´åç”Ÿäº§ç»è¥"],
            
            # ==================== å±å®³å…¬å…±å®‰å…¨ç½ªï¼ˆç¬¬äºŒç« ï¼‰====================
            "æ”¾ç«": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ æ”¾ç«ç½ª å±å®³å…¬å…±å®‰å…¨"],
            "æ”¾ç«ç½ª": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ æ”¾ç« å°šæœªé€ æˆä¸¥é‡åæœ", "ç¬¬ä¸€ç™¾ä¸€åäº”æ¡ æ”¾ç« è‡´äººé‡ä¼¤æ­»äº¡"],
            "å†³æ°´": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ å†³æ°´ç½ª å±å®³å…¬å…±å®‰å…¨"],
            "çˆ†ç‚¸": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ çˆ†ç‚¸ç½ª å±å®³å…¬å…±å®‰å…¨"],
            "çˆ†ç‚¸ç½ª": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ çˆ†ç‚¸ å±å®³å…¬å…±å®‰å…¨ åå¹´ä»¥ä¸Š"],
            "æŠ•æ”¾å±é™©ç‰©è´¨": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ æŠ•æ”¾å±é™©ç‰©è´¨ æŠ•æ¯’"],
            "æŠ•æ¯’": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ æŠ•æ”¾å±é™©ç‰©è´¨"],
            "ä»¥å±é™©æ–¹æ³•å±å®³å…¬å…±å®‰å…¨": ["ç¬¬ä¸€ç™¾ä¸€åå››æ¡ ä»¥å±é™©æ–¹æ³•å±å®³å…¬å…±å®‰å…¨"],
            "ç ´åäº¤é€šå·¥å…·": ["ç¬¬ä¸€ç™¾ä¸€åå…­æ¡ ç ´åäº¤é€šå·¥å…· ç«è½¦ æ±½è½¦ èˆ¹åª"],
            "ç ´åäº¤é€šè®¾æ–½": ["ç¬¬ä¸€ç™¾ä¸€åä¸ƒæ¡ ç ´åäº¤é€šè®¾æ–½"],
            "åŠ«æŒèˆªç©ºå™¨": ["ç¬¬ä¸€ç™¾äºŒåä¸€æ¡ åŠ«æŒèˆªç©ºå™¨ åå¹´ä»¥ä¸Š"],
            "åŠ«æŒ": ["ç¬¬ä¸€ç™¾äºŒåä¸€æ¡ åŠ«æŒèˆªç©ºå™¨ åŠ«æŒèˆ¹åª"],
            "äº¤é€šè‚‡äº‹": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ äº¤é€šè‚‡äº‹ é€ƒé€¸ é‡å¤§äº‹æ•… å±å®³å…¬å…±å®‰å…¨"],
            "äº¤é€šè‚‡äº‹ç½ª": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ äº¤é€šè‚‡äº‹ è‡´äººæ­»äº¡ é€ƒé€¸ ä¸‰å¹´ä»¥ä¸‹"],
            "è‚‡äº‹é€ƒé€¸": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ äº¤é€šè‚‡äº‹åé€ƒé€¸ ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹"],
            "é†‰é©¾": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ é†‰é…’é©¾é©¶ æ‹˜å½¹"],
            "å±é™©é©¾é©¶": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ é†‰é…’ è¿½é€ç«é©¶"],
            "å±é™©é©¾é©¶ç½ª": ["ç¬¬ä¸€ç™¾ä¸‰åä¸‰æ¡ä¹‹ä¸€ å±é™©é©¾é©¶ æ ¡è½¦ å®¢è¿ ä¸¥é‡è¶…è½½"],
            "éæ³•æŒæœ‰æªæ”¯": ["ç¬¬ä¸€ç™¾äºŒåå…«æ¡ éæ³•æŒæœ‰æªæ”¯å¼¹è¯"],
            "æªæ”¯": ["ç¬¬ä¸€ç™¾äºŒåäº”æ¡ éæ³•åˆ¶é€ ä¹°å–æªæ”¯", "ç¬¬ä¸€ç™¾äºŒåå…«æ¡ éæ³•æŒæœ‰æªæ”¯"],
            "é‡å¤§è´£ä»»äº‹æ•…": ["ç¬¬ä¸€ç™¾ä¸‰åå››æ¡ é‡å¤§è´£ä»»äº‹æ•…ç½ª"],
            "é‡å¤§åŠ³åŠ¨å®‰å…¨äº‹æ•…": ["ç¬¬ä¸€ç™¾ä¸‰åäº”æ¡ é‡å¤§åŠ³åŠ¨å®‰å…¨äº‹æ•…ç½ª"],
            
            # ==================== èµ°ç§ç½ªï¼ˆç¬¬ä¸‰ç« ç¬¬äºŒèŠ‚ï¼‰====================
            "èµ°ç§": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§ç½ª æ­¦å™¨å¼¹è¯ æ ¸ææ–™ å‡å¸", "ç¬¬ä¸€ç™¾äº”åä¸‰æ¡ èµ°ç§æ™®é€šè´§ç‰©"],
            "èµ°ç§ç½ª": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§æ­¦å™¨å¼¹è¯ èµ°ç§æ ¸ææ–™ èµ°ç§å‡å¸ æ— æœŸå¾’åˆ‘ æ­»åˆ‘"],
            "èµ°ç§æ­¦å™¨": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§æ­¦å™¨å¼¹è¯ ä¸ƒå¹´ä»¥ä¸Š æ— æœŸå¾’åˆ‘ æ­»åˆ‘"],
            "èµ°ç§æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ èµ°ç§æ¯’å“ è´©å–æ¯’å“ è¿è¾“æ¯’å“ åˆ¶é€ æ¯’å“", "é¸¦ç‰‡ æµ·æ´›å›  ç”²åŸºè‹¯ä¸™èƒº"],
            "èµ°ç§å‡å¸": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§å‡å¸ ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹"],
            "èµ°ç§æ–‡ç‰©": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§æ–‡ç‰© å›½å®¶ç¦æ­¢å‡ºå£"],
            "èµ°ç§è´µé‡é‡‘å±": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§è´µé‡é‡‘å± é»„é‡‘ç™½é“¶"],
            "èµ°ç§çè´µåŠ¨ç‰©": ["ç¬¬ä¸€ç™¾äº”åä¸€æ¡ èµ°ç§çè´µåŠ¨ç‰© çè´µåŠ¨ç‰©åˆ¶å“"],
            "èµ°ç§æ™®é€šè´§ç‰©": ["ç¬¬ä¸€ç™¾äº”åä¸‰æ¡ èµ°ç§æ™®é€šè´§ç‰©ç‰©å“ å·é€ƒå…³ç¨"],
            "èµ°ç§åºŸç‰©": ["ç¬¬ä¸€ç™¾äº”åäºŒæ¡ èµ°ç§åºŸç‰© å›ºä½“åºŸç‰©"],
            
            # ==================== æ¯’å“çŠ¯ç½ªï¼ˆç¬¬å…­ç« ç¬¬ä¸ƒèŠ‚ï¼‰====================
            "æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ èµ°ç§è´©å–è¿è¾“åˆ¶é€ æ¯’å“", "ç¬¬ä¸‰ç™¾å››åå…«æ¡ éæ³•æŒæœ‰æ¯’å“"],
            "è´©æ¯’": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ è´©å–æ¯’å“ èµ°ç§è¿è¾“åˆ¶é€ "],
            "è´©å–æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ è´©å–æ¯’å“ é¸¦ç‰‡ æµ·æ´›å›  ç”²åŸºè‹¯ä¸™èƒº å†°æ¯’"],
            "åˆ¶é€ æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ åˆ¶é€ æ¯’å“ æ— æœŸå¾’åˆ‘ æ­»åˆ‘"],
            "è¿è¾“æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åä¸ƒæ¡ è¿è¾“æ¯’å“"],
            "éæ³•æŒæœ‰æ¯’å“": ["ç¬¬ä¸‰ç™¾å››åå…«æ¡ éæ³•æŒæœ‰æ¯’å“ é¸¦ç‰‡ä¸€åƒå…‹ä»¥ä¸Š"],
            "å®¹ç•™ä»–äººå¸æ¯’": ["ç¬¬ä¸‰ç™¾äº”åå››æ¡ å®¹ç•™ä»–äººå¸æ¯’"],
            "å¼•è¯±å¸æ¯’": ["ç¬¬ä¸‰ç™¾äº”åä¸‰æ¡ å¼•è¯±æ•™å”†æ¬ºéª—ä»–äººå¸æ¯’"],
            
            # ==================== è´ªæ±¡è´¿èµ‚ç½ªï¼ˆç¬¬å…«ç« ï¼‰====================
            "è´ªæ±¡": ["ç¬¬ä¸‰ç™¾å…«åäºŒæ¡ è´ªæ±¡ç½ª å›½å®¶å·¥ä½œäººå‘˜ ä¾µå"],
            "è´ªæ±¡ç½ª": ["ç¬¬ä¸‰ç™¾å…«åäºŒæ¡ è´ªæ±¡ åˆ©ç”¨èŒåŠ¡ä¾¿åˆ© ä¾µåæŒªç”¨éª—å–å…¬å…±è´¢ç‰©"],
            "å—è´¿": ["ç¬¬ä¸‰ç™¾å…«åäº”æ¡ å—è´¿ç½ª å›½å®¶å·¥ä½œäººå‘˜ è°‹å–åˆ©ç›Š"],
            "å—è´¿ç½ª": ["ç¬¬ä¸‰ç™¾å…«åäº”æ¡ å—è´¿ æ”¶å—è´¿èµ‚ ç´¢å–è´¿èµ‚ ä¸ºä»–äººè°‹å–åˆ©ç›Š"],
            "è¡Œè´¿": ["ç¬¬ä¸‰ç™¾å…«åä¹æ¡ è¡Œè´¿ç½ª ç»™äºˆè´¢ç‰©"],
            "è¡Œè´¿ç½ª": ["ç¬¬ä¸‰ç™¾å…«åä¹æ¡ è¡Œè´¿ ä¸ºè°‹å–ä¸æ­£å½“åˆ©ç›Š ç»™äºˆå›½å®¶å·¥ä½œäººå‘˜è´¢ç‰©"],
            "æŒªç”¨å…¬æ¬¾": ["ç¬¬ä¸‰ç™¾å…«åå››æ¡ æŒªç”¨å…¬æ¬¾ å½’ä¸ªäººä½¿ç”¨"],
            "æŒªç”¨å…¬æ¬¾ç½ª": ["ç¬¬ä¸‰ç™¾å…«åå››æ¡ æŒªç”¨å…¬æ¬¾ è¿›è¡Œéæ³•æ´»åŠ¨ è¥åˆ©æ´»åŠ¨"],
            "å·¨é¢è´¢äº§æ¥æºä¸æ˜": ["ç¬¬ä¸‰ç™¾ä¹åäº”æ¡ å·¨é¢è´¢äº§æ¥æºä¸æ˜ç½ª"],
            "ç§åˆ†å›½æœ‰èµ„äº§": ["ç¬¬ä¸‰ç™¾ä¹åå…­æ¡ ç§åˆ†å›½æœ‰èµ„äº§ç½ª"],
            "å•ä½å—è´¿": ["ç¬¬ä¸‰ç™¾å…«åä¸ƒæ¡ å•ä½å—è´¿ç½ª"],
            "å•ä½è¡Œè´¿": ["ç¬¬ä¸‰ç™¾ä¹åä¸‰æ¡ å•ä½è¡Œè´¿ç½ª"],
            
            # ==================== å¦¨å®³ç¤¾ä¼šç®¡ç†ç§©åºç½ªï¼ˆç¬¬å…­ç« ï¼‰====================
            "å¦¨å®³å…¬åŠ¡": ["ç¬¬äºŒç™¾ä¸ƒåä¸ƒæ¡ å¦¨å®³å…¬åŠ¡ æš´åŠ› å¨èƒ"],
            "å¦¨å®³å…¬åŠ¡ç½ª": ["ç¬¬äºŒç™¾ä¸ƒåä¸ƒæ¡ å¦¨å®³å…¬åŠ¡ é˜»ç¢å›½å®¶æœºå…³å·¥ä½œäººå‘˜"],
            "æ‹›æ‘‡æ’éª—": ["ç¬¬äºŒç™¾ä¸ƒåä¹æ¡ æ‹›æ‘‡æ’éª— å†’å……å›½å®¶æœºå…³å·¥ä½œäººå‘˜"],
            "ä¼ªé€ å…¬æ–‡": ["ç¬¬äºŒç™¾å…«åæ¡ ä¼ªé€ å…¬æ–‡å°ç« è¯ä»¶"],
            "ä¼ªé€ å°ç« ": ["ç¬¬äºŒç™¾å…«åæ¡ ä¼ªé€ å…¬å¸å°ç« "],
            "èšä¼—æ–—æ®´": ["ç¬¬äºŒç™¾ä¹åäºŒæ¡ èšä¼—æ–—æ®´ é¦–è¦åˆ†å­ ç§¯æå‚åŠ "],
            "èšä¼—æ–—æ®´ç½ª": ["ç¬¬äºŒç™¾ä¹åäºŒæ¡ èšä¼—æ–—æ®´ å¤šæ¬¡èšä¼—æ–—æ®´ æŒæ¢°èšä¼—æ–—æ®´"],
            "å¯»è¡…æ»‹äº‹": ["ç¬¬äºŒç™¾ä¹åä¸‰æ¡ å¯»è¡…æ»‹äº‹ éšæ„æ®´æ‰“ è¿½é€æ‹¦æˆª"],
            "å¯»è¡…æ»‹äº‹ç½ª": ["ç¬¬äºŒç™¾ä¹åä¸‰æ¡ å¯»è¡…æ»‹äº‹ å¼ºæ‹¿ç¡¬è¦ ä»»æ„æŸæ¯ èµ·å“„é—¹äº‹"],
            "ç»„ç»‡å–æ·«": ["ç¬¬ä¸‰ç™¾äº”åå…«æ¡ ç»„ç»‡å–æ·«ç½ª äº”å¹´ä»¥ä¸Š"],
            "å¼ºè¿«å–æ·«": ["ç¬¬ä¸‰ç™¾äº”åå…«æ¡ å¼ºè¿«å–æ·«ç½ª"],
            "å–æ·«å«–å¨¼": ["ç¬¬ä¸‰ç™¾äº”åä¹æ¡ å¼•è¯±å®¹ç•™ä»‹ç»å–æ·«"],
            "èµŒåš": ["ç¬¬ä¸‰ç™¾é›¶ä¸‰æ¡ èµŒåšç½ª å¼€è®¾èµŒåœº"],
            "èµŒåšç½ª": ["ç¬¬ä¸‰ç™¾é›¶ä¸‰æ¡ èµŒåš ä»¥è¥åˆ©ä¸ºç›®çš„ èšä¼—èµŒåš"],
            "å¼€è®¾èµŒåœº": ["ç¬¬ä¸‰ç™¾é›¶ä¸‰æ¡ å¼€è®¾èµŒåœºç½ª"],
            "ä¼ªè¯": ["ç¬¬ä¸‰ç™¾é›¶äº”æ¡ ä¼ªè¯ç½ª è™šå‡è¯æ˜ è¯äºº"],
            "ä¼ªè¯ç½ª": ["ç¬¬ä¸‰ç™¾é›¶äº”æ¡ ä¼ªè¯ è™šå‡è¯æ˜ éšåŒ¿ç½ªè¯"],
            "åŒ…åº‡": ["ç¬¬ä¸‰ç™¾ä¸€åæ¡ åŒ…åº‡ç½ª çªè— éšç’"],
            "çªè—": ["ç¬¬ä¸‰ç™¾ä¸€åæ¡ çªè—åŒ…åº‡ç½ª æ˜çŸ¥æ˜¯çŠ¯ç½ªçš„äºº"],
            "æ©é¥°éšç’çŠ¯ç½ªæ‰€å¾—": ["ç¬¬ä¸‰ç™¾ä¸€åäºŒæ¡ æ©é¥°éšç’çŠ¯ç½ªæ‰€å¾— é”€èµƒ"],
            "é”€èµƒ": ["ç¬¬ä¸‰ç™¾ä¸€åäºŒæ¡ æ©é¥°éšç’çŠ¯ç½ªæ‰€å¾—"],
            "æ´—é’±": ["ç¬¬ä¸€ç™¾ä¹åä¸€æ¡ æ´—é’±ç½ª æ©é¥°éšç’"],
            "æ‹’ä¸æ‰§è¡Œåˆ¤å†³è£å®š": ["ç¬¬ä¸‰ç™¾ä¸€åä¸‰æ¡ æ‹’ä¸æ‰§è¡Œåˆ¤å†³è£å®šç½ª"],
            "è„±é€ƒ": ["ç¬¬ä¸‰ç™¾ä¸€åå…­æ¡ è„±é€ƒç½ª ä¾æ³•è¢«å…³æŠ¼çš„çŠ¯ç½ªåˆ†å­"],
            
            # ==================== ç ´åç¤¾ä¼šä¸»ä¹‰å¸‚åœºç»æµç§©åºç½ªï¼ˆç¬¬ä¸‰ç« ï¼‰====================
            "ç”Ÿäº§é”€å”®ä¼ªåŠ£äº§å“": ["ç¬¬ä¸€ç™¾å››åæ¡ ç”Ÿäº§é”€å”®ä¼ªåŠ£äº§å“ç½ª"],
            "å‡å†’æ³¨å†Œå•†æ ‡": ["ç¬¬äºŒç™¾ä¸€åä¸‰æ¡ å‡å†’æ³¨å†Œå•†æ ‡ç½ª"],
            "å‡å†’ä¸“åˆ©": ["ç¬¬äºŒç™¾ä¸€åå…­æ¡ å‡å†’ä¸“åˆ©ç½ª"],
            "ä¾µçŠ¯è‘—ä½œæƒ": ["ç¬¬äºŒç™¾ä¸€åä¸ƒæ¡ ä¾µçŠ¯è‘—ä½œæƒç½ª"],
            "ä¾µçŠ¯å•†ä¸šç§˜å¯†": ["ç¬¬äºŒç™¾ä¸€åä¹æ¡ ä¾µçŠ¯å•†ä¸šç§˜å¯†ç½ª"],
            "åˆåŒè¯ˆéª—": ["ç¬¬äºŒç™¾äºŒåå››æ¡ åˆåŒè¯ˆéª—ç½ª"],
            "é›†èµ„è¯ˆéª—": ["ç¬¬ä¸€ç™¾ä¹åäºŒæ¡ é›†èµ„è¯ˆéª—ç½ª"],
            "è´·æ¬¾è¯ˆéª—": ["ç¬¬ä¸€ç™¾ä¹åä¸‰æ¡ è´·æ¬¾è¯ˆéª—ç½ª"],
            "ç¥¨æ®è¯ˆéª—": ["ç¬¬ä¸€ç™¾ä¹åå››æ¡ ç¥¨æ®è¯ˆéª—ç½ª"],
            "ä¿¡ç”¨å¡è¯ˆéª—": ["ç¬¬ä¸€ç™¾ä¹åå…­æ¡ ä¿¡ç”¨å¡è¯ˆéª—ç½ª æ¶æ„é€æ”¯"],
            "ä¿é™©è¯ˆéª—": ["ç¬¬ä¸€ç™¾ä¹åå…«æ¡ ä¿é™©è¯ˆéª—ç½ª"],
            "é€ƒç¨": ["ç¬¬äºŒç™¾é›¶ä¸€æ¡ é€ƒç¨ç½ª å·ç¨"],
            "å·ç¨": ["ç¬¬äºŒç™¾é›¶ä¸€æ¡ é€ƒç¨ç½ª"],
            "è™šå¼€å¢å€¼ç¨å‘ç¥¨": ["ç¬¬äºŒç™¾é›¶äº”æ¡ è™šå¼€å¢å€¼ç¨ä¸“ç”¨å‘ç¥¨ç½ª"],
            "éæ³•å¸æ”¶å…¬ä¼—å­˜æ¬¾": ["ç¬¬ä¸€ç™¾ä¸ƒåå…­æ¡ éæ³•å¸æ”¶å…¬ä¼—å­˜æ¬¾ç½ª"],
            "éæ³•ç»è¥": ["ç¬¬äºŒç™¾äºŒåäº”æ¡ éæ³•ç»è¥ç½ª"],
            "ä¼ªé€ è´§å¸": ["ç¬¬ä¸€ç™¾ä¸ƒåæ¡ ä¼ªé€ è´§å¸ç½ª"],
            "æŒæœ‰ä½¿ç”¨å‡å¸": ["ç¬¬ä¸€ç™¾ä¸ƒåäºŒæ¡ æŒæœ‰ä½¿ç”¨å‡å¸ç½ª"],
            "é«˜åˆ©è½¬è´·": ["ç¬¬ä¸€ç™¾ä¸ƒåäº”æ¡ é«˜åˆ©è½¬è´·ç½ª"],
            "éª—å–è´·æ¬¾": ["ç¬¬ä¸€ç™¾ä¸ƒåäº”æ¡ä¹‹ä¸€ éª—å–è´·æ¬¾ç½ª"],
            "æ“çºµè¯åˆ¸å¸‚åœº": ["ç¬¬ä¸€ç™¾å…«åäºŒæ¡ æ“çºµè¯åˆ¸å¸‚åœºç½ª"],
            "å†…å¹•äº¤æ˜“": ["ç¬¬ä¸€ç™¾å…«åæ¡ å†…å¹•äº¤æ˜“ç½ª"],
            
            # ==================== æ¸èŒç½ªï¼ˆç¬¬ä¹ç« ï¼‰====================
            "æ»¥ç”¨èŒæƒ": ["ç¬¬ä¸‰ç™¾ä¹åä¸ƒæ¡ æ»¥ç”¨èŒæƒç½ª ç©å¿½èŒå®ˆç½ª"],
            "ç©å¿½èŒå®ˆ": ["ç¬¬ä¸‰ç™¾ä¹åä¸ƒæ¡ ç©å¿½èŒå®ˆç½ª è‡´ä½¿å…¬å…±è´¢äº§"],
            "å¾‡ç§æ‰æ³•": ["ç¬¬ä¸‰ç™¾ä¹åä¹æ¡ å¾‡ç§æ‰æ³•ç½ª æ‰æ³•è£åˆ¤"],
            "æ‰æ³•è£åˆ¤": ["ç¬¬ä¸‰ç™¾ä¹åä¹æ¡ æ‰æ³•è£åˆ¤ç½ª"],
            "å¸®åŠ©çŠ¯ç½ªåˆ†å­é€ƒé¿å¤„ç½š": ["ç¬¬å››ç™¾ä¸€åä¸ƒæ¡ å¸®åŠ©çŠ¯ç½ªåˆ†å­é€ƒé¿å¤„ç½šç½ª"],
            "ç§æ”¾åœ¨æŠ¼äººå‘˜": ["ç¬¬å››ç™¾æ¡ ç§æ”¾åœ¨æŠ¼äººå‘˜ç½ª"],
            
            # ==================== åˆ‘ç½šåˆ¶åº¦ï¼ˆæ€»åˆ™ï¼‰====================
            "æ­£å½“é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“ ä¸è´Ÿåˆ‘äº‹è´£ä»» ä¸æ³•ä¾µå®³"],
            "é˜²å«": ["ç¬¬äºŒåæ¡ æ­£å½“é˜²å« é˜²å«è¿‡å½“"],
            "é˜²å«è¿‡å½“": ["ç¬¬äºŒåæ¡ é˜²å«è¿‡å½“ åº”å½“å‡è½»æˆ–è€…å…é™¤å¤„ç½š"],
            "ç´§æ€¥é¿é™©": ["ç¬¬äºŒåä¸€æ¡ ç´§æ€¥é¿é™© é¿å…å±é™©"],
            "è‡ªé¦–": ["ç¬¬å…­åä¸ƒæ¡ è‡ªé¦– ä»è½»å¤„ç½š å‡è½»å¤„ç½š è‡ªåŠ¨æŠ•æ¡ˆ"],
            "ç«‹åŠŸ": ["ç¬¬å…­åå…«æ¡ ç«‹åŠŸ é‡å¤§ç«‹åŠŸ å‡è½»å¤„ç½š"],
            "ç´¯çŠ¯": ["ç¬¬å…­åäº”æ¡ ç´¯çŠ¯ ä»é‡å¤„ç½š äº”å¹´ä»¥å†…"],
            "ç¼“åˆ‘": ["ç¬¬ä¸ƒåäºŒæ¡ ç¼“åˆ‘ å®£å‘Šç¼“åˆ‘ ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘", "ç¬¬ä¸ƒåä¸‰æ¡ ç¼“åˆ‘è€ƒéªŒæœŸ"],
            "å‡åˆ‘": ["ç¬¬ä¸ƒåå…«æ¡ å‡åˆ‘ æ‚”æ”¹è¡¨ç° ç«‹åŠŸè¡¨ç°"],
            "å‡é‡Š": ["ç¬¬å…«åä¸€æ¡ å‡é‡Š æœåˆ‘æœŸé—´ ä¸è‡´å†å±å®³ç¤¾ä¼š"],
            "æœªæˆå¹´": ["ç¬¬åä¸ƒæ¡ æœªæˆå¹´äºº åˆ‘äº‹è´£ä»»å¹´é¾„ ä»è½»å‡è½»"],
            "åˆ‘äº‹è´£ä»»å¹´é¾„": ["ç¬¬åä¸ƒæ¡ åˆ‘äº‹è´£ä»»å¹´é¾„ åå››å‘¨å² åå…­å‘¨å² åäºŒå‘¨å²"],
            "ä»è½»": ["ç¬¬å…­åä¸ƒæ¡ ä»è½»å¤„ç½š", "ç¬¬åä¸ƒæ¡ ä»è½»å‡è½»"],
            "å‡è½»": ["ç¬¬å…­åä¸‰æ¡ å‡è½»å¤„ç½š æ³•å®šåˆ‘ä»¥ä¸‹"],
            "ä»é‡": ["ç¬¬å…­åäº”æ¡ ä»é‡å¤„ç½š"],
            "å…±åŒçŠ¯ç½ª": ["ç¬¬äºŒåäº”æ¡ å…±åŒçŠ¯ç½ª äºŒäººä»¥ä¸Šå…±åŒæ•…æ„"],
            "ä¸»çŠ¯": ["ç¬¬äºŒåå…­æ¡ ä¸»çŠ¯ ç»„ç»‡é¢†å¯¼ ä¸»è¦ä½œç”¨"],
            "ä»çŠ¯": ["ç¬¬äºŒåä¸ƒæ¡ ä»çŠ¯ æ¬¡è¦è¾…åŠ©ä½œç”¨"],
            "æ•™å”†çŠ¯": ["ç¬¬äºŒåä¹æ¡ æ•™å”†çŠ¯ æ•™å”†ä»–äººçŠ¯ç½ª"],
            "çŠ¯ç½ªé¢„å¤‡": ["ç¬¬äºŒåäºŒæ¡ çŠ¯ç½ªé¢„å¤‡ ä¸ºäº†çŠ¯ç½ªå‡†å¤‡å·¥å…·"],
            "çŠ¯ç½ªæœªé‚": ["ç¬¬äºŒåä¸‰æ¡ çŠ¯ç½ªæœªé‚ å·²ç»ç€æ‰‹å®è¡ŒçŠ¯ç½ª"],
            "çŠ¯ç½ªä¸­æ­¢": ["ç¬¬äºŒåå››æ¡ çŠ¯ç½ªä¸­æ­¢ è‡ªåŠ¨æ”¾å¼ƒçŠ¯ç½ª"],
            "è¿½è¯‰æ—¶æ•ˆ": ["ç¬¬å…«åä¸ƒæ¡ è¿½è¯‰æ—¶æ•ˆ ç»è¿‡ä¸€å®šæœŸé™ä¸å†è¿½è¯‰"],
            "æ­»åˆ‘": ["ç¬¬å››åå…«æ¡ æ­»åˆ‘ ç½ªè¡Œæå…¶ä¸¥é‡", "ç¬¬å››åä¹æ¡ æ­»åˆ‘é€‚ç”¨é™åˆ¶"],
            "æ— æœŸå¾’åˆ‘": ["ç¬¬å››åå…­æ¡ æ— æœŸå¾’åˆ‘"],
            "æœ‰æœŸå¾’åˆ‘": ["ç¬¬å››åäº”æ¡ æœ‰æœŸå¾’åˆ‘ å…­ä¸ªæœˆä»¥ä¸Šåäº”å¹´ä»¥ä¸‹"],
            "æ‹˜å½¹": ["ç¬¬å››åäºŒæ¡ æ‹˜å½¹ ä¸€ä¸ªæœˆä»¥ä¸Šå…­ä¸ªæœˆä»¥ä¸‹"],
            "ç®¡åˆ¶": ["ç¬¬ä¸‰åå…«æ¡ ç®¡åˆ¶ ä¸‰ä¸ªæœˆä»¥ä¸ŠäºŒå¹´ä»¥ä¸‹"],
            "ç½šé‡‘": ["ç¬¬äº”åäºŒæ¡ ç½šé‡‘ æ ¹æ®çŠ¯ç½ªæƒ…èŠ‚ç¡®å®š"],
            "æ²¡æ”¶è´¢äº§": ["ç¬¬äº”åä¹æ¡ æ²¡æ”¶è´¢äº§"],
            "å‰¥å¤ºæ”¿æ²»æƒåˆ©": ["ç¬¬äº”åå››æ¡ å‰¥å¤ºæ”¿æ²»æƒåˆ©"],
            "æ•°ç½ªå¹¶ç½š": ["ç¬¬å…­åä¹æ¡ æ•°ç½ªå¹¶ç½š åˆ¤å†³å®£å‘Šä»¥å‰"],
        }
        
        enhanced_queries = [query]  # åŸå§‹æŸ¥è¯¢å§‹ç»ˆä¿ç•™
        matched_keywords = []
        
        # åŒ¹é…æ‰€æœ‰ç›¸å…³å…³é”®è¯
        for keyword, expansions in crime_mappings.items():
            if keyword in query:
                enhanced_queries.extend(expansions)
                matched_keywords.append(keyword)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œå°è¯•é€šç”¨æ³•å¾‹æŸ¥è¯¢å¢å¼º
        if not matched_keywords:
            enhanced_queries.append(f"åˆ‘æ³• {query} å¤„ç½š")
            enhanced_queries.append(f"{query} æœ‰æœŸå¾’åˆ‘ ç½šé‡‘")
        
        return enhanced_queries
    
    def _parallel_retrieve_statutes(
        self, 
        enhanced_queries: List[str], 
        vectorstores: List,
        statute_k: int,
        seen_doc_ids: set
    ) -> List[Document]:
        """
        ã€çŸ­æœŸä¼˜åŒ–ã€‘å¹¶è¡Œæ£€ç´¢æ³•æ¡
        ä½¿ç”¨ThreadPoolExecutorå¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªå‘é‡åº“
        """
        statute_docs = []
        lock = threading.Lock()
        
        def retrieve_from_vs(vs, eq):
            """ä»å•ä¸ªå‘é‡åº“æ£€ç´¢"""
            try:
                results = vs.similarity_search_with_score(
                    eq,
                    k=statute_k * 2,
                    filter={"type": "statute"}
                )
                return results
            except Exception as e:
                print(f"[è­¦å‘Š] æ³•æ¡æ£€ç´¢å¤±è´¥: {e}")
                return []
        
        try:
            with ThreadPoolExecutor(max_workers=MAX_PARALLEL_WORKERS) as executor:
                futures = []
                for eq in enhanced_queries[:3]:  # é™åˆ¶å¢å¼ºæŸ¥è¯¢æ•°é‡
                    for vs in vectorstores:
                        if vs:
                            future = executor.submit(retrieve_from_vs, vs, eq)
                            futures.append(future)
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures, timeout=PARALLEL_RETRIEVAL_TIMEOUT):
                    try:
                        results = future.result()
                        for doc, score in results:
                            with lock:
                                doc_id = doc.metadata.get("doc_id", id(doc))
                                if doc_id not in seen_doc_ids:
                                    seen_doc_ids.add(doc_id)
                                    doc.metadata["relevance_score"] = score
                                    statute_docs.append(doc)
                    except Exception as e:
                        print(f"[è­¦å‘Š] å¹¶è¡Œæ£€ç´¢å¼‚å¸¸: {e}")
        except Exception as e:
            print(f"[è­¦å‘Š] å¹¶è¡Œæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return statute_docs
    
    def _parallel_retrieve_cases(
        self,
        query: str,
        vectorstores: List,
        case_k: int,
        seen_doc_ids: set
    ) -> List[Document]:
        """
        ã€çŸ­æœŸä¼˜åŒ–ã€‘å¹¶è¡Œæ£€ç´¢æ¡ˆä¾‹
        ä½¿ç”¨ThreadPoolExecutorå¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªå‘é‡åº“
        """
        case_docs = []
        lock = threading.Lock()
        
        def retrieve_from_vs(vs):
            """ä»å•ä¸ªå‘é‡åº“æ£€ç´¢"""
            try:
                results = vs.similarity_search_with_score(
                    query,
                    k=case_k * 2,
                    filter={"type": "case"}
                )
                return results
            except Exception as e:
                print(f"[è­¦å‘Š] æ¡ˆä¾‹æ£€ç´¢å¤±è´¥: {e}")
                return []
        
        try:
            with ThreadPoolExecutor(max_workers=MAX_PARALLEL_WORKERS) as executor:
                futures = [executor.submit(retrieve_from_vs, vs) for vs in vectorstores if vs]
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures, timeout=PARALLEL_RETRIEVAL_TIMEOUT):
                    try:
                        results = future.result()
                        for doc, score in results:
                            with lock:
                                doc_id = doc.metadata.get("doc_id", id(doc))
                                if doc_id not in seen_doc_ids:
                                    seen_doc_ids.add(doc_id)
                                    doc.metadata["relevance_score"] = score
                                    case_docs.append(doc)
                    except Exception as e:
                        print(f"[è­¦å‘Š] å¹¶è¡Œæ£€ç´¢å¼‚å¸¸: {e}")
        except Exception as e:
            print(f"[è­¦å‘Š] å¹¶è¡Œæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return case_docs
    
    def _hybrid_retrieve(self, query: str, k: int = RETRIEVAL_TOP_K) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼šåˆ†åˆ«æ£€ç´¢æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œç„¶ååˆå¹¶
        ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼º + å…³é”®è¯è¿‡æ»¤æé«˜æ³•æ¡æ£€ç´¢ç²¾åº¦
        æ”¯æŒå¤šé¢†åŸŸå’Œå•é¢†åŸŸæ¨¡å¼
        ã€çŸ­æœŸä¼˜åŒ–ã€‘æ”¯æŒå¹¶è¡Œæ£€ç´¢å¤šä¸ªå‘é‡åº“
        
        ChromaDBçš„åˆ†æ•°è¶Šä½è¡¨ç¤ºè¶Šç›¸å…³ï¼ˆL2è·ç¦»ï¼‰
        """
        statute_docs = []
        case_docs = []
        seen_doc_ids = set()
        
        statute_k = max(4, k // 2 + 1)  # æ³•æ¡æ•°é‡
        case_k = k - statute_k + 2  # æ¡ˆä¾‹æ•°é‡
        
        # è·å–è¦ä½¿ç”¨çš„å‘é‡åº“åˆ—è¡¨
        vectorstores = []
        if getattr(self, 'multi_domain_mode', False) and hasattr(self, 'vectorstores_multi'):
            # å¤šé¢†åŸŸæ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰é¢†åŸŸçš„å‘é‡åº“
            vectorstores = [item['vectorstore'] for item in self.vectorstores_multi.values() if 'vectorstore' in item]
        elif getattr(self, 'vectorstore', None):
            # å•é¢†åŸŸæ¨¡å¼ï¼šä½¿ç”¨å•ä¸€å‘é‡åº“
            vectorstores = [self.vectorstore]
        else:
            print("[é”™è¯¯] æœªæ‰¾åˆ°å¯ç”¨çš„å‘é‡åº“")
            return []
        
        # 1. æ³•æ¡æ£€ç´¢ï¼šä½¿ç”¨å¤šä¸ªå¢å¼ºæŸ¥è¯¢ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
        enhanced_queries = self._extract_crime_keywords(query)
        
        if ENABLE_PARALLEL_RETRIEVAL and len(vectorstores) > 1:
            # å¹¶è¡Œæ£€ç´¢å¤šä¸ªå‘é‡åº“
            statute_docs = self._parallel_retrieve_statutes(
                enhanced_queries, vectorstores, statute_k, seen_doc_ids
            )
        else:
            # ä¸²è¡Œæ£€ç´¢
            for eq in enhanced_queries:
                for vs in vectorstores:
                    if not vs: continue
                    try:
                        results = vs.similarity_search_with_score(
                            eq,
                            k=statute_k * 2,
                            filter={"type": "statute"}
                        )
                        
                        for doc, score in results:
                            doc_id = doc.metadata.get("doc_id", id(doc))
                            if doc_id not in seen_doc_ids:
                                seen_doc_ids.add(doc_id)
                                doc.metadata["relevance_score"] = score
                                statute_docs.append(doc)
                                
                    except Exception as e:
                        print(f"[è­¦å‘Š] æ³•æ¡æ£€ç´¢å¤±è´¥: {e}")
        
        # 2. æ¡ˆä¾‹æ£€ç´¢ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
        if ENABLE_PARALLEL_RETRIEVAL and len(vectorstores) > 1:
            # å¹¶è¡Œæ£€ç´¢å¤šä¸ªå‘é‡åº“
            case_docs = self._parallel_retrieve_cases(
                query, vectorstores, case_k, seen_doc_ids
            )
        else:
            # ä¸²è¡Œæ£€ç´¢
            for vs in vectorstores:
                if not vs: continue
                try:
                    case_results = vs.similarity_search_with_score(
                        query, 
                        k=case_k * 2,
                        filter={"type": "case"}
                    )
                    
                    for doc, score in case_results:
                        doc_id = doc.metadata.get("doc_id", id(doc))
                        if doc_id not in seen_doc_ids:
                            seen_doc_ids.add(doc_id)
                            doc.metadata["relevance_score"] = score
                            case_docs.append(doc)
                            
                except Exception as e:
                    print(f"[è­¦å‘Š] æ¡ˆä¾‹æ£€ç´¢å¤±è´¥: {e}")
        
        # 3. æ–¹æ¡ˆCå¢å¼ºï¼šå…³é”®è¯é‡æ’åº + è¯­ä¹‰ç›¸å…³æ€§èåˆ
        def get_keyword_score(doc):
            """è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ† - å¢å¼ºç‰ˆ"""
            base_score = doc.metadata.get("relevance_score", 999)
            content = doc.page_content.lower()
            query_lower = query.lower()
            
            # æ‰©å±•çš„å…³é”®è¯åˆ—è¡¨ï¼ˆè¦†ç›–æ›´å¤šç½ªåï¼‰
            crime_keywords = [
                # ä¾µçŠ¯äººèº«æƒåˆ©
                "æ•…æ„æ€äºº", "æ•…æ„ä¼¤å®³", "å¼ºå¥¸", "ç»‘æ¶", "æ‹å–", "éæ³•æ‹˜ç¦",
                # ä¾µçŠ¯è´¢äº§
                "ç›—çªƒ", "æŠ¢åŠ«", "è¯ˆéª—", "æŠ¢å¤º", "æ•²è¯ˆå‹’ç´¢", "ä¾µå ", "æŒªç”¨",
                # å±å®³å…¬å…±å®‰å…¨
                "äº¤é€šè‚‡äº‹", "å±é™©é©¾é©¶", "é†‰é©¾", "æ”¾ç«", "çˆ†ç‚¸",
                # å¦¨å®³ç¤¾ä¼šç®¡ç†
                "èšä¼—æ–—æ®´", "å¯»è¡…æ»‹äº‹", "èµŒåš", "ä¼ªè¯", "åŒ…åº‡", "å¦¨å®³å…¬åŠ¡",
                # è´ªæ±¡è´¿èµ‚
                "è´ªæ±¡", "å—è´¿", "è¡Œè´¿", "æŒªç”¨å…¬æ¬¾",
                # æ¯’å“çŠ¯ç½ª
                "æ¯’å“", "è´©æ¯’", "èµ°ç§",
                # åˆ‘ç½šåˆ¶åº¦
                "æ­£å½“é˜²å«", "ç´§æ€¥é¿é™©", "è‡ªé¦–", "ç«‹åŠŸ", "ç´¯çŠ¯",
                "ç¼“åˆ‘", "å‡åˆ‘", "å‡é‡Š", "æœªæˆå¹´", "å…±åŒçŠ¯ç½ª",
                "ä»è½»", "å‡è½»", "ä»é‡", "ä¸»çŠ¯", "ä»çŠ¯"
            ]
            
            bonus = 0
            matched_count = 0
            
            # ç»Ÿè®¡åŒ¹é…çš„å…³é”®è¯æ•°é‡
            for kw in crime_keywords:
                if kw in query_lower and kw in content:
                    matched_count += 1
                    bonus -= 0.5  # æ¯ä¸ªåŒ¹é…å…³é”®è¯é™ä½0.5åˆ†
            
            # é¢å¤–å¥–åŠ±ï¼šå¤šä¸ªå…³é”®è¯åŒ¹é…
            if matched_count >= 2:
                bonus -= 0.5  # é¢å¤–å¥–åŠ±
            
            # ç²¾ç¡®æ¡æ¬¾åŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            article_nums = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒé›¶\d]+æ¡', query)
            for num in article_nums:
                if num in content:
                    bonus -= 2.0  # ç²¾ç¡®æ¡æ¬¾åŒ¹é…
            
            # æŸ¥è¯¢è¯ç›´æ¥å‡ºç°åœ¨å†…å®¹ä¸­
            query_words = [w for w in query_lower.split() if len(w) >= 2]
            for word in query_words:
                if word in content:
                    bonus -= 0.3
            
            return base_score + bonus
        
        # å¯¹æ³•æ¡è¿›è¡Œé‡æ’åº
        statute_docs.sort(key=get_keyword_score)
        case_docs.sort(key=lambda d: d.metadata.get("relevance_score", 999))
        
        # 4. åˆå¹¶ç»“æœï¼šæ³•æ¡ä¼˜å…ˆ
        final_docs = []
        final_docs.extend(statute_docs[:statute_k])
        final_docs.extend(case_docs[:case_k])
        
        # 5. å›é€€æ£€ç´¢
        if len(final_docs) == 0:
            for vs in vectorstores:
                if not vs: continue
                try:
                    results = vs.similarity_search_with_score(query, k=k)
                    for doc, score in results:
                        doc.metadata["relevance_score"] = score
                        final_docs.append(doc)
                except Exception as e:
                    print(f"[è­¦å‘Š] å›é€€æ£€ç´¢å¤±è´¥: {e}")
        
        return final_docs[:k]
    
    def _init_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS,
            openai_api_base=SILICONFLOW_BASE_URL,
            openai_api_key=SILICONFLOW_API_KEY,
            streaming=self.streaming
        )
    
    def _init_chains(self):
        """åˆå§‹åŒ–RAGé“¾ - ç®€åŒ–ç‰ˆï¼ˆä¸ä¾èµ–ä¼ ç»Ÿchainsï¼‰"""
        # ç”±äºä½¿ç”¨äº†æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä¸å†éœ€è¦ä¼ ç»Ÿçš„chains
        # ç›´æ¥åœ¨queryæ–¹æ³•ä¸­å¤„ç†æ£€ç´¢å’Œç”Ÿæˆ
        self.history_aware_retriever = None
        self.rag_chain = None
    
    def _format_chat_history(self) -> List:
        """æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºLangChainæ¶ˆæ¯æ ¼å¼"""
        messages = []
        for human, ai in self.chat_history[-MAX_HISTORY_TURNS:]:
            messages.append(HumanMessage(content=human))
            messages.append(AIMessage(content=ai))
        return messages
    
    def _format_history_text(self) -> str:
        """æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºæç¤ºè¯"""
        if not self.chat_history:
            return "æ— å†å²å¯¹è¯"
        
        history_text = []
        for human, ai in self.chat_history[-MAX_HISTORY_TURNS:]:
            history_text.append(f"ç”¨æˆ·: {human[:200]}...")
            history_text.append(f"åŠ©æ‰‹: {ai[:300]}...")
        return "\n".join(history_text[-6:])  # æœ€è¿‘3è½®å¯¹è¯
    
    def _extract_citations(self, docs: List[Document]) -> List[Citation]:
        """ä»æ£€ç´¢æ–‡æ¡£ä¸­æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, doc in enumerate(docs):
            # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–ç›¸ä¼¼åº¦åˆ†æ•°
            score = (
                doc.metadata.get("relevance_score") or
                doc.metadata.get("score") or
                doc.metadata.get("_score") or
                0.7  # é»˜è®¤ç»™äºˆä¸­ç­‰ç›¸å…³æ€§
            )
            
            # æ”¹è¿›æ¥æºæ ‡æ³¨ - åŒ…å«æ›´å¤šå…ƒæ•°æ®ä¿¡æ¯
            source_parts = [doc.metadata.get("source", "æœªçŸ¥æ¥æº")]
            
            # æ·»åŠ ç±»å‹ä¿¡æ¯
            doc_type = doc.metadata.get("type", "unknown")
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                if article:
                    source_parts.append(f"({article})")
            elif doc_type == "case":
                accusation = doc.metadata.get("accusation", "")
                case_id = doc.metadata.get("case_id", "")
                if accusation:
                    source_parts.append(f"ã€{accusation}ã€‘")
                if case_id:
                    source_parts.append(f"(æ¡ˆå·:{case_id})")
            
            source_display = "".join(source_parts)
            
            citation = Citation(
                source=source_display,
                doc_type=doc_type,
                content=doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                relevance_score=float(score),
                metadata=doc.metadata
            )
            citations.append(citation)
        return citations

    def _attach_similarity_scores(self, query: str, docs: List[Document]) -> List[Document]:
        """ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£è¡¥å……ç›¸ä¼¼åº¦åˆ†æ•°ã€‚"""
        if not docs:
            return docs
        try:
            # é¢„å–æ›´å¤šå€™é€‰ï¼Œä»¥ä¾¿è¦†ç›– context ä¸­çš„æ–‡æ¡£
            k = max(len(docs), RETRIEVAL_TOP_K * 2)
            score_map = {}
            
            # è·å–è¦ä½¿ç”¨çš„å‘é‡åº“åˆ—è¡¨
            if self.multi_domain_mode:
                vectorstores = [vs['vectorstore'] for vs in self.vectorstores_multi.values()]
            else:
                vectorstores = [self.vectorstore]
            
            # ä»æ‰€æœ‰å‘é‡åº“ä¸­è·å–åˆ†æ•°
            for vs in vectorstores:
                try:
                    scored = vs.similarity_search_with_score(query, k=k)
                    for d, score in scored:
                        doc_id = d.metadata.get("doc_id")
                        if doc_id and doc_id not in score_map:
                            score_map[doc_id] = score
                except Exception:
                    continue
            
            for doc in docs:
                doc_id = doc.metadata.get("doc_id")
                if doc_id and doc_id in score_map:
                    doc.metadata["relevance_score"] = score_map[doc_id]
            return docs
        except Exception:
            return docs
    
    def _calculate_confidence(self, docs: List[Document]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0
        
        # ChromaDBçš„åˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼Œéœ€è¦è½¬æ¢
        scores = []
        has_statute = False
        
        for doc in docs:
            raw_score = doc.metadata.get("relevance_score", 1.0)
            # è½¬æ¢ä¸º0-1çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆåˆ†æ•°è¶Šä½è¶Šç›¸å…³ï¼‰
            relevance = max(0, 1 - raw_score / 2)
            scores.append(relevance)
            
            if doc.metadata.get("type") == "statute":
                has_statute = True
        
        max_score = max(scores) if scores else 0.0
        avg_score = sum(scores) / len(scores) if scores else 0.0
        
        # å¦‚æœæœ‰æ³•æ¡æ–‡æ¡£ï¼Œç½®ä¿¡åº¦æå‡
        statute_bonus = 0.1 if has_statute else 0
        
        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.4 * max_score + 0.4 * avg_score + 0.2 * min(len(docs) / RETRIEVAL_TOP_K, 1.0) + statute_bonus
        
        return round(min(confidence, 0.95), 2)
    
    def query(self, question: str) -> RAGResponse:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ + è¶…èŒƒå›´æ£€æµ‹ + Rerankerä¼˜åŒ–ï¼‰
        
        ä¼˜åŒ–æ–¹æ¡ˆï¼š
        - æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ - å…³é”®è¯ + LLMäºŒæ¬¡éªŒè¯
        - æ–¹æ¡ˆB: Rerankeré‡æ’åº - æå‡ç›¸å…³æ–‡æ¡£æ’å
        - æ–¹æ¡ˆC: æ··åˆæ£€ç´¢ + é‡æ’åº
        - æ–¹æ¡ˆD: ä¼˜åŒ–æç¤ºè¯ + äº‹å®æ ¸æŸ¥
        - æ–¹æ¡ˆE: å¹»è§‰æ£€æµ‹ - éªŒè¯ç­”æ¡ˆä¸æ£€ç´¢å†…å®¹ä¸€è‡´æ€§
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            RAGResponse: åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯
        """
        # ==================== æ–¹æ¡ˆA: è¶…èŒƒå›´æ£€æµ‹ï¼ˆåŒé‡éªŒè¯ï¼‰====================
        # ç¬¬ä¸€å±‚ï¼šå…³é”®è¯æ£€æµ‹
        is_out_of_scope, detected_domain = self._detect_out_of_scope(question)
        
        # ç¬¬äºŒå±‚ï¼šLLMåˆ¤åˆ«å™¨éªŒè¯ï¼ˆä»…åœ¨å…³é”®è¯æœªæ£€æµ‹åˆ°è¶…èŒƒå›´æ—¶å¯ç”¨ï¼‰
        if not is_out_of_scope and RERANKER_AVAILABLE and ENABLE_RERANKER:
            try:
                scope_result = check_scope(question)
                if not scope_result.is_in_scope and scope_result.confidence > 0.7:
                    is_out_of_scope = True
                    detected_domain = scope_result.detected_domain
            except Exception as e:
                print(f"[è­¦å‘Š] LLMèŒƒå›´æ£€æµ‹å¤±è´¥: {e}")
        
        if is_out_of_scope:
            # ç”Ÿæˆè¶…èŒƒå›´æ‹’ç»å“åº”
            out_of_scope_answer = OUT_OF_SCOPE_RESPONSE.format(detected_domain=detected_domain)
            self.chat_history.append((question, out_of_scope_answer))
            return RAGResponse(
                answer=out_of_scope_answer,
                citations=[],
                confidence=0.1,  # ä½ç½®ä¿¡åº¦è¡¨ç¤ºè¿™æ˜¯æ‹’ç»å›ç­”
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ ¼å¼åŒ–å†å²å¯¹è¯
        chat_history = self._format_chat_history()
        
        # ==================== æ–¹æ¡ˆC: æ··åˆæ£€ç´¢ + Rerankeré‡æ’åº ====================
        # ä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥è·å–æ–‡æ¡£
        docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K * 2)  # æ£€ç´¢æ›´å¤šï¼Œä¾›é‡æ’åºç­›é€‰
        
        # ä½¿ç”¨Rerankerè¿›è¡Œé‡æ’åºå’Œç›¸å…³æ€§è¿‡æ»¤
        # ä¼˜åŒ–ï¼šä»…åœ¨éè¶…èŒƒå›´é—®é¢˜ä¸”æ–‡æ¡£å……è¶³æ—¶å¯ç”¨Rerankerï¼Œé¿å…ä¸å¿…è¦è°ƒç”¨
        if RERANKER_AVAILABLE and ENABLE_RERANKER and docs and len(docs) > 3:
            try:
                reranked_docs = rerank_documents(question, docs[:5], top_k=RERANKER_TOP_K)  # ä»…å¤„ç†top5
                # åªä¿ç•™ç›¸å…³æ–‡æ¡£
                docs = [rd.document for rd in reranked_docs if rd.is_relevant]
                # å¦‚æœé‡æ’åºåæ–‡æ¡£æ•°è¿‡å°‘ï¼Œå›é€€åˆ°åŸå§‹æ£€ç´¢ç»“æœ
                if not docs or len(docs) < 3:
                    docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K)
                # æ›´æ–°ç›¸å…³æ€§åˆ†æ•°
                for i, rd in enumerate(reranked_docs):
                    if rd.is_relevant and i < len(docs):
                        docs[i].metadata["reranker_score"] = rd.relevance_score
            except Exception as e:
                print(f"[è­¦å‘Š] Rerankeré‡æ’åºå¤±è´¥: {e}")
                docs = docs[:RETRIEVAL_TOP_K]
        else:
            docs = docs[:RETRIEVAL_TOP_K]
        
        # æ£€æµ‹æ£€ç´¢ç»“æœç›¸å…³æ€§æ˜¯å¦è¿‡ä½ï¼ˆç¬¬äºŒé“é˜²çº¿ï¼‰
        if self._is_low_relevance(docs):
            low_relevance_answer = """æŠ±æ­‰ï¼Œåœ¨ç°æœ‰çš„åˆ‘æ³•æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜é«˜åº¦ç›¸å…³çš„æ³•æ¡ã€‚

**å¯èƒ½çš„åŸå› **ï¼š
1. é—®é¢˜æ¶‰åŠçš„å…·ä½“æ³•å¾‹è§„å®šä¸åœ¨å½“å‰çŸ¥è¯†åº“è¦†ç›–èŒƒå›´å†…
2. é—®é¢˜è¡¨è¿°å¯èƒ½éœ€è¦æ›´å…·ä½“çš„æ³•å¾‹æœ¯è¯­
3. è¯¥é—®é¢˜å¯èƒ½æ¶‰åŠå…¶ä»–æ³•å¾‹é¢†åŸŸ

**å»ºè®®**ï¼š
- è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„æ³•å¾‹æœ¯è¯­æè¿°é—®é¢˜
- å¦‚æœæ¶‰åŠå…·ä½“æ¡ˆä»¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåˆ‘äº‹å¾‹å¸ˆ
- å¦‚æœ‰å…¶ä»–åˆ‘æ³•ç›¸å…³é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­å’¨è¯¢"""
            self.chat_history.append((question, low_relevance_answer))
            return RAGResponse(
                answer=low_relevance_answer,
                citations=[],
                confidence=0.2,
                is_uncertain=True,
                retrieved_docs=docs
            )
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆæ–‡æ¡£
        if not docs:
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(docs, 1):
            doc_type = doc.metadata.get("type", "unknown")
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ³•æ¡ã€‘{source} {article}\n{doc.page_content}")
            else:
                accusation = doc.metadata.get("accusation", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ¡ˆä¾‹ã€‘{source}ï¼ˆ{accusation}ï¼‰\n{doc.page_content}")
        
        context_text = "\n\n".join(context_parts)
        
        # ç›´æ¥è°ƒç”¨LLMç”Ÿæˆå›ç­”
        from langchain_core.messages import HumanMessage, SystemMessage
        
        # ==================== æ–¹æ¡ˆD: ä¼˜åŒ–æç¤ºè¯ - å¢å¼ºäº‹å®æ ¸æŸ¥æŒ‡ä»¤ï¼ˆç²¾ç‚¼ç‰ˆä»¥å‡å°‘ç”Ÿæˆtokenï¼‰====================
        history_text = self._format_history_text()
        qa_prompt = f"""ä½ æ˜¯æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨ä¸­å›½åˆ‘æ³•é¢†åŸŸã€‚

ã€ç³»ç»Ÿè¯´æ˜ã€‘
æœ¬ç³»ç»Ÿæ¶µç›–åˆ‘äº‹çŠ¯ç½ªè®¤å®šã€é‡åˆ‘æ ‡å‡†ã€åˆ‘äº‹è´£ä»»å¹´é¾„ã€è‡ªé¦–ç«‹åŠŸã€æ­£å½“é˜²å«ç­‰ã€‚

ã€å›ç­”è¦æ±‚ã€‘
1. ä¸¥æ ¼ä½¿ç”¨æ£€ç´¢å†…å®¹ï¼Œä¸å¾—ç¼–é€ æ³•æ¡æˆ–æ¡ˆä¾‹
2. ä¼˜å…ˆå¼•ç”¨æ³•æ¡åŸæ–‡åŠæ¡æ¬¾å·ï¼Œæ ‡æ³¨[æ¥æºX]
3. åˆ†ææ„æˆè¦ä»¶ã€é‡åˆ‘å› ç´ ã€ç‰¹æ®Šæƒ…å½¢
4. ä¸ç¡®å®šæ—¶æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·

ã€å›ç­”æ ¼å¼ï¼ˆç®€æ´ç‰ˆï¼‰ã€‘
## æ ¸å¿ƒç­”æ¡ˆ
ç®€æ˜æ¦‚æ‹¬æ ¸å¿ƒç»“è®ºã€‚

## æ³•å¾‹ä¾æ®  
å¼•ç”¨ç›¸å…³æ³•æ¡åŸæ–‡[æ¥æºX]ï¼Œç®€è¦è§£è¯»ã€‚

## å…·ä½“åˆ†æ
åˆ†ææ„æˆè¦ä»¶ã€é‡åˆ‘å› ç´ ã€ç‰¹æ®Šæƒ…å½¢ã€‚

## é‡è¦æç¤º
æ³¨æ„äº‹é¡¹å’Œå»ºè®®ã€‚

ã€æ£€ç´¢å†…å®¹ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€å¯¹è¯å†å²ã€‘
{history_text}

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„æ³•å¾‹è§£ç­”ã€‚"""
        
        messages = [HumanMessage(content=qa_prompt)]
        
        response = self.llm.invoke(messages)
        answer = response.content
        
        # ==================== æ–¹æ¡ˆD: æ··åˆå¹»è§‰æ£€æµ‹ç­–ç•¥ ====================
        # é«˜é£é™©â†’LLMæ£€æµ‹ï¼ˆå‡†ç¡®ï¼‰| æ­£å¸¸â†’ä¸æ£€æµ‹ï¼ˆå¿«é€Ÿï¼‰| è¶…èŒƒå›´â†’ä¸æ£€æµ‹ï¼ˆå·²æ‹’ç­”ï¼‰
        hallucination_warning = ""
        should_check, use_llm = self._should_check_hallucination(is_out_of_scope, docs)
        
        if should_check and RERANKER_AVAILABLE:
            try:
                import time
                start = time.time()
                # å¯¹é«˜é£é™©é—®é¢˜ä½¿ç”¨LLMæ·±åº¦æ£€æµ‹ï¼Œå¯¹ä½é£é™©ä½¿ç”¨æœ¬åœ°å¿«é€Ÿæ£€æµ‹
                hallucination_result = check_hallucination(question, answer, docs, use_llm=use_llm)
                elapsed = time.time() - start
                
                # å¦‚æœæ£€æµ‹è€—æ—¶è¿‡é•¿ï¼Œè·³è¿‡
                if elapsed > HALLUCINATION_CHECK_TIMEOUT:
                    print(f"[è­¦å‘Š] å¹»è§‰æ£€æµ‹è€—æ—¶è¿‡é•¿ ({elapsed:.2f}s)ï¼Œå·²è·³è¿‡")
                    hallucination_warning = ""
                elif hallucination_result.risk_level in ["medium", "high"]:
                    hallucination_warning = f"\nâš ï¸  æ³¨æ„ï¼šè¯¥å›ç­”å¯èƒ½å­˜åœ¨å¹»è§‰é£é™©ï¼ˆ{hallucination_result.risk_level}ï¼‰ï¼Œè¯·è°¨æ…å‚è€ƒã€‚"
                    if hallucination_result.problematic_claims:
                        hallucination_warning += f"\né—®é¢˜é™ˆè¿°ï¼š{', '.join(hallucination_result.problematic_claims)}"
            except Exception as e:
                print(f"[è­¦å‘Š] å¹»è§‰æ£€æµ‹å‡ºé”™: {e}")
                hallucination_warning = ""
        
        answer = answer.strip()
        if hallucination_warning:
            answer += hallucination_warning
        
        # æå–å¼•ç”¨
        citations = self._extract_citations(docs)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(docs)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸ç¡®å®šå›ç­”
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        # å¦‚æœç½®ä¿¡åº¦è¿‡ä½ï¼Œä½¿ç”¨ä¸ç¡®å®šå›ç­”æ¨¡æ¿
        if is_uncertain and len(docs) == 0:
            answer = UNCERTAIN_RESPONSE
        
        # æ›´æ–°å¯¹è¯å†å²
        self.chat_history.append((question, answer))
        
        return RAGResponse(
            answer=answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def query_stream(self, question: str) -> Generator[str, None, RAGResponse]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢ + è¶…èŒƒå›´æ£€æµ‹ + Rerankerï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            str: ç­”æ¡ˆç‰‡æ®µ
            
        Returns:
            RAGResponse: å®Œæ•´å“åº”ï¼ˆåœ¨ç”Ÿæˆå™¨ç»“æŸæ—¶ï¼‰
        """
        # è¶…èŒƒå›´æ£€æµ‹ï¼ˆåŒé‡éªŒè¯ï¼‰
        is_out_of_scope, detected_domain = self._detect_out_of_scope(question)
        
        # LLMäºŒæ¬¡éªŒè¯
        if not is_out_of_scope and RERANKER_AVAILABLE and ENABLE_RERANKER:
            try:
                scope_result = check_scope(question)
                if not scope_result.is_in_scope and scope_result.confidence > 0.7:
                    is_out_of_scope = True
                    detected_domain = scope_result.detected_domain
            except Exception:
                pass
        
        if is_out_of_scope:
            out_of_scope_answer = OUT_OF_SCOPE_RESPONSE.format(detected_domain=detected_domain)
            yield out_of_scope_answer
            self.chat_history.append((question, out_of_scope_answer))
            return RAGResponse(
                answer=out_of_scope_answer,
                citations=[],
                confidence=0.1,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢ + Reranker
        docs = self._hybrid_retrieve(question, k=RETRIEVAL_TOP_K * 2)
        
        # Rerankeré‡æ’åº
        if RERANKER_AVAILABLE and ENABLE_RERANKER and docs:
            try:
                reranked_docs = rerank_documents(question, docs, top_k=RETRIEVAL_TOP_K)
                docs = [rd.document for rd in reranked_docs if rd.is_relevant]
            except Exception:
                docs = docs[:RETRIEVAL_TOP_K]
        else:
            docs = docs[:RETRIEVAL_TOP_K]
        
        # æ£€æµ‹ç›¸å…³æ€§
        if self._is_low_relevance(docs):
            low_relevance_answer = """æŠ±æ­‰ï¼Œåœ¨ç°æœ‰çš„åˆ‘æ³•æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜é«˜åº¦ç›¸å…³çš„æ³•æ¡ã€‚å»ºè®®å’¨è¯¢ä¸“ä¸šåˆ‘äº‹å¾‹å¸ˆã€‚"""
            yield low_relevance_answer
            self.chat_history.append((question, low_relevance_answer))
            return RAGResponse(
                answer=low_relevance_answer,
                citations=[],
                confidence=0.2,
                is_uncertain=True,
                retrieved_docs=docs
            )
        
        citations = self._extract_citations(docs)
        confidence = self._calculate_confidence(docs)
        is_uncertain = confidence < CONFIDENCE_THRESHOLD or len(docs) == 0
        
        if is_uncertain and len(docs) == 0:
            yield UNCERTAIN_RESPONSE
            self.chat_history.append((question, UNCERTAIN_RESPONSE))
            return RAGResponse(
                answer=UNCERTAIN_RESPONSE,
                citations=[],
                confidence=0.0,
                is_uncertain=True,
                retrieved_docs=[]
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(docs, 1):
            doc_type = doc.metadata.get("type", "unknown")
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            
            if doc_type == "statute":
                article = doc.metadata.get("article", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ³•æ¡ã€‘{source} {article}\n{doc.page_content}")
            else:
                accusation = doc.metadata.get("accusation", "")
                context_parts.append(f"[æ¥æº{i}] ã€æ¡ˆä¾‹ã€‘{source}ï¼ˆ{accusation}ï¼‰\n{doc.page_content}")
        
        context_text = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯ï¼ˆå¢å¼ºäº‹å®æ ¸æŸ¥ï¼‰
        history_text = self._format_history_text()
        qa_prompt = f"""ä½ æ˜¯"æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹"ï¼Œä¸€ä¸ªä¸“ä¸šçš„ä¸­å›½åˆ‘æ³•é—®ç­”AIï¼Œæ“…é•¿æä¾›è¯¦ç»†ã€ä¸“ä¸šã€æ˜“æ‡‚çš„æ³•å¾‹è§£ç­”ã€‚

ã€å›ç­”è¦æ±‚ - äº‹å®æ ¸æŸ¥ä¼˜å…ˆã€‘
1. **ä¸¥æ ¼ä¾æ®æ£€ç´¢å†…å®¹**ï¼šåªä½¿ç”¨æ£€ç´¢åˆ°çš„æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œä¸å¾—ç¼–é€ 
2. æ³•æ¡ä¼˜å…ˆï¼šå¿…é¡»å¼•ç”¨æ£€ç´¢åˆ°çš„ã€Šåˆ‘æ³•ã€‹æ¡æ–‡åŸæ–‡
3. é€šä¿—æ˜“æ‡‚ï¼šç”¨æ¸…æ™°çš„è¯­è¨€è§£é‡Šæ³•å¾‹æœ¯è¯­
4. å‡†ç¡®å¼•ç”¨ï¼šæ ‡æ³¨[æ¥æºX]

ã€å›ç­”æ ¼å¼ã€‘
## ğŸ“Œ æ ¸å¿ƒç»“è®º
æ¦‚æ‹¬é—®é¢˜çš„æ ¸å¿ƒç­”æ¡ˆã€‚

## ğŸ“– æ³•å¾‹ä¾æ®
å¼•ç”¨æ³•æ¡åŸæ–‡å¹¶è§£è¯»ã€‚

## ğŸ” è¯¦ç»†åˆ†æ
æ·±å…¥åˆ†ææ„æˆè¦ä»¶ã€é‡åˆ‘å› ç´ ç­‰ã€‚

## âš ï¸ é‡è¦æç¤º
æ³¨æ„äº‹é¡¹å’Œå»ºè®®ã€‚

ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘
{context_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€å¯¹è¯å†å²ã€‘
{history_text}

è¯·æä¾›è¯¦å°½ã€ä¸“ä¸šçš„è§£ç­”ã€‚"""
        
        # æµå¼ç”Ÿæˆå›ç­”
        full_answer = ""
        for chunk in self.llm.stream([HumanMessage(content=qa_prompt)]):
            if chunk.content:
                full_answer += chunk.content
                yield chunk.content
        
        self.chat_history.append((question, full_answer))
        
        return RAGResponse(
            answer=full_answer,
            citations=citations,
            confidence=confidence,
            is_uncertain=is_uncertain,
            retrieved_docs=docs
        )
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
    
    def get_history(self) -> List[Tuple[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.chat_history.copy()
    
    def search_similar(self, query: str, k: int = 5) -> List[Document]:
        """
        ç›´æ¥æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆä¸ç»è¿‡LLMï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            
        Returns:
            List[Document]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        if self.multi_domain_mode:
            # å¤šé¢†åŸŸæ¨¡å¼ï¼šä»æ‰€æœ‰é¢†åŸŸæ£€ç´¢å¹¶åˆå¹¶
            all_docs = []
            seen_doc_ids = set()
            for vs_info in self.vectorstores_multi.values():
                try:
                    docs = vs_info['vectorstore'].similarity_search(query, k=k)
                    for doc in docs:
                        doc_id = doc.metadata.get("doc_id", id(doc))
                        if doc_id not in seen_doc_ids:
                            seen_doc_ids.add(doc_id)
                            all_docs.append(doc)
                except Exception as e:
                    print(f"[è­¦å‘Š] æ£€ç´¢å¤±è´¥: {e}")
            return all_docs[:k]
        else:
            # å•é¢†åŸŸæ¨¡å¼
            return self.vectorstore.similarity_search(query, k=k)


# ä¾¿æ·å‡½æ•°ï¼šè·å–é»˜è®¤RAGå¼•æ“å®ä¾‹
_default_engine: Optional[JurisRAGEngine] = None

def get_rag_engine(streaming: bool = True) -> JurisRAGEngine:
    """è·å–RAGå¼•æ“å•ä¾‹"""
    global _default_engine
    if _default_engine is None:
        _default_engine = JurisRAGEngine(streaming=streaming)
    return _default_engine


def get_retriever():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–æ£€ç´¢å™¨"""
    engine = get_rag_engine()
    return engine.retriever


def get_rag_chain():
    """å…¼å®¹æ—§æ¥å£ï¼šè·å–RAGé“¾"""
    engine = get_rag_engine()
    return engine.rag_chain


# --- å‘½ä»¤è¡Œæµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– Juris-RAG å¼•æ“...")
    
    try:
        engine = JurisRAGEngine(streaming=False)
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸï¼\n")
        
        # æµ‹è¯•é—®é¢˜åˆ—è¡¨
        test_questions = [
            "æ•…æ„æ€äººç½ªæ€ä¹ˆåˆ¤åˆ‘ï¼Ÿ",
            "å¦‚æœæ˜¯æƒ…èŠ‚è¾ƒè½»çš„å‘¢ï¼Ÿ",  # æµ‹è¯•å¤šè½®å¯¹è¯
            "ç›—çªƒç½ªçš„é‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for q in test_questions:
            print(f"ğŸ‘¤ ç”¨æˆ·: {q}")
            print("-" * 50)
            
            response = engine.query(q)
            
            print(f"ğŸ¤– åŠ©æ‰‹: {response.answer}")
            print(f"\nğŸ“Š ç½®ä¿¡åº¦: {response.confidence}")
            print(f"â“ ä¸ç¡®å®šå›ç­”: {response.is_uncertain}")
            
            if response.citations:
                print("\nğŸ“š å¼•ç”¨æ¥æº:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.source} ({citation.doc_type})")
                    if citation.metadata.get("accusation"):
                        print(f"       ç½ªå: {citation.metadata['accusation']}")
            
            print("\n" + "=" * 60 + "\n")
            
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬æ„å»ºå‘é‡åº“ã€‚")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
